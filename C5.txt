5.1 Arbeiten mit Dateien


Vor einer Datenanalyse oder der Anwendung statistischer Verfahren stehen im Allgemeinen eine ganze Reihe von Arbeitsschritten, die ein Data Analyst durchzuführen hat. Einige typische Beispiele dafür sind in Abbildung 5.1 zu finden.


Abb. 5.1: Arbeitsschritte vor einer statistischen Analyse.

In diesem Abschnitt liegt der Fokus auf den beiden ersten Schritten im Kontext der Arbeit mit klassischen Dateien, u.a. in den Abschnitten 6.1.1 und 6.1.2 des Folgekapitels werden wir auf weitere Techniken zur Datentransformation und insbesondere auch den Umgang mit fehlenden Daten eingehen. Das Thema Datenbanken wird in den Abschnitten 5.3 für die relationalen Vertreter bzw. 5.4 für die sog. No-SQL-Datenbanken genauer besprochen.

Wir beschäftigen uns hier zunächst mit physikalischen (im Sinn von wo liegen die Daten) und logischen (in welchem Format liegen die Daten vor) Aspekten der Datenspeicherung und gehen im Anschluss beispielhaft auf den Umgang mit einigen häufig verwendeten Dateiformaten ein.


Tab. 5.1:Einige mögliche Datenquellen für eine statistische Anwendung.



Textformate offene binäre Formate Datenbanken proprietäre Formate

CSV Protocol Buffer Oracle .mat-Files (Matlab)

JSON HDF5 Postgres SAS

XML Avro SQL Server

(Excel) Java Serialization/pickle MongoDB/Cassandra




5.1.1 Datenspeichermedien


Die dauerhafte Speicherung digitaler Dokumente erfolgt in Form von Dateien auf elektronischen Medien wie Festplatten. Abhängig von der konkreten Arbeitssituation (Ist Teamzugriff erforderlich?), von Vertraulichkeitsanforderungen oder ganz allgemein der Verfügbarkeit kommen hierbei verschiedene Varianten zum Einsatz, Beispiele sind u.a.

– die Ablage der Dateien auf der lokalen Festplatte eines Arbeitsplatzrechners,



– die Ablage von Dateien auf zentralen Netzlaufwerken im Unternehmen,



– die Benutzung von Kollaborationsplattformen wie Microsofts SharePoint,



– die Speicherung in einem Cloud-Angebot wie z.B. Google Docs, Google Drive oder Microsoft OneDrive,



– Cloud Object Storage wie Amazons S3 (insbesondere für Bilder o.Ä.),



– fremd- oder selbstgehostete Versionierungs-/Kollaborationsplattformen wie Git.





Hierbei muss man im Einzelfall normalerweise Kompromisse eingehen, da jede Variante spezifische Vor- und Nachteile mit sich bringt. Zugriffszeiten beispielsweise sind für die lokale Festplatte im Normalfall deutlich kürzer als bei einem Netzlaufwerk, Zugriffsrechte über Unternehmensgrenzen lassen sich in den beiden erstgenannten Ansätzen kaum realisieren und geteiltes Arbeiten am gleichen Dokument wird erst mit sinnvollen Konflikterkennungs- und -auflösungsmechanismen sinnvoll möglich (wie z.B. in Git, aber auch dort nur für reine Textformate). Zu beachten sind hier ferner rechtliche bzw. Compliance-Vorgaben der Unternehmen, die es ggf. untersagen, bestimmte Daten einfach auf der Festplatte eines Laptops abzulegen.


5.1.2 Datenformate


Neben dem Speichermedium ist auch zu überlegen, wie die Daten gespeichert werden, d.h. welche Dateiformate gelesen bzw. erzeugt werden müssen. Der Möglichkeiten sind undenkbar viele, die folgende Tabelle 5.1 soll daher nur als beispielhafte Annäherung verstanden werden.

Bei Dateien liefert die Einteilung in textbasierte bzw. binäre Dateien eine erste grobe logische Untergliederung. Auf dem Speichermedium liegen hierbei alle Daten als Sequenz von Bytes vor. Spricht man von binären Daten oder Dateien, so soll damit zum Ausdruck gebracht werden, dass es sich um beliebige Daten in Byteform handelt, die ein verarbeitender Prozess auf eine spezielle Art zu interpretieren hat. Die Byte-Sequenz ist also eine codierte Form der Nutzdaten und erst durch Anwendung und Interpretation durch ein vorgegebenes Schema erhält man die eigentlichen Nutzdaten.

Als Beispiel für eine binäre Datenspeicherung betrachten wir die Speicherung einer Matrix von Flieẞkommazahlen in einer Datei. Dies könnte man z.B. so erreichen, dass man zunächst zwei 16-Bit Zahlen mit den Dimensionen n und m der Matrix und dann nm 8-Byte double precision Zahlen anfügt1. Die Anzahl der benötigten Bytes wäre also dann 4 + 8nm.




Was ist darüber hinaus noch festzulegen?





Im Beispiel war die Kodierung naheliegend aber im Grunde individuell, sodass das Wiedereinlesen der Daten wiederum durch einen individuellen Prozess erfolgen wird. Ein Binärformat kann aber auch öffentlich sein, als Beispiele seien hier genannt:

– das PDF-Format, dessen Spezifikation von Adobe offengelegt wurde und welches von vielen Programmen und Bibliotheken unterstützt wird2,



– das .mat Format von Matlab3.





Einen Sonderfall bilden hier in gewisser Weise die sog. Textdateien, wie sie z.B. von Texteditoren erzeugt werden. Ein vor allem im Kontext der Internationalisierung wichtiges Thema ist das des Kodierungsformates für Textdateien. Wir gehen im Folgenden kurz darauf ein, verweisen aber auch auf JoelOnUnicode4 für einen amüsanten Einstieg. Als Hintergrund sollte man sich klarmachen, dass eine Textdatei interpretiert wird als Sequenz von Zeichen eines bestimmmten Alphabets, welche in einer bestimmten Codierung in eine Datei gespeichert werden. Bis vor nicht allzu langer Zeit wurde hierzu meist ASCII verwendet, ein Code, der 128 Zeichen mit sieben Bits codiert. Da diese Zeichenmenge in vielen Zusammenhängen nicht mehr ausreicht, sind verschiedene Erweiterungen entwickelt worden. Das Standardalphabet für druckbare Zeichen ist heutzutage Unicode5. Zur Speicherung der Zeichen ist wiederum eine Codierung in Bytes erforderlich, wobei ein Unicode-Zeichen ein, zwei oder noch mehr Bytes beanspruchen kann. Hierfür sind verschiedene Standards entwickelt worden, die Bytezahl je Zeichen kann entweder für alle Zeichen gleich sein (z.B. UTF-32) oder es wird eine unterschiedliche Anzahl von Bytes je nach Zeichen verwendet (z.B. UTF-8). Welche Unicode-Codierung standardmäẞig zum Einsatz kommt, ist teilweise plattformabhängig und sollte bewusst entschieden werden.6 Als Beispiel betrachten wir einmal das Einlesen und Ausgeben einer Textdatei unter Angabe der Codierung in Java:

File fileDir = new File("c:\\temp\\test.txt");

BufferedReader in = new BufferedReader(

new InputStreamReader(

new FileInputStream(fileDir), "UTF8"));

String str;

while ((str = in.readLine()) != null) {

System.out.println(str);

}

in.close();

Die Stream-Klassen in Java machen den Prozess für den Nutzer transparent: Um an die Daten in der Datei heranzukommen wird die Funktionalität von drei Klassen genutzt. Der FileInputStream arbeitet hierbei noch auf der Byteebene, in der nächsten Verarbeitungsstufe im InputStreamReader erfolgt die Interpretation der Byte-Sequenz (oder des Byte-Streams in der Java-Terminologie) als Textsequenz. Als letztes wird darüber noch ein BufferedReader gelegt, was eine komfortablere Schnittstelle und Performanceverbesserungen mit sich bringt.

Analog dazu erfolgt auch das Schreiben einer Text-Datei als Umkehrung des Prozesses oben in zwei Schritten, wobei nicht jede Programmierschnittstelle oder -sprache dies auch explizit macht. Hier ein Beispiel in C#:

string fileName = "test.txt";

string textToAdd = "Text f\"ur die Datei.";

Encoding encoding = Encoding.GetEncoding("ISO-8859-1");

using (FileStream fs = new FileStream(fileName, FileMode.CreateNew)) {

using (StreamWriter writer = new StreamWriter(fs, encoding)) {

writer.Write(textToAdd);

}

}

Im Gegensatz zu Textdateien repräsentieren die Bytes in einer binären Datei nicht notwendig Zeichen. Möchte man z.B. eine double precision Zahl in einer Datei speichern, so hat man acht Bytes zu speichern, wobei die einzelnen Bytes zunächst nichts mit einer textuellen Repräsentation der Zahl als Dezimalzahl zu tun haben. Das soeben für Zahlen gesagte gilt natürlich auch bei komplexeren Datenstrukturen. Als Konsequenz halten wir fest, dass binäre Speicherung i.d.R. kompakter und effizienter verarbeitbar ist, jedoch die genaue Kenntnis der Struktur erfordert und damit in der Regel nur von (der korrekten Version) der richtigen Software lesbar ist. Textformate (auch für strukturierte Daten) sind universell verarbeitbar und oft kann das Schema aus der vorliegenden Struktur erraten werden. Eine Kombination von einigen Vorteilen beider Möglichkeiten erreicht man z.B. dadurch, dass Textdateien in Form von komprimierten Archiven wie zip gespeichert werden, ein Verfahren wie es beispielsweise bei verschiedenen Office-Applikationen zum Einsatz kommt.

Wir wollen in den folgenden Ab"!!""schnitten exemplarisch einige häufig verwendete Formate und deren Verarbeitung in Python/R vorstellen, für die es in den genannten Plattformen (meist mehrere) vorgefertigte Lösungen gibt. Der Import aus Datenbanken bleibt zunächst unberücksichtigt, da er später (Abschnitt 5.3.1) ausführlicher besprochen wird.


5.1.3 Tabellarische Daten


In vielen Fällen werden Eingabedaten für eine Datenanalyse schon in Tabellenform vorliegen oder sie werden in einem Vorverarbeitungsschritt dahin überführt. Die einzelnen Datensätze (auch Records oder Beobachtungen) sind dann in den Zeilen angeordnet. Zur Repräsentation im Rechner bedient man sich in R meist des Datentyps data.frame und in Python steht über die Bibliothek pandas ein mächtiges Äquivalent zur Verfügung (pandas.DataFrame). In diesem Abschnitt gehen wir davon aus, dass die Menge der zu bearbeitenden Daten es zulässt, sie problemlos in den Hauptspeicher zu laden7, anderenfalls werden Technologien wie Hadoop/Spark relevant, vgl. dafür Abschnitt 5.8.

Wir beginnen mit einem einfachen, in der folgenden Tabelle 5.2 wiedergegebenen Beispiel.


Tab. 5.2:Einfaches Tabellenbeispiel.



NAME SEX DATE_OF_BIRTH

Matzerat, Oskar m 1926-10-11

Bieberkopf, Franz m 1901-03-22



Im CSV-Format, was originär für comma separated values steht aber letztlich (s.u.) eine Familie von eng verwandten Formaten beschreibt, sieht die Darstellung dann beispielsweise wie folgt aus:

NAME,SEX,DATE_OF_BIRTH

"Matzerat, Oskar",m,1926-10-27

"Bieberkopf, Franz",m,1901-03-22

Die Datensätze werden also zeilenweise gespeichert und zwischen den einzelnen Spalten steht ein Trennzeichen. Es gibt einige Parameter des CSV-Formats, die zur Verarbeitung zu spezifizieren sind:

– Trennungszeichen (hier “,”, aber auch TAB oder im Deutschen “;” oder das Leerzeichen sind gebräuchlich),



– Spaltenüberschriften (vorhanden/nicht vorhanden),



– Zeilenendezeichen - in unixoidalen Betriebssystemen normalerweise “\n” und unter Windows “\n\r”,



– quote character (meist das doppelte Anführungszeichen, s.u.),



– escape character (meist “\”, s.u.),



– Dateiencodierung,



– Unterstrukturen (z.B. im Namensfeld oben oder bei Datumsangaben relevant).





Die Notwendigkeit des quote characters ergibt sich daraus, dass das Trennungszeichen sonst nicht Teil eines Tabelleneintrags sein kann. Ist das der Fall, so ist der Tabelleneintrag zu quotieren (in Anführungszeichen zu setzen). Soll nun wiederum der quote character Teil eines Tabelleneintrags sein, so ist er mit dem escape character einzuleiten. Hier beispielhaft das programmatische Einlesen einer CSV-Datei mit Python

import pandas as pd

df = pd.read_csv("data/data1.csv")

bzw. R

read.csv("data\\data1.csv")

Für die genaue Spezifikation sei auf die Hilfe zu den einzelnen Kommandos verwiesen, in R zum Beispiel mit ?read.csv zu erfragen.


5.1.4 Excel - Nicht-tabellarische Daten


Als weiteres, in der aktuariellen Praxis durchaus relevantes Beispiel werfen wir einen Blick auf Excel Dateien. Flache Tabellen in xls/xlsx-Dateien können mit ähnlichen Befehlen wie gerade eben für CSV demonstriert gelesen werden (z.B. pandas.read_excel an Stelle von read_csv – die Dokumentation ist online leicht zu finden).

Wir betrachten das in Abbildung 5.2 wiedergegebene Beispiel, in dem Informationen in zwei Spalten geordnet zu finden sind. Die erste Spalte ist jeweils ein Feldname und die zweite enthält die eigentliche Information. Zeilen mit Daten werden dabei von Überschriftszeilen unterbrochen.


Abb. 5.2: Spreadsheet-Daten mit zwei relevanten Spalten.

Es soll nun beispielhaft das Feld model_point_id und die Tabelle unter der Überschrift SCR Calculation gelesen werden. Das wäre auch mit pandas. read_excel möglich, wobei dann aus dem eingelesenen DataFrame die Daten auf geeignete Weise extrahiert werden müssten. Im folgenden Beispiel nutzen wir die Bibliothek openpyxl, die insbesondere auch zum Schreiben von Excel-Dateien zusätzliche Features anbietet und deren Interface an den Zugriff über Makros in Visual Basic for Applications (VBA) erinnert.

from openpyxl import load_workbook

# Worksheet-Object besorgen

wb_mp = load_workbook("mp_status_example.xlsx")

ws_status = wb_mp['status']

# einzelne Zelle auslesen

model_point_id = ws_status["B7"].value

# Lesen der Einträge unter "SCR properties"

scr_properties = {}

for row in ws_status["A19:B26"]:

scr_properties[row[0].value.upper()] = row[1].value

Die Variable model_point_id hat dann den Wert ”2015_152535” und die Ausgabe des eingelesenen scr_properties Dictionaries ergibt

{'CALCULATION DATE': '1/21/2016 03:47:36 PM',

'CALCULATION STATUS': 'Valid',

'FX RATES (END OF QUARTER)': '12-31-2015',

'NUMBER OF ITERATIONS': 100000,

'RESULTS FOLDER': 'Result_1',

'VERSION MATH KERNEL': '2015.13.2',

'YIELD CURVE DATE': '12-31-2015',

'YIELD CURVE NAME': 'YC Q4 2015 EIOPA original spot'}

In anderen Sprachen stehen teilweise eigene freie oder kommerzielle Excel-Libraries zur Verfügung (z.B. Apache POI 8 für Java).

Speziell für Excel besteht natürlich auch die Möglichkeit, die Datei als .csv zu speichern bzw. VBA-Skripte zu nutzen, die die Daten extrahieren und als csv/json-Dateien exportieren, sie direkt in Datenbanken schreiben oder sie an eine Web-API übermitteln. Die Ausführung solcher Skripte in vollständig automatisierten Arbeitsabläufen kann dann über die COM-Schnittstelle von Excel angestoẞen werden (z.B. von einem Python/Matlab-Skript aus, vgl. hierzu auch das im Abschnitt 5.6 über Testautomatisierung angesprochene AutoIT). Insgesamt scheint jedoch die Komplexität deutlich geringer zu bleiben, wenn die Datenextraktion in einem Tool vorgenommen werden kann.


5.1.5 Textdateien


Die programmatische Verarbeitung von Text aus einfachen Textdateien erfolgt dadurch, dass der Text (oder jeweils nur eine oder wenige Zeilen) in eine entsprechende Variable eingelesen werden. Als Beispiel sei angenommen, dass in der Datei ycfwd. sql im aktuellen Arbeitsverzeichnis eine SQL-Query gespeichert ist. Diese kann beispielsweise in Python durch

with open("ycfwd.sql", "r") as query_file:

yc_query = query_file.read()

in die Variable yc_query eingelesen und dann weiterverarbeitet werden.

Das programmatische Einlesen von Texten, die in Formaten wie docx oder pdf vorliegen, kann in vielen Fällen ähnlich wie oben beschrieben erfolgen, wobei natürlich entsprechende Pakete einzubinden sind, die mit den Dateiformaten umgehen können. Hierzu wird jedoch im Allgemeinen eine eingehendere Beschäftigung mit diesen Paketen von Nöten sein, um auftretende Probleme lösen zu können, das Thema soll deshalb an dieser Stelle nicht weiter vertieft werden.


Digression: Textanalyse


In diesem Abschnitt soll als kurzer Exkurs eine beispielhafte Aufbereitung eines Textes gezeigt werden, deren Ergebnis dann (zusammen mit vielen anderen aufbereiteten Texten) als Eingabe für Algorithmen des maschinellen Lernens benutzt werden könnte.

Der Startpunkt ist also ein Text, den wir im folgenden Skript zunächst herunterladen.9

import collections

import re

import requests

# einen Text aus dem Internet laden

url_lessing = "https://www.gutenberg.org/ebooks/9375.txt.utf-8"

fabeln_lessing = requests.get(url_lessing).content.decode("utf-8")

# Markierungen für Beginn/Ende des Textes

marker_head = "http://gutenberg.spiegel.de/ erreichbar."

marker_foot = "Ende dieses Projekt Gutenberg"

text_start_pos = fabeln_lessing.find(marker_head) + len(marker_head) + 4

tex_end_pos = fabeln_lessing.find(marker_foot)

fabeln_lessing_stripped = fabeln_lessing[text_start_pos:tex_end_pos]

print(fabeln_lessing_stripped)

Nach dem Laden des Textes über das Internet schneiden wir noch Vor- und Nachspann weg, die nicht zum Text gehören. Die ersten Zeilen des bisherigen Ergebnisses sind im Folgenden wiedergegeben.

Ausgewählte Fabeln

Gotthold Ephraim Lessing

1759

Inhalt:

Das Geschenk der Feien

Das Ro und der Stier

Der Affe und der Fuchs

Der Besitzer des Bogens

Der Esel mit dem

Für die meisten Analysevorhaben wird man den Text in eine geeignete Form überführen wollen, der nächste Schritt dahin ist meist eine Liste von einzelnen Worten, was mit Python-Bordmitteln leicht zu bewerkstelligen ist. Der Befehl

word_list = fabeln_lessing_stripped.split()

liefert die (gekürzte) Ausgabe

['Ausgewählte',

'Fabeln',

'Gotthold',

'Ephraim',

'Lessing',

'1759',

'Inhalt:',

'Das',

'Geschenk',

'der']

Je nach Zweck wird man noch etwas nacharbeiten müssen, wir wollen beispielsweise den Doppelpunkt entfernen. Wir versuchen es erneut, indem wir zunächst – wiederum mit Python Bordmitteln – einige Sonderzeichen entfernen:

def replace_special_chars(text):

""" Sonderzeichen im Text durch Leerzeichen ersetzen. """

for ch in [':', '"', ',', ';', '?', '\\','`','*','_','{','}',

'[',']','(',')','>','#','+','-','.','!','$','\'']:

if ch in text:

text = text.replace(ch," ")

return text

# Ersetzen und Trennen

word_list = replace_special_chars(fabeln_lessing_stripped).split()

Dieses Mal ist die Ausgabe der ersten zehn Worte

['Ausgewählte',

'Fabeln',

'Gotthold',

'Ephraim',

'Lessing',

'1759',

'Inhalt',

'Das',

'Geschenk',

'der']

Typische nächste Schritte der Textaufbereitung wären die Entfernung von Füllwörtern (engl. stopwords, das sind Wörter, die keine inhaltliche Bedeutung tragen) und die Lemmatisierung (engl. stemming, die Rückführung auf eine grammatikalische Grundform). Hierzu existieren leistungsfähige Bibliotheken (vgl. spaCy10 oder NLTK11 für Python), Listen mit Stopwords für verschiedene Sprachen sind auch leicht im Internet zu finden. Wir beschränken uns hier auf eine triviale Typisierung und einen anschlieẞenden Wordcount mit der Python-Standardbibliothek, die hierfür auch schon sehr mächtige Tools wie reguläre Ausdrücke bereitstellt.

Im nächsten Codefragment teilen wir jedes Wort des Textes in eine von drei Kategorien ein, nämlich als Zeichenkette, die entweder ausschlieẞlich aus Kleinbuchstaben besteht (Kategorie 1), als Zeichenkette, die mit einem Groẞbuchstaben beginnt und ggf. weitere Kleinbuchstaben enthält (Kategorie 2) oder als ein Wort, dass nur aus Ziffern besteht (Kategorie 3). Wir lassen in der Einteilung theoretisch auch eine Restkategorie zu, die im Beispiel aber nicht vorkommt. Hierfür wird die Wortfolge einmal durchlaufen und für jedes Wort w wird festgestellt, in welche Kategorie K es fällt und das Ergebnis wird in Form eines Tupels (w, K) in der Liste words_with_types festgehalten. Jedes Wort wird dabei mit den Mustern

expr1 = "^[a-z|ä|ö|ü|ẞ]*$"

expr2 = "^[A-Z|Ä|Ö|Ü][a-z|ä|ö|ü|ẞ]*$"

verglichen. Dies sind Beispiele für reguläre Ausdrücke, die ein überaus mächtiges Werkzeug zur Textanalyse darstellen und hier nur kurz angerissen werden sollen. Zur Erklärung: Die Symbole ^ und $ kennzeichnen den Anfang bzw. das Ende eines Wortes. Das erste Pattern passt hierbei, wenn ein Wort ausschlieẞlich aus einer beliebigen Anzahl (dafür steht das Symbol *) der vorkommenden Symbole (hier eine zwischen eckigen Klammern eingeschlossene Alternativliste von einzelnen Buchstaben) aufgebaut ist, das zweite Pattern passt, wenn ein Wort mit einem Groẞbuchstaben (einschlieẞlich Umlaut) beginnt und dann eine beliebige Anzahl von Kleinbuchstaben (inkl. Umlauten oder ẞ) folgt. Das dritte verwendete Muster (vgl. nächster Codeblock) sollte dann leicht zu verstehen sein.

# die Klassifizierung wird mit regulären Ausdrücken vorgenommen

check_lowercase_word = re.compile(expr1)

check_capital_first = re.compile(expr2)

check_number = re.compile(r"^[0-9]*$")

words_with_types = []

for s in word_list:

if check_lowercase_word.match(s):

words_with_types.append((s.lower(), 1))

elif check_capital_first.match(s):

words_with_types.append((s.lower(), 2))

elif check_number.match(s):

words_with_types.append((s.lower(), 3))

else:

words_with_types.append((s.lower(), 0))

# Zählen der Häufigkeiten der einzelnen Kategorien

collections.Counter([w[1] for w in words_with_types])

In der letzten Zeile wurden die Kategorienhäufigkeiten gezählt, die Ausgabe ist

Counter({1: 4427, 2: 1712, 3: 12})

Damit sind also alle Wörter klassifiziert als Wort in Klein- bzw. als mit Groẞbuchstaben beginnend oder als Zahl. Auch das Feststellen der Häufigkeiten der einzelnen Wörter geht nun schnell:

counter_words = collections.Counter([w[0] for w in words_with_types])

print("Anzahl unterschiedlicher Wörter:", len(counter_words))

print("Häufigste Wörter:")

counter_words.most_common(10)

Das Ergebnis ist nicht sonderlich überraschend, was wiederum die Notwendigkeit von nicht-trivialen Techniken wie stemming und die Entfernung von Stopwords unterstreicht.

Anzahl unterschiedlicher Wörter: 1861

Häufigste Wörter:

[('der', 266),

('und', 191),

('die', 160),

('ich', 128),

('zu', 105),

('das', 86),

('du', 78),

('ein', 75),

('den', 72),

('nicht', 67)]

Neben dem Zählen und Typisieren von Wörtern kann man auch Machine Learning Algorithmen auf Texte anwenden. Hierzu muss man die Texte in für die Algorithmen geeignete Eingabevektoren umkodieren. Ein einfacher Ansatz besteht z.B. darin, ein Vokabular vorzugeben (z.B. alle vorkommenden Wörter eines interessierenden Textkorpus, also einer Gesamtheit von zu untersuchenden Texten) und diese zu ordnen und abzuzählen. Ein einzelner Text wird dann durch einen Vektor kodiert, dessen j-te Komponente die Häufigkeit des j-ten Wortes des Textkorpus im vorliegenden Text angibt.12

Es ist auch möglich nicht nur Vorkommen einzelner Worte, sondern von Wortgruppen im Text zu zählen. Eine Wortgruppe der Länge N wird auch ein N -gramm genannt. Ein Digramm ist hierbei ein Wortpaar. Auf diese Art wird erreicht, dass die Kodierung neben der Häufigkeit auch Informationen über die Reihenfolge der Wörter abbildet.

Man erhält so im Allgemeinen hochdimensionale Featureräume, so dass weitere Vorverarbeitungsschritte notwendig sein können, die die Dimension des Problems reduzieren können. Das können auch klassische Verfahren zur Dimensionsreduktion (vgl. Abschnitt 6.5) sein, aber insbesondere die schon erwähnten Vorverarbeitungsschritte im Kontext der Textverarbeitung spielen eine wichtige Rolle:

– Standardisierung wie Umwandlung in Kleinbuchstaben und das Entfernen von Sonderzeichen



– das Entfernen von Stopwords (wie zum Beispiel Artikel)



– Stemming/Lemmatisierung: Die Rückführung eines Wortes auf auf eine grammatikalische Grundform.





Wir illustrieren das Vorgehen für den einfachsten Fall. Zunächst konstruieren wir aus der zuvor erzeugten Wortliste ein Vokabular, indem wir die Wörter in Kleinbuchstaben zunächst in ein Python set überführen und dann in eine sortierte Liste, der erste Schritt eliminiert dabei alle Wortwiederholungen. Anschlieẞend werden die Wörter mit der Funktion enumerate abgezählt und daraus ein Dictionary erzeugt, das jedes Wort auf seine Nummer abbildet:

vokabular = sorted({w.lower() for w in word_list})

word_index = {word: index for (index, word) in enumerate(vokabular)}

Als Beispieltext soll uns nun ein Ausschnitt des Orginaltexts dienen:

test_text = word_list[1105:1119]

Nun können wir den Text einfach kodieren, indem wir ausgehend vom einem Null-Vektor der Länge des Vokabulars über den Text iterieren und für jedes Wort die entsprechende Komponente des Vektors erhöhen.

import numpy as np

text_vec = np.zeros(len(vokabular))

for wrd in test_text:

text_vec[word_index[wrd.lower()]] += 1

Mit dem Vektor text_vec liegt nun eine einfache, dem Bag of Words-Modell entsprechende, numerische Repräsentation des betrachteten Textauschnittes vor. Die Summe der Komponenten des erhaltene Vektors test_vec ist 14 und entspricht der Anzahl der Wörter des Textausschnittes.

Anstelle von Häufigkeiten von Wörtern kann es auch sinnvoll sein, Wortfrequenzen als normierte Maẞe zu Grunde zu legen. Um sinnvolle Maẞe zu erhalten wird dabei sowohl die Häufigkeit eines Wortes im zu untersuchenden Text als auch die Häufigkeit von Wörtern im ganzen Textkorpus zu Grunde gelegt, vgl. [73]. Darüber hinaus gibt es auch noch andere Techniken zur Kodierung von Texten als die besprochenen N-gramme, das Tutorial [27] stellt u.a. das Konzept der Worteinbettungen vor.


5.1.6 Techniken für Verschlüsselung und Pseudonymisierung


Für die in diesem Abschnitt behandelten Themen gibt es eine Reihe unterschiedlicher denkbarer Ziele und Anwendungsfälle, wie z.B.

– Teile von Daten sollen an Dritte weitergegeben werden, z.B. Policendaten an ein Biometrie-Department oder einen Rückversicherer, damit dieser statistische Untersuchungen vornehmen kann. Dafür ist ein Klartextname und normalerweise auch die genaue Adresse nicht erforderlich, möglicherweise aber eine Berufsgruppe.



– Bei der Bonitätseinschätzung eines Kreditnehmers durch eine Spezialistenabteilung oder eine externe Agentur könnten Aspekte des Wohnorts erforderlich sein, der Name sollte jedoch keine Rolle spielen.





Kritisch sind bei einer Datenweitergabe vor allem personenbezogene Daten wie z.B. Namen oder Adressdaten.13 Für die in solchen Fällen angebrachte Anonymisierung oder Pseudonymisierung kann man sich mit einer Hash-Funktion behelfen, die z.B. die Namen auf eine abstrakte Menge M von IDs abbildet (Zeichenkette oder Zahl):



Um den Zweck zu erfüllen, sollte die Funktion möglichst injektiv sein und nicht leicht umkehrbar. Beides wird man im Allgemeinen nicht erreichen, es gibt jedoch universelle secure hash Funktionen (SHA-XXX, MD514), die in den meisten Fällen ausreichend stark sein sollten (und durch salting weiter verstärkt werden können, s.u.). Diese Pseudonymisierung ist dann deterministisch und kann jederzeit konsistent wiederholt werden, wodurch sich Vorteile gegenüber dem einfachen Abzählen der Datensätze ergeben, weil dann auch beim Empfänger eine Historie für einzelne (anonyme) Individuen aufgebaut werden kann

Um die Sicherheit noch weiter zu erhöhen, nutzt man in der Praxis normalerweise einen sogenannten salted hash. Man wendet dann die Hash-Funktion nicht auf das Originalwort, sondern auf ein um einen festen Bestandteil verlängertes Wort an, dessen genaue Konstruktion natürlich unter Verschluss zu halten ist. Diese Technik wird auch oft verwendet, wenn z.B. Passwörter in einer Datenbank gespeichert werden sollen. Es wird üblicherweise nicht das Klartextpasswort abgelegt sondern ein Hashwert des Passworts. Durch das erwähnte Verlängern wird nun zusätzlich erreicht, dass beispielsweise bekannte Standardpasswörter nicht mit ihrem Standardhashwert gespeichert werden, wodurch zusätzliche Sicherheit entsteht, falls ein Unbefugter Kenntnis von den gespeicherten Passwort-Hashwerten erlangt. Beispiele zur Berechnung von Hashwerten zeigen die folgenden Beispiele.

Python:

import hashlib

m = hashlib.sha1()

m.update(b"Mein Name")

m.digest().hex() # ergibt '7ac8c4aa30fba6eb2ec92ef3ff81b7f24159a82a'

R:

digest::digest("Mein Name", algo='sha1', serialize=FALSE)

#

# [1] "7ac8c4aa30fba6eb2ec92ef3ff81b7f24159a82a"

Auch einige Datenbanken können Hashwerte berechnen. In Postgres beispielsweise liefert SELECT encode(digest(’Mein Name’, ’sha1’), ’hex’) ebenfalls den angegebenen Hash-String, wofür allerdings die Erweiterung pgcrypto aktiviert sein muss.

Eine Verschlüsselung soll im Unterschied zum Hashing umkehrbar sein, wenn man im Besitz des Schlüssels ist. Man unterscheidet zwischen symmetrischen und asymmetrischen Verfahren. Bei symmetrischer Verschlüsselung sind die zum Ver- und Entschlüsseln eingesetzten Schlüssel identisch, bei asymmetrischen sind sie verschieden.

Symmetrische Verfahren sind in der Regel effizienter zu berechnen und werden für gröẞere Datensätze (z.B. groẞe Dateien) eingesetzt. Bei einem Datenaustausch muss der Schlüssel sowohl dem Sender als auch dem Empfänger bekannt sein. Es ist also ein Schlüsselaustausch erforderlich, wodurch zusätzliche Kompromittierungsrisiken entstehen.

Asymmetrische Verfahren werden häufig so eingesetzt, dass ein privater Schlüssel (nur) dem Empfänger und ein öffentlicher Schlüssel (mindestens auch) dem Sender bekannt ist. Der Sender verschlüsselt die Nachricht (Daten) mit dem öffentlichen Schlüssel und (nur) der Empfänger kann die Nachricht mit dem privaten Schlüssel entschlüsseln. In der praktischen Umsetzung sind die benutzten Algorithmen häufig teuer in der Ausführung und somit eher nur für kleine Datensätze geeignet. Ein Zusatzrisiko durch einen Austausch von Schlüsseln entsteht in diesem Fall nicht.

In der Praxis setzt man bei gröẞerem Datenvolumen oft eine Kombination beider Verfahren ein, um die Vorteile der asymmetrischen Verfahren zu den Kosten der symmetrischen zu nutzen, hierzu kann etwa der Schlüsselaustausch nach Diffie-Hellman15 oder ein Protokoll ähnlich dem folgenden Schema verwendet werden:

– Der Sender generiert einen neuen symmetrischen Schlüssel k



– Die Daten D werden symmetrisch damit verschlüsselt (z.B. mit dem AES-Verfahren16) zum Datensatz V



– der Schlüssel k wird mit dem öffentlichen Schlüssel des Empfängers asymmetrisch verschlüsselt zu s



– (s, V) wird an Empfänger übermittelt



– Empfänger entschlüsselt s unter Verwendung des ihm bekannten privaten Schlüssels und erhält k



– die symmetrische Verschlüsselung wird mittels k invertiert, um D zu erhalten





Ein weiteres Einsatzfeld für die asymmetrischen Verfahren ist das Signieren von Nachrichten, indem die zu übermittelnde Nachricht (oder ein Hashwert derselben) noch mit dem privaten Schlüssel des Senders verschlüsselt wird und das erhaltene Ergebnis als Kontrollcode der Nachricht hinzugefügt wird. Ein Empfänger kann dann durch Entschlüsseln des Controllcodes mit dem bekannten öffentlichen Schlüssel verifizieren, ob der Sender Kenntnis vom entsprechenden privaten Schlüssel hatte und schlieẞt dadurch auf die Identität des Senders.




In der praktischen Anwendung sind selbst erstellte Lehrbuch-Varianten der genannten Algorithmen in jedem Fall zu vermeiden und stattdessen die jeweils aktuelle Version einer etablierten Bibliothek zu verwenden. Das ist nicht nur eine Frage der Berechnungseffizienz, es gibt auch dramatische Sicherheitslücken, die durch Implementierungsfehler von kryptographischen Algorithmen entstanden sind17.





5.2 Arbeiten mit Daten aus dem Netz


Bezieht man Daten direkt aus dem Internet, trifft man neben den bekannten Dateiformaten auch auf andere, speziell für das Web entwickelte Formate with html und json. Wir stellen in diesem Abschnitt einige wichtige Beispiele vor und zeigen, wie man Daten in eine Data-Science-Umgebung laden kann und erklären Grundsätzliches zu ihrer Verarbeitung.


5.2.1 Objekt-Darstellungen in JSON


Zur textuellen Repräsentation und zur Übertragung von Datensätzen in einem Rechnernetz werden oft Formate wie XML, JSON oder Yaml genutzt18. Wir konzentrieren uns hier auf JSON (Java Script Object Notation), da es vermutlich das wichtigste der genannten Formate ist. Dies ist u.a. auf folgende Eigenschaft von JSON zurückzuführen:

– das Format ist leicht maschinell verarbeitbar,



– JSON ist menschenlesbar,



– JSON ist kompakter als viele andere Formate (z.B. als XML),



– JSON ist Teil der JavaScript-Spezifikation, der wahrscheinlich derzeit am weitesten verbreiteten Programmiersprache.





Die Notation in JavaScript kann etwas laxer gehandhabt werden als die offizielle JSON-Spezifikation vorgibt19. In JSON wird ein Datenobjekt als eine in geschweifte Klammern eingeschlossene Liste von Schlüssel/Wert-Paaren notiert:

{

"key_1": member_1,

...

"key_N": member_N

}

wobei key_1, . . ., key_N die Schlüsselnamen und member_j die Werte, also die eigentlichen Daten darstellen. Als Schlüssel sind hier beliebige, im jeweiligen Datenobjekt eindeutige Zeichenketten verwendbar, Werte können Folgendes sein:

– null,



– True/False,



– eine Zahl (Dezimal mit ".", Exponentialnotation möglich),



– eine Zeichenkette (in Anführungszeichen),



– ein eingebettetes JSON-Objekt,



– eine Liste, die ihrerseits als [o_1, o_2, ..., o_N] notiert wird, wobei die o_k jeweils Elemente dieser Aufzählung sind.





Der Datentyp ist also rekursiv, da sowohl das JSON-Objekt als als auch die JSON-Liste Verschachtelungen erlauben. Als Beispiel betrachten wir Verlustdaten eines Versicherers infolge von Naturkatastrophen. Ein Beispieldatensatz könnte etwa so aussehen:

{

"event_id": "dkjtz8989_2014",

"event_type": "Winterstorm",

"event_date": "2014-01-16",

"losses": [

{"policy_id": 22289378, "loss": 484200},

{"policy_id": 3873829, "loss": 1232000},

{"policy_id": 13122122, "loss": 27600, "subrogation": true}

],

"valid-at": "2014-02-25"

}

Die folgenden Zeilen konvertieren einen als JSON-Zeichenkette vorliegenden Datensatz in ein Python-Dictionary:

json_str = """ {

"event_id": "dkjtz8989_2014",

"event_type": "Winterstorm",

"event_date": "2014-01-16",

"losses": [ {"policy_id": 22289378, "loss": 484200},

{"policy_id": 3873829, "loss": 1232000},

{"policy_id": 13122122, "loss": 27600, "subrogation": true}

],

"valid-at": "2014-02-25"

}"""

import json

obj = json.loads(json_str)

bzw. eine R-Liste unter Verwendung des Packages jsonlite

json_str <- ' {

\"event_id\": \"dkjtz8989_2014\",

\"event_type\": \"Winterstorm\",

\"event_date\": \"2014-01-16\",

\"losses\": [

{\"policy_id\": 22289378, \"loss\": 484200},

{\"policy_id\": 3873829, \"loss\": 1232000},

{\"policy_id\": 13122122, \"loss\": 27600, \"subrogation\": true}

],

\"valid-at\": \"2014-02-25\"

}'

obj <- jsonlite::fromJSON(json_str)

Auf die eingelesene Information kann man dann direkt zugreifen, z.B. ergibt obj[' losses'][2]['policy_id'] (bzw. in R obj$losses$policy_id[3]) jeweils den Wert 13122122. Die unterschiedliche Syntax für den Zugriff in Python/R hat mit den unterschiedlichen Datenstrukturen zu tun, in welche die jeweilige Library die JSON-Struktur genau überträgt und damit, dass Arrayindexierung in Python generell bei 0 und in R mit 1 beginnt. Für beide Sprachen gibt es verschiedene Libraries, die einen JSON-String parsen und in eine Datenstruktur der Sprache übertragen können. Will man aus dem Beispiel eine flache Tabelle erzeugen, wie sie üblicherweise für Anwendungen des maschinellen Lernens benötigt wird, so ist zunächst zu überlegen, wie die verschachtelten Objekte aufgelöst werden können. Für das betrachteten Beispiel löst zum Beispiel der Pandas DataFrame-Konstruktor pd.DataFrame(obj) die eingebettete Liste gleich sinnvoll auf:


Tab. 5.3:Datensatz nach Auflösung der Liste.



event_date event_id event_type losses valid-at

0 2014-01-16 dkjtz8989_2014 Winterstorm {’loss’: 484200, ’policy_id’: 22289378} 2014-02-25

1 2014-01-16 dkjtz8989_2014 Winterstorm {’loss’: 1232000, ’policy_id’: 3873829} 2014-02-25

2 2014-01-16 dkjtz8989_2014 Winterstorm {’subrogation’: True, ’loss’: 27600, ’policy_id’: 13122122} 2014-02-25



Die in die Liste eingebetten JSON-Objekte wurden jedoch in einem Feld übernommen, so dass auf die Informationen darin nicht direkt zugegriffen werden kann. Eine in diesem einfachen Fall funktionierende Variante ist es, die Liste selbst zunächst in einen DataFrame zu übertragen und diesen dann mit demjenigen für das Objekt zu verknüpfen, etwa durch den Befehl

pd.DataFrame(obj).drop("losses", axis=1).join(pd.DataFrame(obj['losses'])),

der dann Folgendes zurückliefert:


Tab. 5.4:Denormalisierter DataFrame.



event_date event_id event_type valid-at loss policy_id subrogation

0 2014-01-16 dkjtz8989_2014 Winterstorm 2014-02-25 484 200 22289378 NaN

1 2014-01-16 dkjtz8989_2014 Winterstorm 2014-02-25 1 232 000 3873829 NaN

2 2014-01-16 dkjtz8989_2014 Winterstorm 2014-02-25 27 600 13122122 True



Über das Vorgehen in solchen Fällen wird man von Fall zu Fall entscheiden müssen, die gezeigte Denormalisierung20 muss auch nicht immer sinnvoll sein.


5.2.2 Datenabfragen von Web-APIs


Wir werfen einen kurzen Blick auf zwei Beispiele, in denen Daten über Web-APIs bezogen werden. Der Begriff Web-API steht heutzutage für eine durch einen Server bereitgestellte Schnittstelle, die über HTTP-Anfragen von anderen Softwaresystemen (in diesem Zusammenhang dann Clients genannt) angesprochen werden kann.

Das erste Beispiel benutzt die Alpha Vantage API21, über die bestimmte Kapitalmarktdaten bezogen werden können. Zur Benutzung ist es erforderlich, sich einen API-Key zu generieren, vgl. dazu die Angaben auf der Homepage des Services. Damit sind dann beispielsweise Abfragen von aktuellen Wechselkursen möglich. Für die Abfragen selbst nutzen wir im Beispiel die Python-Bibliothek requests, die hierfür natürlich zu installieren ist.

import requests

def make_api_call(from_currency = "BTC", to_currency = "CNY",

api_key = "demo"):

""" Funktion laedt Wechselkurse von AV-API. """

curr_exch_url = "https://www.alphavantage.co/query" +\

"?function=CURRENCY_EXCHANGE_RATE&" +\

"from_currency={}&to_currency={}&apikey={}".\

format(from_currency, to_currency, api_key)

return requests.get(curr_exch_url).json()

make_api_call("USD", "EUR", "MY_API_KEY")

Hier haben wir die Abfrage gleich in einer Funktion gekapselt. Als Ergebnis erhält man ein Python-Dictionary, in dessen Feld '5. Exchange Rate' der gesuchte Wechselkurs vorzufinden ist:

{'Realtime Currency Exchange Rate': {'1. From_Currency Code': 'USD',

'2. From_Currency Name': 'United States Dollar',

'3. To_Currency Code': 'EUR',

'4. To_Currency Name': 'Euro',

'5. Exchange Rate': '0.88287400',

'6. Last Refreshed': '2018-08-17 16:36:20',

'7. Time Zone': 'UTC'}}

Als weiteres Beispiel betrachten wir die Google Geocoding API. Damit kann man sich u.a. die latitude/longitude Koordinaten zu gegebenen Adressen anzeigen lassen. Wir betrachten ein Beispiel:

import requests

address = "K\"oniginstra\"se 107, M\"unchen"

baseurl = "https://maps.googleapis.com/maps/api/geocode/json"

url = "{}?address={}".format(baseurl, address)

reply = requests .get(url).json()

Vor Kurzem hat Google die hier gezeigte öffentliche Abfrage an die API ohne Registrierungsschlüssel leider unterbunden. Die Antwort lautet

{'results': [{'address_components': [{'long_name': '107',

'short_name': '107',

'types': ['street_number']},

{'long_name': 'K\"oniginstra\"se',

'short_name': 'K\"oniginstra\"se',

'types': ['route']},

{'long_name': 'Schwabing-Freimann',

'short_name': 'Schwabing-Freimann',

'types': ['political', 'sublocality', 'sublocality_level_1']},

{'long_name': 'M\"unchen',

'short_name': 'M\"unchen',

'types': ['locality', 'political']},

{'long_name': 'Oberbayern',

'short_name': 'Oberbayern',

'types': ['administrative_area_level_2', 'political']},

{'long_name': 'Bayern',

'short_name': 'BY',

'types': ['administrative_area_level_1', 'political']},

{'long_name': 'Germany',

'short_name': 'DE',

'types': ['country', 'political']},

{'long_name': '80802', 'short_name': '80802', 'types': ['postal_code']}],

'formatted_address': 'K\"oniginstra\"se 107, 80802 M\"unchen, Germany',

'geometry': {'bounds': {'northeast': {'lat': 48.156728, 'lng': 11.5887816},

'southwest': {'lat': 48.1558867, 'lng': 11.5874051}},

'location': {'lat': 48.1562679, 'lng': 11.5879099},

'location_type': 'ROOFTOP',

'viewport': {'northeast': {'lat': 48.1576563302915,

'lng': 11.5894423302915},

'southwest': {'lat': 48.1549583697085, 'lng': 11.5867443697085}}},

'place_id': 'ChIJw_WUi751nkcRpEYxJ2gwFFQ',

'types': ['premise']}],

'status': 'OK'}

Aus der Struktur kann man sich nun die interessierenden Daten extrahieren, indem auf die entsprechenden Objekt-Member zugegriffen wird.


5.2.3 Daten aus Webseiten


Den Inhalt von Webseiten zu lesen ist bereits auf der syntaktischen Ebene schwierig, weil real existierende HTML-Seiten vor Syntaxfehlern oft nur so strotzen (z.B. vergessene oder falsch platzierte schlieẞende Tags). Der Webseitenbetrachter bemerkt davon meist nichts, weil die Browser die Fehler so gut es geht kaschieren. Will man jedoch zu Fuẞ oder über einen der vielen XML-Parser die Inhalte verarbeiten, stöẞt man oft schnell auf Probleme.

Abhilfe schaffen spezialisierte Bibliotheken wie JSoup (Java)22 oder Beautiful Soup (Python)23 die – ebenso wie die Webbrowser – eine Fehlertoleranz gegenüber falschem HTML an den Tag legen. Als Beispiel für ihren Einsatz wollen wir die Überschriften der Artikel auf www.spiegel.de auslesen. Mit Beautiful Soup können wir so vorgehen:

import requests

from bs4 import BeautifulSoup

html = requests.get("http://www.spiegel.de"). content

soup = BeautifulSoup(html, 'html.parser')

header_tags = soup.find_all(class_="headline")

headers = [h.contents if len(h)>0 else "" for h in header_tags]

Nach dem Laden der Seite in die Variable html wird diese an ein Objekt namens soup des Typs BeautifulSoup übergeben, welches Methoden zum Analysieren der Datenstruktur bereitstellt. Das Beispiel selektiert dann alle Objekte im HTML-Baum, die ein Attribut class="headline" aufweisen. Um auf diese Weise eine interessierende Information aus einer Webseite zu erhalten, muss also etwas über deren Struktur in Erfahrung gebracht werden, was im Allgemeinen auf ein Quellcodestudium hinausläuft. Dabei ist damit zu rechnen, dass diese Struktur irgendwann einmal geändert wird und die Extraktion dann nicht mehr funktioniert.

Das Ergebnis des Code-Fragments oben könnte etwa folgendermaẞen aussehen und kann dann in Python weiter verarbeitet werden:

['',

['Noch verdammt viel Arbeit'],

['Gewaltsame Proteste und\xa0Tote am Gazastreifen'],

['Als Aschwak Talo ihren IS-Peiniger in Schwäbisch-Gmünd wiedererkannte'],

['Der Tod lauert im Tragwerk'],

['Ursache für Brückeneinsturz war womöglich gerissenes Stahlseil'],

['"Eltern schreiben ihren Kindern weniger vor"'],

['Warum die Tourismusbranche auf "Adults only" setzt'],

['Wir müssen drauen bleiben'],

['Wie eine Behörde Hunderte fragwürdige Asylbescheide ausstellen konnte'],

['Mutmalicher Taliban-Elitekämpfer angeklagt'],

['Die brisante Folie "2"'],

Ein etwas anderes Vorgehen ist meistens notwendig, wenn die Inhalte von Webseiten dynamisch nachgeladen werden. Damit ist der inzwischen sehr häufig auftretende Fall gemeint, dass eine Webseite letztlich nur als ein Strukturgerüst konzipiert wird und nach dem Laden der Seite durch den Browser der mitgelieferte JavaScript-Code die konkreten Inhalte erst nachlädt (möglicherweise in Abhängigkeit einer Login-Information oder von einem gespeicherten Nutzerprofil). Hier greift die Analyse des statischen Contents der HTML-Seite zu kurz, weil die eigentlich interessierenden Daten gar nicht vorhanden sind. Eine Möglichkeit, um an dieser Stelle dennoch weiterzukommen, besteht darin, die Verarbeitungsschritte der Browser (wie das Verfolgen von Redirect- Request, das Mitsenden von Cookies in Anfragen sowie maẞgeblich das Nachladen von verlinkten Inhalten und die Ausführung von JavaScript-Code) nachzuvollziehen und erst wenn dies erfolgt ist, den Seiteninhalt zu untersuchen. Auch hierfür gibt es Lösungen, mit denen man sich bei Bedarf genauer auseinander setzen sollte (z.B. das Tool Selenium24, das eine eingebettete Browser-Runtime (z.B. Firefox) mit sich führt und Zugriffe darauf ermöglicht).


5.2.4 Daten in Amazons S3-Object Storage


S3 ist ein Angebot aus der Kategorie Object Storage in der Amazon-Cloud AWS (Amazon Web Services)25. Vergleichbare Angebote gibt es auch von anderen groẞen und kleineren Cloudanbietern, deren Ziel es teilweise sogar ist, eine mit S3 kompatible API anzubieten. Das Konzept des Object Storage erlaubt es, beliebige binäre Daten (Objekte) zusammen mit einem begrenzten Umfang an Metadaten (einer Menge von Schlüssel-Wert-Paaren) unter einem Schlüssel (key) zu speichern. In S3werden die Objekte zusätzlich jeweils in einem bucket (eine Art Ordner auf höchster Ebene) abgelegt. Wir illustrieren mit diesem Fall den Zugriff über eine programmatische Bibliothek, die ein komplexes Zugriffsprotokoll implementiert und dieses – zumindest in dem einfachen Fall – vor dem Nutzer verbirgt.

Um das folgende Beispiel nachzuvollziehen sind einige vorbereitende Schritte notwendig, die man im Detail auf den Internet-Seiten von Amazon Web Services nachlesen kann, dazu gehören:

– einen AWS-Account anlegen,



– ein bucket anlegen,



– eine Gruppe anlegen,



– einen User anlegen und Credentials herunterladen,



– Credentials in die (anzulegende) Datei ~/.aws/credentials eintragen,



– das Python-Modul Boto3 mit pip install boto3 installieren.





Nach Abschluss der Vorbereitungen, kann man über die Python-Console programmatisch auf die Daten zugreifen und sich z.B. die vorhandenen buckets anzeigen lassen:

D:\programming\py\aws_access>ipython

Python 3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:25:24)

Type 'copyright', 'credits' or 'license' for more information

IPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import boto3

In [2]: s3 = boto3.resource('s3')

In [3]: for bucket in s3.buckets.all():

...: print(bucket.name)

...:

adsdemo.ms

Die Ausgabe zeigt, dass genau ein Bucket mit dem Namen adsdemo.ms vorhanden ist. Es ist nun auch leicht möglich, mit Skripten Daten in S3 zu speichern oder sie von dort herunterzuladen. Das folgende Codefragment beispielsweise lädt die lokale Datei lokale_datei.txt nach S3 hoch und macht sie allgemein lesbar:

file_name = "lokale_datei.txt"

s3_client = boto3.client('s3')

s3_client.upload_file(file_name,

"adsdemo.xyz",

file_name,

ExtraArgs={'ACL':'public-read'})

Eine denkbare Anwendung könnte beispielsweise so aussehen, dass eine Anwendung Ressourcen (z.B. Bilder) in S3 hostet, die bei Bedarf von dort auf einen Client (Webseite, App, ...) geladen werden können, wodurch die Skalierungseffekte der Cloud genutzt und Kapazitäten auf dem eigenen Server gespart werden können.


5.3 Arbeiten mit Relationalen Datenbanken


Die relationalen Datenbanksysteme stellen das wichtigste System zur Datenhaltung im Versicherungsunternehmen dar und nahezu alle operativen Systeme (vgl. Abschnitt 5.10) nutzen sie für die Datenhaltung. Wir geben u.a. eine allgemeine Einführung in Datenbanksysteme, in die Datenmodellierung und die Abfragesprache SQL.


5.3.1 Der Nutzen von (relationalen) Datenbanken


Die anwendungsspezifische Verwaltung von Daten durch jeweils eine Applikation war lange Zeit die Regel, wodurch im Routinebetrieb eine hohe Effizienz erreicht werden konnte. Die Applikation schreibt und liest alle benötigten Daten exklusiv direkt im Dateisystem. In diesem Szenario führt der Austausch von Daten zwischen verschiedenen Applikationen über Ex- bzw. Importe und als Zwischenschritt ist u.U. noch eine Konvertierung erforderlich, vgl. Abbildung 5.3.


Abb. 5.3: Datenmanagement ohne zentralisierte Datenbank, in Anlehnung an Jarosch ([44]), Seite 15.

Neben hoher Effizienz und der Vermeidung von externen Abhängigkeiten der Applikationen hat die exklusive Datennutzung aber auch Nachteile:

– Daten, die von mehreren Anwendungssystemen benötigt werden, werden ggf. mehrfach gespeichert (redundante Datenspeicherung)



– Notwendigkeit von Konverter-Programmen, um Daten zwischen verschiedenen Anwendungen austauschen zu können



– Änderungen in der Datenstruktur erzwingen Umprogrammierung der Anwendungen (abhängig von der physischen Datenstruktur)



– Exklusive Nutzung durch eine Anwendung, d.h. keine parallelen Zugriffe



– Jedes Anwendungssystem sorgt selbst für die Integrität seiner Daten (i.A. kein allgemeiner Mechanismus zur Datenkonsistenz/Zugriffsschutz)





Der letztgenannte Punkt impliziert natürlich auch, dass die entsprechenden nichttrivialen Mechanismen des Datenmanagements für jede Applikation von Grund auf neu konzipiert und erstellt werden müssen. Um diesen Problemen zu begegnen, wurde in den 60er Jahren das Konzept von zentralisierten Datenspeichern entwickelt, sogenannten Datenbanken (DB). Eine Datenbank ist eine Sammlung von strukturierten Daten, zwischen denen sachlogische Zusammenhänge bestehen.

Zugriffe auf die Datenbank erfolgen über ein spezielles Softwaresystem, ein Datenbank-Managementsystem (DBMS). Letzteres ist ein Programm, das die notwendige System-Software für alle Aspekte der Datenverwaltung bereitstellt (Abbildung 5.4). Umgangssprachlich wird allerdings oft nicht zwischen den Daten (Datenbank) und der Verwaltungssoftware (Datenbankmanagementsystem) unterschieden.


Abb. 5.4: Mehrere Anwendungen greifen auf eine zentrale Datenbank zu, Bild in Anlehnung an Jarosch ([44]), Seite 18.

Es gibt verschiedene Ansätze zur Datenspeicherung in Datenbanken, der bei weitem populärste wird durch die Relationalen Datenbanksysteme umgesetzt. Der Begriff Relation steht in diesem Fall für eine zweidimensionale Tabelle (Spalten und Zeilen) und leitet sich aus dem entsprechenden mengentheoretischen Begriff ab. Die Gesamtheit der überhaupt möglichen Einträge (oder Zeilen) einer Tabelle mit k Spalten C1, . . . , Ck ist demnach das kartesische Produkt



der Wertemengen Mj der Spalten (j = 1, . . . , k). Eine reale Tabelle ist dann eine Teilmenge R ⊆M1 × M2 ×· · · × Mk, was einer mathematischen Relation entspricht. Die Begriffsbildung basierend auf mengentheoretischen Konzepten deckt gewisse Analogien zur Praxis ab, aber beispielsweise das Vorhandensein mehrerer identischer Zeilen in einer Tabelle ist nicht direkt erfasst.

Normalerweise beinhaltet eine Datenbank nicht nur eine, sondern mehrere Tabellen und die Daten darin stehen in bestimmten Verhältnissen zueinander. Die Aufgabe, ein reales Problem auf ein solches Tabellenmodell abzubilden, wird Datenmodellierung genannt. Der Kern der (relationalen) Datenmodellierung ist es, zunächst denjenigen Ausschnitt der Realität zu umreiẞen, über den Daten gesammelt werden sollen und dann die darin enthaltenen Objekte (Entities) mit den kontextrelevanten Eigenschaften sowie die notwendigen Beziehungen (Relationships) zwischen den Entitäten zu bestimmen.

Oft wird die Datenmodellierung mit grafischen Hilfsmitteln durchgeführt, eine weit verbreitete Methode mit mehreren gebräuchlichen Varianten sind die sogenannten Entity-Relationship-Diagramme. Wir wollen im Folgenden die eingeführten Begriffe anhand eines Beispiels genauer verdeutlichen.


Beispiel: Für ein Versicherungsunternehmen (VU) wird ein einfaches System zur Vertragsverwaltung für Eigen- und Fremdverträge modelliert. Kunden und Interessenten des Unternehmens sollen mit ihren Daten erfasst werden. Für jede Person sollen Vorname, Name und Geburtsdatum gespeichert werden. Weiter soll es möglich sein, einer Person einen Ehepartner sowie mehrere Kinder zuzuordnen, die ebenfalls in der Datenbank erfasst werden sollen.

Das VU verkauft als Produkt nur Risikounfallversicherungen. Dabei besteht jede Versicherung aus einer Vertragsnummer, einem Beginndatum, einem (monatlichen) Beitrag und einer Invaliditätssumme.

Jeder Versicherungsvertrag ist mit mindestens einem Kunden abgeschlossen, und Kunden können mehrere Versicherungsverträge abschlieẞen. Zusätzlich können für Kunden und Interessenten Fremdverträge gespeichert werden. Jeder Fremdvertrag besitzt eine laufende Nummer, eine Versicherungsnummer, eine Versicherungsgesellschaft, eine Art, einen monatlichen Beitrag, ein Beginn- und ein Enddatum.





Es soll nun ein Modell für dieses Beispiel entworfen werden, indem wir formal von realen Objekten zum Datenmodell übergehen wie in Abbildung 5.5 konzeptionell veranschaulicht.


Abb. 5.5: Übergang von realer Welt zu modellierten Entitäten.


5.3.2 Objekte und Objekttypen in relationalen Datenbanken


Bevor wir ein formales Datenmodell für das Beispiel aus dem vorausgegangenen Abschnitt erstellen, sollen in diesem Abschnitt die notwendigen Konzepte noch etwas genauer vorgestellt werden.

Unter einer Entität (entity) verstehen wir ein konkretes Objekt, über das Informationen gespeichert werden. Objekte können Personen (z.B. Kunden), Gegenstände (Gebäude, Autos, . . .) oder nicht-materielle Dinge wie ein Beschäftigungsverhältnis sein. Ein Entitätstyp (entity type) ist eine eindeutig benannte Klasse von Objekten, über die strukturell identische Informationen gespeichert werden und die in prinzipiell gleicher Weise verarbeitet werden.

Neben seinem Entitätstyp wird ein Objekt durch die mit ihm assoziierten (und gespeicherten) Eigenschaften charakterisiert. Eigenschaften von Objekttypen können in identifizierende (bzw. teilidentifizierende) und beschreibende Eigenschaften eingeteilt werden. Identifizierende Eigenschaften ermöglichen die Identifizierung eines Objekts innerhalb eines Objekttyps (z.B. eine eindeutige Kundennummer). In der von uns im Folgenden verwendeten grafischen Notation werden identifizierende (bzw. teilidentifizierende) Eigenschaften durch eine Unterstreichung kenntlich gemacht.

Einige typische Objekte und deren jeweils gespeicherte Eigenschaften, wie sie bei der Datenmodellierung im Versicherungskontext (bzw. in unserem Beispiel) auftreten können, sind in Abbildung 5.6 dargestellt.

Entitäten bzw. Entitätstypen treten nicht isoliert auf, sondern sie stehen in vielfältiger Weise miteinander in Beziehung: Ein Versicherungsvertrag steht beispielsweise mit einer natürlichen Person in Beziehung, z.B. als versicherte Person (VP) oder als Versicherungsnehmer (VN). Im relationalen Modell wird jede Beziehung als eine Menge von dualen Beziehungen, das sind Beziehungen zwischen genau zwei Entitätsklassen, modelliert. Umkomplexere Beziehungen modellieren zu können, müssen hierfür dann u.U. neue Entitätstypen definiert werden, die die Beziehungen selbst repräsentieren. Auch der Spezialfall, dass Entitäten einer Klasse mit sich selbst in Beziehung stehen, ist möglich, eine solche Beziehung wird rekursiv genannt (Beispiel: ein Mitarbeiter kann der Vorgesetzte eines anderen Mitarbeiters sein).


Abb. 5.6: Einige Beispiele für Entitätstypen und der für sie relevanten Eigenschaften, (teil)identifizierende Eigenschaften sind unterstrichen.

Eine duale Beziehung kann immer von zwei Richtungen betrachtet werden: Ein Versicherungsvertrag versichert eine natürliche Person und eine natürliche Person hat einen Versicherungsvertrag abgeschlossen. Jede Richtung wird durch folgende Punkte konkretisiert:

– die Benennung der Beziehung, die beschreibt, was gespeichert wird



– die Optionalität, die definiert, ob mindestens eine Beziehung bestehen muss



– die Kardinalität, die beschreibt ob es mehrere Beziehungen geben kann





Für Entity-Relationship-Diagramme gibt es verschiedene graphische Notationen. Die hier verwendete ist die Krähenfuẞnotation, veranschaulicht in Abbildung 5.7. Eine weitere weit verbreitete Notation ist die im Jahr 1976 von P. Chen entwickelte, in der Entity-Mengen als Rechtecke und die Attribute als Kreise (bzw. Ellipsen) dargestellt werden.


Abb. 5.7: Krähenfuẞnotation für Beziehungen in Entity-Relationship-Diagrammen.

Abbildung 5.8 zeigt allgemein, in welchen Beziehungen zwei Objekte zueinander stehen können.

Von einem Objekt A zu einem Objekt B kann es in der Modellierung mehrere Beziehungen (“Brücken”) geben. Manchmal sind dann eine oder ggf. mehrere der Beziehungen überflüssig und solche Fälle nennt man redundante Beziehungstypen. In der Praxis werden Redundanzen teilweise bewusst herbeigeführt, um Suchprozesse in der Datenbank zu beschleunigen. Allerdings kann es dabei zu Problemen kommen, wie dem Verbrauch von unnötigem Speicherplatz oder der erzwungenen Änderung von Daten an mehreren Stellen, um die Konsistenz zu erhalten. Wir betrachten hierzu zwei Beispiele für verschiedene Beziehungstypen zwischen den Entitätstypen Kunde, Auftrag und Produkt:

Erteilt ein Kunde einen Auftrag (modelliert durch eine entsprechende Beziehung) und stehen die bestellten Produkte wiederum in Beziehung zum Auftrag, so ist eine weitere Beziehung vom Typ “Ein Kunde bestellt einen Artikel” redundant.



In der gleichen Situation wäre “Ein Kunde reklamiert einen Artikel” eine nicht redundante Information, die durch eine separate Beziehung im Datenmodell zu erfassen ist.





Es kommt vor, dass die Attribute einer Entität keine eindeutige Identifizierung erlauben, man spricht dann auch von schwachen Objekttypen, ein Beispiel ist in Abbildung 5.9 zu finden. Daher muss zur Identifizierung in einem solchen Fall, neben den Eigenschaften, auch auf die Beziehungen zurückgegriffen werden. Die Beziehungstyp-Richtung, die den schwachen Objekttyp mit einem anderen Objekt verbindet, wird damit zu einem identifizierenden Element für den schwachen Objekttyp (dabei handelt es sich um eine nicht-optionale Beziehung der Kardinalität 1, vom schwachen Objekttyp zu dem anderen Objekt).


Abb. 5.8: Mögliche Beziehungen zwischen zwei verschiedenen Objekttypen A und B (die Eigenschaften sind aus Vereinfachungsgründen nicht dargestellt). Zudem sind aus Symmetriegründen nur die relevanten 10 Beziehungen dargestellt. Mit den Buchstaben M und N werden Kardinalitäten gröẞer 1 bezeichnet. Zudem verdeutlicht das C die Optionalität, d.h. die Abwesenheit einer Beziehung ist zugelassen.

Eine weitere Besonderheit stellen die schon erwähnten rekursiven Beziehungstypen dar, durch die sachlogische Zusammenhänge zwischen Objekten des gleichen Objekttyps beschrieben werden. Auch hier sind die drei Punkte – Benennung, Optionalität, Kardinalität – relevant.

Das vervollständigte Datenmodell zu dem Beispiel aus Abschnitt 5.3.1 ist in Abbildung 5.10 dargestellt.


Weitere Modellierungsfälle


Die bisher betrachteten Modellierungen können als Grundgerüst angesehen werden. Jedoch gibt es Spezialfälle, die hier nicht weiter betrachtet werden können. Dazu gehören z.B. sachlogische Zusammenhänge zwischen mehr als zwei Objekten oder Eigenschaften von Beziehungstypen. Diese können mit Modifizierungen in der Modellierung mit den hier besprochenen Mitteln durchgeführt werden.


Abb. 5.9: Schwacher Objekttyp: Zur eindeutigen Identifizierung muss auf die Beziehungen zurückgegriffen werden. Die identifizierende Beziehungstyp-Richtung wird durch ein Dreieck dargestellt, welches vom schwachen Objekttyp weg zeigt.


Abb. 5.10: Datenmodell zu dem Beispiel aus Abschnitt 5.3.1.


Qualitätssicherung von konzeptionellen Datenmodellen


Die Modellierung mit ER-Diagrammen unterliegt zunächst einer gewissen Willkür, da bei komplexeren Konstellationen i.d.R. mehrere sachlich richtige Modellierungsmöglichkeiten bestehen. Daher ist es wünschenswert, einen gewissen Regelkatalog zu haben, anhand dessen sich die Modelle überprüfen lassen.

Einen Beitrag hierzu leisten die sogenannten Normalformen (siehe z.B. [44]), als Beispiel wird hier die erste Normalform angeführt: Ein Datenmodell liegt in der ersten Normalform vor, wenn seine Eigenschaften atomar sind. Eine Eigenschaft wie Adresse wird im Allgemeinen nicht atomar sein, da sie in weitere Bestandteile wie Ort, Postleitzahl usw. zerlegt werden kann. Für die Überführung in die erste Normalform müssen also nichtatomare Eigenschaften aufgebrochen werden.


5.3.3 Datenbank-Modelle: Relationale Datenbanken


Nach dem bisher Eingeführten kann durch ER-Diagramme ein konzeptionelles Datenbankmodell auf syntaktischer Ebene formuliert werden. Im nächsten Schritt soll auf der Basis desselben ein logisches Datenschema abgeleitet werden. Dabei sind die Möglichkeiten zu berücksichtigen, die das Datenbank-Managementsystem für die Strukturierung der Daten vorsieht. Diese hängen vom Datenbank-Modell ab, das dem Datenbank-Managementsystem zugrunde liegt. Ein Datenbank-Modell ist dabei ein logisches Beziehungsgebilde, durch das festgelegt wird, in welcher Weise Datensätze (durch die Objekte beschrieben werden), miteinander in Verbindung gebracht werden. Neben dem relationalen Datenbankmodell gibt es u.a. das hierarchische Datenbank-Modell und das Netzwerk-Datenbank-Modell. Im Folgenden wird jedoch nur auf das relationale Datenbankmodell eingegangen. Es wird durch folgende Punkte charakterisiert:

– Jeder Objekttyp ist ein möglicher Einstiegspunkt für die Informationssuche.



– Die einzelnen Objekttypen können durch ein beliebiges strukturiertes Netzwerk miteinander verbunden werden.



– Brücken zwischen den Dateninseln werden nicht zum Zeitpunkt der Speicherung, sondern erst zum Zeitpunkt der Informationssuche angelegt.





Das relationale Datenbankmodell wurde 1970 von Codd vorgeschlagen. Analog der beschriebenen ER-Diagramme wird im Folgenden die Strukturierung der Daten in diesem Modell beschrieben. Dann wird gezeigt, wie ER-Diagrammein dieses Modell übersetzt werden.

Den Ausgangspunkt bildet der Begriff des Attributs A, welches eine wesentliche Eigenschaft eines Objekttyps T darstellt. Für die Dokumentation der Datenstruktur werden beiden Elementen Bezeichnungen zugeordnet: Bez[A] und Bez[T].

Beispiel: Der Objekttyp T mit Bez[T] = “VVertrag” wird durch die vier Elemente A1, A2, A3 und A4 beschrieben mit

– Bez[A1] = “VertragsNr”,



– Bez[A2] = “BeginnDat”,



– Bez[A3] = “Beitragpm” und



– Bez[A4] = “InvSumme”.





Die Typbeschreibung für den Objekttyp VVertrag hat die Form:

VVertrag(VertragsNr, BeginnDat, Beitragpm, InvSumme)

Der Wertebereich (Domäne) des i-ten Attributs wird wie folgt angegeben:



Ist bei einem konkreten Objekt für ein Attribut kein Wert angebbar (z.B. ist noch keine

Dateneingabe erfolgt), wird dieses mit einer speziellen Markierung (Null) belegt.

Für ein konkretes Objekt vom Objekttyp T und Typbeschreibung Bez[T] (Bez[A1],. . .,Bez[An]) kann die Folge seiner Werte als n-Tupel angegeben werden:



Die Menge der möglichen Versicherungsverträge ist dann die Produktmenge

Dom[VertragsNr] × Dom[BeginnDat] × Dom[Beitragpm] × Dom[InvSumme].

Die Menge der tatsächlich gespeicherten n-Tupel ist wesentlich kleiner (dies ist wiederum die im Abschnitt 5.3.1 angesprochene für das Paradigma namensgebende nstellige Relation R ⊆ Dom[A1]× ···× Dom[An] über den Wertebereichen der Attribute A1, A2, ..., An).


Darstellung des relationalen Datenbankmodells als Tabellen


Die Darstellung der Typbeschreibung und der Relation kann in einfacher Weise als Tabelle graphisch dargestellt werden.


Tab. 5.5:Tabellentypbeschreibung, der Tabellentyp ist definiert durch seine Bezeichnung, sowie die Bezeichnungen und die Wertebereiche der Attribute.



Typbeschreibung Bez[T] Bez(A1) Bez(A2) . . . Bez(An)

Relation R ⊆ Dom(A1) Dom(A2) . . . Dom(An)



Der Zugriff auf einzelne Elemente der Tabelle erfolgt nicht durch hardwarenahe Adressierung (z.B. durch Adressverweise, d.h. Pointer) wie etwa im hierarchischen Datenmodell, sondern auf logischer (Tabellen-)Ebene. Ein Datensatz ist nur über spezielle Werte, genannt Schlüssel, ansprechbar (analog den ER-Diagrammen).26

Ein einfacher Schlüssel ist ein Attribut und ein zusammengesetzter Schlüssel ist eine Attribute-Kombination mit der Eigenschaft, dass jede Zeile der Tabelle eindeutig durch die Angabe des Wertes dieser Attribute (bzw. der Werte-Kombination dieser Attribute) zu identifizieren ist. Dieser Schlüssel bzw. die Kombination darf in einer Tabelle nur einmal vorkommen, und ist daher unikal.

Aus der Menge der Schlüssel für eine Tabelle wird einer als Primärschlüssel ausgezeichnet. Ein Primärschlüssel ist daher ein ausgewählter Schlüssel dieser Tabelle, der in erster Linie für den Zugriff auf die Tabellenzeilen verwendet wird. Alle anderen Schlüssel heiẞen Sekundärschlüssel. Der Primärschlüssel wird zusätzlich unterstrichen oder kursiv gesetzt.

Beispiel: Das Objekt VVertrag enthält als Primärschlüssel das Attribut VertragsNr. Damit kann das Objekt in folgender Form dargestellt werden:

VVertrag(VertragsNr, BeginnDat, Beitragpm, InvSumme)

Würde das Objekt Person durch die Felder GebDatum und Nachname identifiziert, dann ist folgende Darstellung denkbar:

Person(GebDatum, Nachname, Vorname)

Eine wichtige Rolle bei der Modellierung von Beziehungen spielen die sog. Fremdschlüssel. Fremdschlüssel sind Attribute einer Tabelle A, die einen Verweis auf eine Zeile einer anderen Tabelle B darstellen. Konkret sind also der oder die Schlüssel von B als Attribute in A aufzunehmen. In der Typbeschreibung werden die Attribute, die den Fremdschlüssel bezeichnen, wie folgt dargestellt: ⇑ Bez[A] ⇑.

Beispiel: Ein Fremdvertrag soll auf die dazugehörige Person verweisen (im Beispiel ist das genau eine Person). Hierfür wird die Personenid (Primärschlüssel in der Tabelle Person) mit in die Typbeschreibung des Objektes FVertrag aufgenommen:

FVertrag(Fid, VersNum, VersG, Art, Beitrag, BeginnDat, EndDat, ⇑ PersId ⇑)

Wenn der sachlogische Zusammenhang zwischen zwei Tabellen A und B dadurch repräsentiert wird, dass die Werte des Fremdschlüssels in A mit Werten des Primärschlüssels in B übereinstimmen, dann besagt das Gebot der referenziellen Integrität, dass entweder jeder in A auftretende Wert des Fremdschlüssels mit einem Wert des Primärschlüssels in B übereinstimmen muss oder der Fremdschlüssel in A mit der NULL-Marke belegt sein muss. Daher muss ein Verweis entweder auf ein existierendes Objekt zielen oder explizit zum Ausdruck gebracht werden, dass eben kein Verweis vorliegt. Daraus ergibt sich ein wesentlicher Unterschied zwischen Fremd- und Primärschlüssel: Keines der Attribute eines Primärschlüssels darf mit der NULL-Marke belegt werden, dagegen können die Attribute des Fremdschlüssels gemeinschaftlich mit der NULL-Marke belegt sein.

Häufig treten auch duale M: N -Beziehungstypen auf. Sie lassen sich nicht direkt in natürlicher Weise im relationalen Datenbankmodell repräsentieren, weil ein Fremdschlüssel einer Tabellenzeile nicht auf mehrere andere Tabellenzeilen verweisen kann.

Beispiel: Wir betrachten die Tabellentypen

Person(PerId, Vorname, Nachname, GebDatum),

VVertrag(VertragsNr, BeginnDat, Beitragpm, InvSumme)

In diesem Beispiel kann eine Person mehrere Versicherungsverträge abschlieẞen, und ein Vertrag kann einer oder mehreren Personen zugeordnet werden. Zur Lösung des Problems wird eine neue Tabelle eingeführt, die Personen und Versicherungsverträge einander zuordnet (Koppel-Tabelle). Diese enthält den Primärschlüssel der VVertrag-Tabelle und den Primärschlüssel der Person-Tabelle. Das führt zu folgender zusätzlicher Tabellentypbeschreibung

Person/VVertrag(⇑ PerId ⇑, ⇑ VertragsNr ⇑)

Auch die angesprochenen rekursiven Beziehungen lassen sich ebenfalls in der gezeigten Weise modellieren, da Fremdschlüssel nicht notwendig auf eine andere Tabelle verweisen müssen. In diesem Fall ist also der Fremdschlüssel ein weiteres Attribut einer Tabelle, das auf den Primärschlüssel derselben Tabelle verweist.

Beispiel: Im Kontext des Beispiels dieses Abschnittes können Kinder wie folgt durch die Abänderung des Tabellentyps Person abgebildet werden:

Person(Pid, Vorname, Nachname, GebDat, ⇑ Elternteil-Pid ⇑).

Am Ende dies Abschnittes geben wir das transformierte Modell zu dem behandelten Beispiel an:

Person(Pid, Vorname, Nachname, GebDat, ⇑ Elternteil-Pid ⇑, ⇑ Ehepartner-Pid ⇑)

FVertrag(Fid, VersG, VersNum, Art, Beitrag, BeginnDat, EndDat)

Person/FVertrag(⇑ PerId ⇑, ⇑ Fid ⇑)

VVertrag(VertragsNr, BeginnDat, Beitragpm, InvSumme)

Person/VVertrag(⇑ PerId ⇑, ⇑ VertragsNr ⇑)


5.3.4 Structured Query Language


Die Structured Query Language (SQL) ist eine standardisierte Zugriffssprache für relationale Datenbanken, die fünf Untersprachen beinhaltet:

– Abfragen: Um Zeilen von Tabellen zu selektierten, wird die Anweisung SELECT verwendet.



– Data Manipulation Language (DML): Hierzu gehören Anweisungen, die den Inhalt von Tabellen modifizieren können, das sind insbesondere



– INSERT (Zeile in einer Tabelle hinzufügen)



– UPDATE (Änderung von Zeilen)



– DELETE (Löschen von Zeilen)



– Data Definition Language (DDL): Sie dient zur Definition von Datenstrukturen, wie z.B. Tabellen. Hierzu gehören:



– CREATE (Erzeugung von Datenstrukturen, wie z.B. Tabellen oder Benutzern)



– ALTER (Modifikation von z.B. Tabellen)



– DROP (Löschen von Datenstrukturen, wie z.B. Tabellen)



– RENAME (Ändert den Namen einer Tabelle)



– TRUNCATE (Löscht alle Zeilen einer Tabelle)



– Transaction Control Language (TCL): Dies sind Anweisungen, die das transaktionale Verhalten (siehe unten) steuern und damit insbesondere die permanente Speicherung (COMMIT) bzw. die Rücksetzung von bereits erfolgten Änderungen veranlassen.



– Data Control Language (DCL): Hiermit wird das Erteilen von Berechtigungen gesteuert, z.B. durch GRANT oder REVOKE Anweisungen.





Die beiden letztgenannten Bereiche werden im folgenden Beispiel nicht benötigt.

Zugriffe auf eine Datenbank können lesend und schreibend erfolgen. Es wird zwischen den drei Zugriffsarten Abfrage (Query), Mutation und Transaktion unterschieden.

Durch eine Abfrage wird ein Ausschnitt der Datenbank ausgewählt und der Inhalt gelesen. Dabei bleibt die Datenbank selbst unverändert (SQL-Befehl: SELECT).

Bei einer Mutation wird ein Ausschnitt der Datenbank ausgewählt und der Inhalt wird geändert, gelöscht oder neuer Inhalt wird hinzugefügt (SQL-Befehle: UPDATE, DELETE und INSERT).

Transaktionen bezeichnen konsistenzerhaltende Operationen auf einer Datenbank. Sie bestehen eventuell aus mehreren Abfragen und Mutationen. Ausgehend von einer konsistenten Datenbank steht nach Ausführung der Transaktion wieder eine konsistente Datenbank zu Verfügung. Transaktionen bilden die Kernfunktionalität von Datenbanksystemen: Sobald mehr als ein Nutzer zugreift, besteht die Gefahr von Dateninkonsistenzen. Wir können sie an dieser Stelle trotzdem nur oberflächlich diskutieren.

Beispiel Banküberweisung: Es sollen 100 EUR vom Konto X auf das Konto Y überwiesen werden, wobei als Nebenbedingung vorausgesetzt wird, dass auf dem Konto X mehr als 100 EUR vorhanden sind. Schriebe das System nun zuerst Y die 100 EUR zu und stellt danach fest, dass X die nötige Deckung nicht hat, hat man ein Problem, denn es könnte einen Stromausfall geben, der in diesem Fall nach dem Hochfahren unbeabsichtigte ökonomische Auswirkungen haben könnte.

Selbst wenn X die Deckung hat, muss sichergestellt sein, dass beide Vorgänge nur gemeinsam stattfinden (oder gemeinsam scheitern). Hierbei sollen also gerade auch externe Faktoren wie Netzwerkausfall oder Serverabsturz (zu wenig Speicher o.Ä.) berücksichtigt sein. Tritt ein Fehler auf, wird ein ROLLBACK ausgelöst, der alle Aktionen der noch nicht beendeten Transaktionen rückabwickelt.

Der erfolgreiche Abschluss einer Transaktion ist der COMMIT. Erst nach dem COMMIT sind die Änderungen der Transaktion für andere Verbindungen bzw. Benutzer sichtbar (Ausnahmen gibt es in manchen Systemen wie MySQL oder MS-SQL-Server, sie werden als dirty read bezeichnet). Die Kapselung von Aktionen in Transaktionen allein deckt aber noch nicht alle möglich Probleme ab. Unter dem Stichwort transaction isolation kann man sich über die Details der Garantien informieren, welche ein interessierendes Datenbanksystem bietet. Weitere ggf. notwendige Mittel um Konsistenzerhaltung zu garantieren stellen sogenannte Locks dar, die einzelne Zeilen oder ganze Tabellen für andere Verbindungen temporär sperren. Zugriffe mit gesetzten Locks sind dann exklusiv für eine Transaktion und so können bestimmte Konflikte vermieden werden.


Das Konsistenzmodell ACID


Wesentliche Erwartungen an eine Datenbank sind die dauerhafte, sichere und konsistente Speicherung der Daten. Mehrere Benutzer sollen problemlos parallel lesend und schreibend auf die Daten zugreifen können. Diese Erwartungen motivieren die folgenden Begriffe für das Konsistenzmodell ACID (Härder und Reuter, 1983):

– Atomarität



– Konsistenz (Consistency)



– Isolation



– Dauerhaftigkeit (Durability)





Wir erläutern die Eigenschaften noch etwas genauer:

Atomarität: Jede Transaktion erfolgt atomar, d.h. zusammenhängend. Entweder wird die gesamte Transaktion vollständig ausgeführt, oder die Transaktion wird überhaupt nicht vollzogen (Wichtige SQL-Befehle: COMMIT bzw. ROLLBACK).

Konsistenz: Die Atomarität garantiert, dass Transaktionen vollständig ausgeführt werden. Dabei bleibt die Konsistenz der Datenbank gewährleistet.

Isolation: Eine Datenbank muss die einzelnen Benutzer gegenseitig schützen. Jede einzelne Transaktion soll unbeeinflusst von Seiteneffekten anderer Transaktionen ablaufen.

Dauerhaftigkeit: Das langfristige und dauerhafte sichere Ablegen der Daten ist die Basis jeder Datenbank. Nach dem Beenden einer Transaktion darf ein einmal gespeichertes Datum grundsätzlich nicht mehr verloren gehen.


5.3.5 Umsetzung in einer relationalen Datenbank


Wir illustrieren nun die praktische Umsetzung unseres Beispiels. Die folgenden R-Befehle legen einige Tabellen an. Zu beachten ist dabei, dass das Anlegen eines Fremdschlüssels nur möglich ist, falls die Tabelle, auf die er sich bezieht, bereits existiert. Wie im Abschnitt über Transaktionen beschrieben wurde, muss bei Änderungen an der Datenbank stets darauf geachtet werden, dass keine Inkonsistenzen entstehen. Die Zugriffssprache SQL unterstützt dazu den Transaktionsbetrieb, der hier aber nicht explizit zum Einsatz kommt.

Um die Beispiele in diesem Abschnitt ausführen zu können, wird eine Datenbank benötigt. Das hier präsentierte Beispiel wurde mit einer lokalen Installation von MyS-QL27 getestet, funktioniert aber auch ohne separate Installation mit SQLite28. Hierzu ist lediglich der dbConnect-Befehl anzupassen:

# install.packages("RSQLite") bzw. install.packages("RMySQL")

mydb <- dbConnect(RSQLite::SQLite(), dbname="dav_turorial.sqlite")

# bzw. dbConnect(RMySQL::MySQL(), user='dav', password='abc',

# dbname='dav', host='localhost')

Die Parameter USER, PASSWORD und DBNAME müssen natürlich angepasst werden.


Eintabellenabfragen


Als erstes erstellen wir die Tabellen aus unserem Datenmodell, wobei der Befehl CREATE TABLE zum Einsatz kommt. Bei den SQL-Schlüsselworten muss übrigens nicht auf Groẞ- bzw. Kleinschreibung geachtet werden.

# Tabelle VVertrag anlegen

dbExecute(mydb, "

CREATE TABLE VVertrag (

VertragsNr INTEGER NOT NULL,

BeginnDat DATE,

Beitragpm INTEGER,

InvSumme INTEGER,

PRIMARY KEY(VertragsNr));")

# Tabelle FVertrag anlegen

dbExecute(mydb, "

CREATE TABLE FVertrag (

Fid INTEGER NOT NULL,

VersG VARCHAR(100),

VersNum VARCHAR(50),

Art VARCHAR(100),

Beitrag INTEGER,

BeginnDat DATE,

EndDat DATE,

PRIMARY KEY(Fid));")

# Tabelle Person anlegen

dbExecute(mydb, "

CREATE TABLE Person (

PerId INTEGER NOT NULL,

Vorname VARCHAR(50),

Nachname VARCHAR(50),

GebDat DATE,

EhepartnerPid INTEGER,

ElternteilPid INTEGER,

PRIMARY KEY (PerId),

FOREIGN KEY (EhepartnerPid) REFERENCES Person(PerId),

FOREIGN KEY (ElternteilPid) REFERENCES Person(PerId));")

# Koppeltabelle Person/FVertrag anlegen

dbExecute(mydb, "

CREATE TABLE PersonFVertrag (

PerId INTEGER NOT NULL,

FId INTEGER NOT NULL,

FOREIGN KEY (PerId) REFERENCES Person(PerId),

FOREIGN KEY (FId) REFERENCES FVertrag(FId));")

# Koppeltabelle Person/VVertrag anlegen

dbExecute(mydb, "

CREATE TABLE PersonVVertrag (

PerId INTEGER NOT NULL,

VertragsNr INTEGER NOT NULL,

FOREIGN KEY (PerId) REFERENCES Person(PerId),

FOREIGN KEY (VertragsNr) REFERENCES VVertrag(VertragsNr));

")

# Alle Tabellen der Datenbank auflisten

dbListTables(mydb)

Wird SQLite verwendet, sollte nach dem Durchlauf eine neue Datenbankdatei im Arbeitsverzeichnis (zu finden mit getwd()) erstellt worden sein und die Ausgabe der vorletzten Anweisung sollte die Namen der fünf erstellten Tabellen geliefert haben. Das Beispiel zeigt die Erstellung von Tabellen über Angabe des Namens und der Definition, wobei hier in der Definition der Spalten neben dem Spaltennamen und dem Datentyp auch sogenannte Constraints zum Einsatz kommen (z.B. NOT NULL). Man beachte, dass die Definition auch die Anlage von Primär- und Fremdschlüsseln einschlieẞt.

Zu erwähnen ist hier auch, dass das verwendete R-Paket RSQLite standardmäẞig den sog. autocommit-Modus verwendet. Hierbei wird jedes Statement implizit in einer eigenen Transaktion ausgeführt. Sollen mehrere Anweisungen in einer Transaktion ablaufen, so müssen sie in die Befehle dbBegin(mydb) und dbCommit(mydb) eingeschlossen werden.

Im nächsten Schritt sollen einige Beispieldaten in die Datenbank eingespielt werden. Dabei kommt der SQL-Befehl INSERT (Mutation, DML) zum Einsatz. Es können damit ein oder auch gleich mehrere neue Tupel in einer Anweisung zu einer bestehenden Relation hinzugefügt werden. Die vereinfachte Syntax des Befehls ist

INSERT INTO Tabellenname [ (Spaltenliste) ] { VALUES (Auswahlliste)

[,...] | SELECT-Befehl }

Wir fügen nun einige Daten in die soeben angelegten Tabellen ein:

# Anlegen der Personen

dbExecute(mydb, "

INSERT INTO Person (PerId, Vorname, Nachname,

GebDat, EhepartnerPid, ElternteilPid) VALUES

(1012, 'Heinz', 'Meier', '1960-06-01', NULL,NULL),

(1013, 'Laura', 'Hitz', '1978-05-13', NULL, NULL),

(1014, 'Thomas', 'Hitz', '1978-12-06', 1013, NULL),

(1015, 'Nina', 'Hitz', '2005-05-13', NULL, 1013),

(1016, 'Jonas', 'Hitz', '2008-06-17', NULL, 1013),

(1017, 'Tim', 'Wasser', '1965-08-04', NULL, NULL),

(1020, 'Johann', 'Stein', '1958-06-01', NULL, NULL)")

# Anlegen der Fremdverträge

dbExecute(mydb, "

INSERT INTO FVertrag (Fid, VersNum, VersG, Art, Beitrag, BeginnDat, EndDat)

VALUES

(5156, '56668976', 'Jumbo AG', 'Haftpflicht', 10,

'2015-08-01', '2018-08-01'),

(5157, '56668978', 'Jumbo AG', 'Lebensversicherung', 300,

'2000-04-01', '2023-04-01'),

(5310, 'AS00982341', 'Taxo AG', 'Rechtsschutz', 15,

'2016-10-01', '2019,10-01')")

# Koppeltabelle Person/FVertrag

dbExecute(mydb, "

INSERT INTO PersonFVertrag(PerId, FId) VALUES

(1012, 5156),

(1012, 5157),

(1017, 5310)")

# Eigenverträge

dbExecute(mydb, "

INSERT INTO VVertrag(VertragsNr, BeginnDat, Beitragpm, InvSumme) VALUES

(3912, '2015-05-01', 20, 120000),

(3915, '2017-04-23', 40, 80000),

(3320, '2010-05-01', 35, 200000) ")

# Koppeltabelle Person/VVertrag

dbExecute(mydb, "

INSERT INTO PersonVVertrag(PerId, VertragsNr) VALUES

(1012, 3912),

(1013, 3915),

(1014, 3915),

(1015, 3915),

(1016, 3915),

(1020, 3320) ")

Nach dem Einfügen sollten die Daten in der Datenbank stehen, vgl. Abbildung 5.11, worüber man sich durch Abfragen oder die Benutzung eines externen Datenbank-Viewers29 vergewissern kann.


Abb. 5.11: Personen-Tabelle in der Datenbank nach der Einfüge-Operation.

Die normalerweise bei weitem häufigste Anweisung ist der SELECT-Befehl. Sie dient zur Suche und Abfrage von Daten in einer Datenbank. Es gibt viele Varianten, die hier natürlich nicht alle besprochen werden können. Der Hauptteil des SELECT-Befehls sieht in etwa so aus:

SELECT [ ALL | DISTINCT] Spaltenauswahlliste

FROM Tabellenliste [ WHERE Bedingung] [ GROUP BY Spaltenliste] [

HAVING Bedingung]

Damit enthält jeder SELECT-Befehl mindestens die Schlüsselwörter SELECT und FROM. Auf FROM folgen Tabellennamen, aus denen Daten zu lesen sind. Im Abfrageergebnis werden die in der Spaltenauswahlliste angegebenen Spalten zurückgegeben.

# Beispiel: Abfrage aller Personen in der Tabelle Person

rs = dbGetQuery(mydb, "SELECT * FROM Person")

Das *-Symbol ist hierbei ein Platzhalter und steht für alle Spalten. Das Ergebnis steht jetzt als R-DataFrame zur Verfügung. Will man ausgewählte Datensätze erhalten, kann man sie über Kriterien in der WHERE-Klausel filtern:

# Beispiel mit Filterung

rs = dbGetQuery(mydb, "SELECT * FROM Person WHERE PerId = 1017")

Die bisher gezeigten Beispiele greifen nur auf eine Tabelle zurück. Die FROM-Klausel ermöglicht jedoch die Angabe einer Liste von Tabellen, die dann mittels Kreuzprodukt oder Verbund (s.u.) die Datengrundlage für die Abfrage bilden.

# Beispiel einer Kreuzreferenz

rs = dbGetQuery(mydb, "

SELECT

V1.VertragsNr AS V1,

V2.VertragsNr AS V2

FROM VVertrag V1, VVertrag V2")

In dieser Abfrage werden zunächst in der FROM-Klausel Aliasnamen für die Tabellen eingeführt, die dann anschlieẞend im SELECT-Teil verwendet werden. Dies ist in diesem Fall auch notwendig, um Eindeutigkeit herzustellen. Auẞerdem werden dann auch noch Alias-Namen für die Spalten in der Ergebnismenge eingeführt. In R-Studio wird das Ergebnis wie folgt in Abbildung 5.12 gezeigt dargestellt.


Abb. 5.12: Kreuzprodukt der Vertragsnummern.

In SQL-Abfragen können Operationen auf Spalten angewendet werden, dies können arithmetische Operationen oder auch Operationen auf Zeichenketten sein, wie z.B.

– UPPER (wandelt Kleinbuchstaben in Groẞbuchstaben)



– LOWER (wandelt Groẞbuchstaben in Kleinbuchstaben)



– TRIM (führende und schlieẞende Leerzeichen entfernen)



– RTRIM (schlieẞende Leerzeichen entfernen)



– SUBSTRING (Extraktion einer Teilzeichenkette)





Daneben sind auch noch folgende Aggregationsfunktionen vorhanden:

– AVG (Mittelwert)



– COUNT (Anzahl aller Zeilen)



– MAX (Maximumwert)



– MIN (Minimumwert)



– SUM (Summenwert)





Das folgenden Beispiel kombiniert beide Typen: Zunächst wird der Beitrag eines jeden Datensatzes mit zwölf multipliziert und dann liefert die Aggregatfunktion SUM die Summe aller Jahresbeiträge über alle selektierten Datensätze:

SELECT SUM(Beitragpm*12) AS Beitragpa

FROM VVertrag

Will man nicht über alle sondern über gewisse Teilmengen summieren, so bedient man sich der GROUP BY-Klausel.


Joins


Die Tabellenreferenz in der FROM-Klausel wird nun um den Joinausdruck erweitert. Dieser hat folgende vereinfachte Form:

Tabellenreferenz { [ NATURAL ] [INNER] | [ NATURAL {

LEFT | RIGHT } [ OUTER ] } JOIN Tabellenreferenz [ ON Bedingung |

USING { SPALTENLISTE } ]

Wir betrachten als erstes ein Beispiel für einen INNER-Join. Hier werden zwei Tabellen unter Berücksichtigung von einer oder mehreren Bedingungen verknüpft, die die Ergebnismenge des Kreuzproduktes einschränken. Das Ergebnis besteht beim INNER-Join genau aus den Zeilen, die die Bedingungen erfüllen (und somit fallen Zeilen aus einer der Tabellen, zu denen es keine passende Zeile in der zweiten Tabelle gibt, weg). Das folgenden Beispiel selektiert aus allen Versicherungsverträgen die Versicherungssumme und zeigt sie zusammen mit dem Namen des Versicherten an:

SELECT z.Vorname, z.Nachname, v .InvSumme

FROM (

SELECT p.Vorname, p.Nachname, v.VertragsNr

FROM Person p INNER JOIN PersonVVertrag v

WHERE p.PerId = v.PerId) z INNER JOIN VVertrag v

WHERE z.VertragsNr = v.VertragsNr

Anstelle eines verschachtelten Joins und der temporären Tabelle z können auch alle Tabellen direkt verwendet werden:

SELECT p.Vorname, p.Nachname, v.InvSumme

FROM Person p INNER JOIN PersonVVertrag pv INNER JOIN VVertrag v

WHERE (p.PerId = pv.PerId) AND (pv.VertragsNr = v.VertragsNr)

Der LEFT OUTER-Join wird so gebildet, dass zusätzlich zum Ergebnis des INNER-Join auch alle Sätze des linken Join-Partners erhalten bleiben, zu denen es keinen passenden Eintrag in der rechten Tabelle gibt. Sind Spalten aus der rechten Tabelle in der SELECT-Spaltenliste, wird hier die NULL-Marke geliefert. Entsprechend umgekehrt wirkt der RIGHT OUTER-Join. Der OUTER-JOIN ist dann schlieẞlich die Vereinigungsmenge aus LEFT und RIGHT JOIN.


5.3.6 Fortgeschrittene Themen zu Relationalen Datenbanken


Indizes


Ein Mittel zur Erhöhung der Performanz von Datenbankabfragen können Indizes sein. Zum Verständnis mache man sich klar, dass Daten in relationalen Datenbanken grundsätzlich nicht sortiert sind, wodurch das Einfügen neuer Daten vereinfacht wird. Gleichzeitig entsteht natürlich ein Nachteil, wenn gezielt nach Datensätzen gesucht werden muss.

Um diesen Nachteil auszugleichen, kann auf einer Tabelle ein Index (oder auch mehrere) als zusätzliche Struktur angelegt werden. Er wird in der Regel wie eine eigenständige sortierte Tabelle intern vom Datenbanksystem angelegt und verwaltet und er kann somit auch wieder entfernt werden.

Bei der Suche nach Datensätzen kann die Datenbank dann über die schnelle Auswertung des sortierten Index die Zeilen aus der Basistabelle identifizieren, die gelesen werden müssen. Ein Index wird in SQL wie folgt angelegt:

CREATE [Unique] INDEX

Name ON Tabellenname ( { Spalte [ASC | DESC ] } [ , ... ] )

Oft legen Datenbanksysteme automatisch Indizes für die Primärschlüssel einer Tabelle an.

Als Beispiel soll in der Tabelle Person auf den Nachnamen ein Index in aufsteigender Reihenfolge (Voreinstellung) angelegt werden. Das Kommando dazu sieht nun wie folgt aus:

CREATE INDEX IPersonen On Person(Nachname)





Logische Abarbeitungsreihenfolge von SQL-Statements


Im Gegensatz zu etwa R verfolgt die Programmiersprache SQL einen deklarativen Ansatz: Die SQL-Statements beschreiben das gewünscht Ergebnis, wie es erstellt wird, entscheidet das Datenbanksystem. Das bedeutet natürlich auch, dass die Abarbeitung der einzelnen Teilausdrücke in einer SQL-Abfrage nicht in der Reihenfolge passiert, in der sie in der Abfrage erscheinen. Es gibt jedoch eine logische Abarbeitungsreihenfolge, die durch den SQL-Standard vorgegeben wird. Die Einhaltung des SQL-Standards ist nicht immer garantiert, so dass einzelne Datenbanksysteme die einzelnen Schritte möglicherweise dennoch in einer anderen logischen Reihenfolge durchführen. Die logische Abarbeitungsreihenfolge ist nicht zwingend intuitiv. Das im Hinterkopf zu behalten kann helfen, falls man über nicht akzeptierte oder nicht (wie erwartet) funktionierende SQL-Statements nachdenkt.

Die Implementierung selbst bestimmt die physikalische Abarbeitungsreihenfolge. Sie ist dem Systembenutzer in der Regel nicht bekannt und muss dies auch nicht sein, solange garantiert ist, dass sie das im Sinne der logischen Abarbeitung richtige Ergebnis liefert. Allerdings ist anzunehmen, dass die physikalische Abarbeitung entscheidend für die Performanz des Systems ist.

Die logische Reihenfolge definiert Einzelschritte, die – wenn sie aufeinander folgend ausgeführt würden – das Abfrageergebnis generieren.30 Von den bislang besprochenen Konstrukten kann zusammenfassend gesagt werden, dass die FROM-Klausel am Anfang, die WHERE-Klausel im Anschluss und die am Anfang des Statements stehende Spaltenselektion innerhalb des SELECT-Blocks erst deutlich später ausgeführt wird. Damit erklärt sich sich zum Beispiel auch, warum ein Alias-Name aus dem SELECT-Block nicht in der WHERE-Klausel benutzt werden kann: Das Alias wird erst im SELECT-Schritt verarbeitet und ist daher im WHERE-Schritt noch nicht bekannt.


Common Table Expressions und Turing Vollständigkeit


Dieser Abschnitt weist auf einige Sprachelemente von SQL hin, die erst im SQL-99-Standard eingeführt wurden, insbesondere die rekursiven common table expressions (CTE). CTEs werden möglicherweise nicht von allen Datenbanksystemen gleichermaẞen unterstützt. Die Beispiel in diesem Abschnitt wurden mit Postgres 9.6 getestet. CTEs sind temporäre Tabellen, die in einem SQL-Statement erzeugt werden und die dann in der Hauptabfrage weiterverwendet werden können. Die Syntax hierfür ist

WITH

expr_name_1 AS ( <table_def 1 via SQL>),

...,

expr_name_N AS ( <table_def_N via SQL>)

SELECT

...

FROM ..., expr_name_1, ..., expr_name_N, ...

Die temporären Tabellen sind hierbei expr_name_1, . . ., expr_name_N, sie werden i.d.R. durch SQL-Select-Statements definiert und können in der Hauptabfrage genauso wie normale Tabellen verwendet werden. Ferner darf es Abhängigkeiten zwischen den einzelnen Ausdrücke geben (d.h. expr_name_N kann wiederum durch expr_name_1 definiert sein). Auf diese Weise wird es möglich, sehr komplexe Ausdrücke übersichtlicher zu notieren. Allerdings kann eine äquivalente aber vielleicht weniger verständliche Schreibweise durchaus dazu führen, dass die Abfrage durch die Datenbank performanter verarbeitet werden kann.

Man unterscheidet weiter zwischen rekursiven und nicht-rekursiven CTEs, ein Beispiel für eine nicht-rekursive CTE ist:

WITH nums(counter) AS (

VALUES (1), (2), (3), (4), (5), (6), (7), (8), (9), (10)

)

SELECT * FROM nums;

Im WITH-Teil wird hier die temporäre Tabelle nums mit der Spalte counter erzeugt, die dann im Hauptstatement gelesen wird. Das Ergebnis sind eine Spalte und zehn Zeilen mit den Werten eins bis zehn. Das gleiche Resultat kann man auch rekursiv erzeugen:

WITH RECURSIVE nums_to_ten(counter) AS

( SELECT 1 UNION ALL SELECT counter+1 FROM nums_to_ten WHERE counter+1<= 10 )

SELECT * FROM nums_to_ten;

Eine etwas erweiterte Variante des vorstehenden Beispiels ist das folgende Statement, welches allerdings Postgres-spezifische Datumsfunktionen verwendet:

WITH RECURSIVE oct_dates(date_counter) AS (

SELECT DATE '2018-10-1'

UNION ALL

SELECT date_counter + 1

FROM oct_dates WHERE date_counter <= DATE '2018-11-1'

)

SELECT date_counter, to_char(date_counter, 'dy') as day_of_week

FROM oct_dates;

Es erzeugt als Ergebnis eine Liste aller Tage im Oktober 2018 mit dem zugehörigen Wochentag.

SQL lässt also Rekursion zu und erlaubt damit sehr allgemeine Berechnungen. SQL ist sogar Turing-vollständig: Theoretisch kann man damit jedes Problem lösen, welches in irgend einer anderen Programmiersprache gelöst werden kann.


Datenbankzugriff aus Pythonskripten


Früher in diesem Kapitel wurde gezeigt, wie der Zugriff von R auf eine MySQL-Datenbank bzw. eine SQLite-Datenbank erreicht werden kann. Hier illustrieren wir den Zugriff von Python auf eine Postgres-Datenbank.

Wir starten mit einer leeren Tabelle in Postgres, die wie folgt definiert ist:

CREATE SEQUENCE tbl_period_id_seq;

CREATE TABLE public.tbl_period

(

updated_at timestamp without time zone DEFAULT now(),

id integer NOT NULL DEFAULT nextval('tbl_period_id_seq'::regclass),

period_name character varying(50) NOT NULL,

year integer,

CONSTRAINT pk_tbl_period PRIMARY KEY (id),

CONSTRAINT uq_tbl_period_id UNIQUE (id),

CONSTRAINT uq_tbl_period_period_name UNIQUE (period_name)

)

Es gibt also die vier Spalten updated_at, id, period_name und year. Man beachte, dass updated_at und id mittels DEFAULT-Klauseln definiert sind, wodurch bei Einfügeoperationen die Datenbank automatisch einen Wert setzt, falls nicht explizit ein Wert vorgegeben wird. Beim Feld id ist die eine fortlaufenden Nummer und bei dem Feld updated_at ist das der aktuelle Zeitstempel. Der Zugriff von Python aus setzt das Treiberpaket psycopg2 voraus, welches die Python DBAPI31 implementiert und ggf. nachinstalliert werden muss. Der erste Schritt beim Zugriff ist, eine Verbindung zur Datenbank zu öffnen. Anschlieẞend fügen wir einige Daten ein:

import psycopg2

conn = psycopg2.connect(f"dbname=dwh user=postgres password={pwd}")

# zwei Zeilen einfügen

with conn.cursor() as cur:

cur.execute("""INSERT INTO tbl_period (period_name, year)

VALUES ('2016', 2016), ('2017', 2017)""")

conn.commit()

Die Daten können nun auch wieder eingelesen werden:

with conn.cursor() as cur:

cur.execute("SELECT * FROM tbl_period")

periods = cur.fetchall()

Die Variable periods ist eine Liste und hat jetzt folgenden Wert:

[(datetime.datetime(2018, 12, 9, 9, 47, 45, 464066), 1, '2016', 2016),

(datetime.datetime(2018, 12, 9, 9, 47, 45, 464066), 2, '2017', 2017)]

Der nächste Schritt ist nun häufig, die Datenstruktur, die der Datenbanktreiber geliefert hat (in diesem Fall eine Liste von Tupeln), in etwas Brauchbareres zu transformieren. Manchmal kann man das z.B. mit dem Pandas-Package einfach erreichen, indem man das Ergebnis direkt in einen Pandas-DataFrame übertragen lässt:

import pandas as pd

with psycopg2.connect(f"dbname=dwh user=postgres password={pwd}") as con:

df = pd.read_sql("SELECT * FROM tbl_period", con=con)


ORMs: Object Relational Mappers


Ein anderes sehr mächtiges Konzept sind Object-Relational-Mappers. Sie stellen eine Schicht dar, die Objekte der Programmiersprache in Zeilen von Datenbanktabellen überträgt, und zwar in beide Richtungen.

Wir betrachten ein einfaches Beispiel, das SqlAlchemy verwendet, einen weit verbreiteten Mapper in Python. Das Ziel ist es zunächst, das Datenmodell in Python zu beschreiben. Hierzu muss zu jeder Tabelle eine entsprechende Python-Klasse erstellt werden. Das erfordert einige Vorbereitungen:

from sqlalchemy import *

from sqlalchemy.ext.declarative import declarative_base

from sqlalchemy.orm import sessionmaker

conn_str = f"postgresql://postgres:{pwd}@localhost:5432/dwh"

session_maker = sessionmaker(bind=create_engine(conn_str))

_Base = declarative_base()

Mit Hilfe des Objektes session_maker wird später eine Verbindung aufgebaut werden, das Objekt_Base dient als Basisklasse. Der Kern des Ansatzes sind nun die Klassendefinitionen in Python, deren Objekte in der Datenbank gespeichert werden sollen. Wir kommen im folgenden Beispiel zurück auf die Struktur des vorherigen Abschnitts:

class Period(_Base):

__tablename__ = 'tbl_period'

id = Column(Integer, primary_key=True, unique=True)

updated_at = Column(DateTime, server_default=func.now(),

onupdate=func.current_timestamp())

period_name = Column(String(50), nullable=False, unique=True)

year = Column(Integer, nullable=True)

def __repr__(self):

return f"Period: id={self.id}, " +\

f"name={self.period_name}, year={self.year}"

Die Klasse hat als Klassenvariablen die Spalten der korrespondierenden Tabelle. Lesender Datenbankzugriff erfolgt nun direkt über SqlAlchemy, welches die notwendigen SQL-Statements automatisch generiert und die Rückgabewerte der Datenbank in entsprechende Python-Objekte konvertiert. Hierzu erzeugt man zunächst ein session-Objekt, welches nach dem Zugriff wieder geschlossen wird. Als Beispiel laden wir alle Einträge der Tabelle tbl_period in eine Liste von Instanzen der Klasse Period:

session = session_maker()

periods = session .query(Period).all()

session.close()

Die anschlieẞende Verwendung der Objekte ist nun leicht möglich:

for p in periods:

print(p)

Wir erhalten die folgende Ausgabe (man beachte: beim print-Aufruf kommt implizit die oben definierte Methode __repr__ der Klasse Period zum Einsatz):

Period: id=5, name=2016, year=2016

Period: id=6, name=2017, year=2017


Verwundbarkeit: SQL Injection


Eine Gefahr beim Einsatz von SQL in Software-Applikationen kann entstehen, wenn Nutzereingaben unverändert in SQL-Code integriert werden.

Als Beispiel soll angenommen werden, dass ein Benutzer etwa in einem Anmeldungs-Formularfeld seinen Nutzernamen eingeben soll. Ein schematischer Beispiel-Code für die Weiterverarbeitung könnte vielleicht so aussehen:

name = ... # Wert direkt aus Eingabefeld übernommen

prepared_sql = """select email_address

from userdetails where name = '""" + name + "'"

execute (prepared_sql)

In diesem Fall hat ein Benutzer nun die Möglichkeit, durch eine gezielte Eingabe das SQL-Statement so zu manipulieren, dass eine nicht intendierte Antwort geliefert wird. Zum Beispiel führt die Eingabe von

name = "' or ''='"

dazu, dass folgendes SQL-Statement an die Datenbank geschickt wird:

select email_address from userdetails where name = '' or ''=''.

Derartiger Missbrauch muss natürlich vermieden werden. Der Ausweg hierzu ist, die Ersetzung der Parameter in der Abfrage der Datenbank oder dem verwendeten Treiber zu überlassen, der eine korrekte Behandlung von Sonderzeichen in Texteingaben sicherstellt. Hierzu sind sog. parametrisierte Abfragen vorgesehen, wobei die genaue Syntax produktabhängig sein kann. Die Lösung besteht allgemein darin eine Anfrage mit einem Platzhalter zu formulieren (beispielhaft für eine Oracle-DB):

prepared_sql = """select email_address

from userdetails where name = :name_parameter'"""

Dann wird die Abfrage zusammen mit dem Parameter an die Datenbank (oder den genutzten Treiber) übergeben. In der weiteren Verarbeitung werden dann Steuerzeichen im Parameterstring (also z.B. die einfachen Anführungszeichen in der Benutzereingabe) so maskiert, dass sie die Semantik nicht verändert wird.


Trigger und Stored Procedures


Viele Datenbanken bieten die Möglichkeit, bestimmte Aktionen durch direkt im Datenbanksystem gespeicherte und ablaufende Skripte erledigen zu lassen. Diese Skripte heiẞen auch Stored Procedures (gespeicherte Prozeduren). Die benutzte Skriptsprache hängt hierbei vom benutzten System ab, oft kommt eine um etwa Variablen, Verzweigungen und Schleifen angereicherte Variante von SQL zum Einsatz aber teilweise werden auch Hochsprachen wie Java, Python, C# unterstützt. Stored Procedures haben mehrere Vorteile:

– Da sie (meist) vom Client aus aufgerufen werden und komplexe Logik beinhalten können, erspart der Einsatz ggf. den Datentransfer von und zum Client über das Netzwerk. Das kann sich deutlich positiv auf die Performanz auswirken.



– Sie können die Sicherheit verbessern, da sie einerseits SQL-Injection verhindern können und es andererseits auch ermöglichen, restriktivere Rechte auf den unterliegenden Datentabellen zu vergeben. So können manche Benutzer beispielsweise die Berechtigung für bestimmte Stored Procedures erhalten, die Daten einer Tabelle modifizieren, nicht aber den vollen Schreibzugriff auf die betreffende Tabelle.



– Sie können dazu beitragen Codeduplizierungen zu vermeiden.





Dem entgegen stehen auch Nachteile: Gerade SQL-lastige Stored Procedures sind oft imperativ angelegt und die Skripte sind nicht immer leicht zu verstehen, schwer zu warten und zu debuggen. Darüber hinaus sind sie auch systemspezifisch und somit nicht portabel, wodurch der sog. Vendor-Lock befördert wird.

Stored Procedures treten oft gemeinsam mit sog. Triggern auf. Dies sind definierte Einsprungpunkte für vordefinierte Routinen oder Stored Procedures, die es erlauben, Skripte oder andere Aktionen vor oder nach bestimmten Ereignissen ausführen zu lassen Ein typisches Beispiel ist etwa das Löschen eines Datensatzes, das u.U. eine ganze Kaskade weiterer Löschvorgänge initiieren muss, um die Konsistenz zu erhalten, ggf. sollen auch manche dieser Datensätze noch vor dem finalen Löschen in einer Archiv-Tabelle aufgehoben werden. Andere Szenarien sind beispielsweise die automatische Validierungen oder die Anreicherung eines Datensatzes vor dem Einfügen.


5.4 Arbeiten mit No-SQL-Datenbanken


Das Modell der relationalen Datenbank ist nicht das einzig denkbare. Im Gegenteil, es gibt schon seit langem konkurrierende Konzepte, deren Vertreter aber lange Zeit keine signifikanten Marktanteile gewinnen konnten und auch heute dominieren relationale Datenbanken (in diesem Zusammenhang etwas irreführend auch SQL-Datenbanken genannt) die meisten kommerziellen Einsatzszenarien.

Erst kurz nach der Jahrtausendwende erreichten in einer kurzen Zeitspanne mehrere Datenbanksysteme, die sich von den relationalen Datenbanken jeweils in bestimmter Hinsicht unterschieden, eine Qualitätsstufe, die sie fortan als echte Alternativen erscheinen lieẞen.

Einige dieser Produkte wurden dann als No-SQL Datenbanken bekannt, da sie andere Abfragesprachen als SQL einsetzten – andere neue Vertreter wiederum setzen auch hier auf Vertrautes. Nicht nur durch die Abfragesprache sondern auch in anderer Hinsicht verhielten sich diese Produkte anders als die relationalen Modelle. Der Begriff No-SQL wird heute meist als Not-Only-SQL gelesen. Die Vorteile der No-SQL Datenbanken bestehen vor allem in höherem Durchsatz (Performanz) und Skalierbarkeit, aber es gibt teilweise auch gröẞere Einschränkungen in Bezug auf Konsistenzgarantien, wodurch die No-SQL-Vertreter für einige Anwendungstypen, nicht jedoch für alle, geeignet erscheinen.

In der Folge haben die Entwickler der traditioneller Datenbanken teilweise die Ideen der No-SQL-Produkte übernommen, so dass die Unterschiede der Paradigmen wieder etwas aufgeweicht wurden. Der neuere Begriff New-SQL trägt diesem hybriden Ansatz Rechnung.

In diesem Abschnitt sollen einige allgemeine Merkmale der No-SQL-Datenbanken besprochen und einige bekannte Vertreter vorgestellt werden, die jeweils verschiedene Ansätze verfolgen. Der Abschnitt erhebt nicht den Anspruch das Konzept No-SQL theoretisch zu durchdringen und auch die Auswahl der Vertreter ist nicht repräsentativ sondern durch den subjektiven Erfahrungshorizont der Autoren bestimmt.


5.4.1 Verteilte Datenbanksysteme


Viele Entwicklungen im Bereich der sogenannten No-SQL Datenbanken haben ihren Ursprung in dem Ziel der Bereitstellung von verteilten Datenbanken bzw. Datenbankmanagementsystemen. Es gibt eine ganze Reihe von Gründen, warum man Datenbanken als verteilte Systeme betreiben möchte:

– Gröẞe: Die Datenmenge kann einfach zu groẞ für einen einzelnen Rechner sein, man muss die Daten auf verschiedene Rechner bzw. Speichermedien verteilen.



– Verfügbarkeit: Man möchte sich gegen (auch nur kurze) Ausfälle des Systems durch mehrere sogenannte (Hot)-Standbye Systeme schützen.



– Sicherheit: Schutz vor Datenverlusten durch redundante Speicherung.



– Performance: Ist die Anfragelast hoch, kann eine Verteilung der Anfragen helfen, die Antwortzeiten zu verkürzen. Hierbei werden die Zugriffe jeweils auf verschiedene Server verteilt (Load-Balancing), so dass eine höhere Parallelität in der Verarbeitung erreicht werden kann.





Es ist unmittelbar klar, dass durch die Verteilung die Komplexität der Systeme deutlich steigt. Betrachten wir kurz das letztgenannten Szenario, so ist einsichtig, dass es auf einem stabilen Datenbestand mit read-only Zugriffen gut funktionieren wird, wenn die Daten auf mehrere Systemteile repliziert wurden. Schwieriger wird es aber, wenn Änderungen an Daten vorgenommen und diese zu mehreren Instanzen propagiert werden müssen.

Auch viele Relationale Datenmanagementsystem (RDBMS) bieten Funktionalitäten, die die besprochenen Problemstellungen teilweise adressieren, ein Beispiel ist die zeitnahe Replikation von Änderungen auf einen oder mehrere entfernte Server, vgl. Abbildung 5.13, die auch von vielen RDBMS direkt unterstützt wird.


5.4.2 CAP Theorem für verteilte Systeme


Es zeigt sich, dass die Zielsetzung der vollständigen Lösung des Problems, die verteilten Datenbanken mit den Konsistenzgarantien des Relationalen Modells zu verbinden, nicht erreicht werden kann. Brewers CAP-Theorem zufolge ist es nicht möglich, die Eigenschaften Konsistenz (Constistency), Verfügbarkeit (Availability) und Partition Tolerance eines verteilten Datenbankmanagementsystems zugleich zu gewährleisten. Hierbei wird angenommen, dass Daten an verschiedenen Orten eines vernetzten Systems vorgehalten werden und die Datenhaltung redundant (oder zumindest mit Bestehen/Gewährleistung bestimmter Abhängigkeiten von einander) erfolgt.

Die im CAP-Theorem verwendeten Begriffe können nun folgendermaẞen interpretiert werden:

– Nach P sollte das System funktionieren, auch wenn die Kommunikation innerhalb des Systems gestört ist.



– Nach A sollte (jeder Teil) des Systems Transaktionen verarbeiten, auch wenn die Kommunikation innerhalb gestört ist.



– Nach C sollten die Antworten von unterschiedlichen Systemteilen konsistent sein.





Bei gestörter Kommunikation innerhalb des Systems sind die Lieferung konsistenter Antworten und die gleichzeitige Verarbeitung von Schreibvorgängen nicht miteinander zu vereinbaren. Bestimmte Szenarien wie das Sharding- und das Replikationsszenario lassen sich jedoch realisieren, vgl. Abbildungen 5.13 und 5.14.


Abb. 5.13: Replikations-Szenario: Hier werden die gesamten Daten redundant vorgehalten. Werden Schreibvorgänge parallel auf allen Instanzen durchgeführt, so können durch geeignetes Load-Balancing alle Instanzen auf Leseanfragen reagieren und somit die Leseperformanz steigern (aber jede Anfrage geht dabei komplett an eine Instanz). Die Art der Schreibvorgänge kann hierbei synchron oder asynchron realisiert werden: Bei synchroner Replikation gilt der Schreibvorgang erst als erfolgt, wenn er auf allen Instanzen gültig ist. Eine Netzwerkpartition verhindert dann Schreibvorgänge. Bei asynchroner Replikation erfolgt die Propagierung von Schreibvorgängen im Hintergrund und oft auch zeitverzögert. Hier kann es kurzfristig zu unterschiedlichen Datenständen auf den verschiedenen Subsystemen kommen.


Abb. 5.14: Sharding-Szenario: Daten werden abhängig vom Wert eines Schlüssels auf jeweils einer separaten Instanz vorgehalten (keine Redundanz). In dieser Form ist jeder Datensatz nur einmal vorhanden. Um maximale Parallelität erreichen zu können, sollten sich die Zugriffe möglichst gleichmäẞig auf die verschiedenen Subsysteme verteilen. Bei Zugriffen, die mehrere Instanzen betreffen, müssen die einzelnen Antworten durch ein zusätzliches Koordinationsmodul zusammengefügt werden.

Auch Kombinationen der besprochenen Szenarien sind gebräuchlich, in denen jeder Shard wiederum mehrfach repliziert wird, vgl. Abbildung 5.15.


Abb. 5.15: Eine mögliche Konfiguration einer produktiven verteilten Datenbank. Die Datenbank zerfällt in sechs Subsysteme, die die Daten in drei separaten Einheiten vorhalten, die wiederum jeweils aus zwei synchron gehaltenen Einheiten bestehen, um Lesezugriffe schnell verarbeiten zu können. Das Produktivsystem wird ergänzt durch zwei Backup-Systeme: Ein Hot-Standbye-System wird hierbei stets synchron zur produktiven Datenbank gehalten und kann beim Ausfall kurzfristig das Produktivsystem ersetzen. Der Remote-Standbye-Server befindet sich an einem anderen Ort und sorgt durch die räumliche Trennung für zusätzliche Datensicherheit.


5.4.3 Einsatz von No-SQL


Wie eingangs des Kapitels gesagt, verstehen wir unter No-SQL-Systemen Datenbankmanagementsysteme, die hinsichtlich der Konsistenzgarantien oder durch eine andere logische Strukturierung der Daten vom relationalen Modell abweichen. Auẞerdem werden diese Systeme meistens nicht über SQL angesprochen und sie kommen oft in Szenarien zum Einsatz, in denen verteilte Systeme benötigt werden, was z.B. wegen hoher Anforderungen an die Performanz oder das Datenvolumen der Fall sein kann.

In den nächsten Abschnitten sollen einige der Ideen und Konzepte beispielhaft an in der Praxis verwendeten Systemtypen und deren Vertretern erläutert werden.


5.4.3.1 Key-Value Stores


Die in diese Kategorie fallenden Systeme kann man als Verallgemeinerungen von Dictionary/Hashmap-Objekten verstehen, wie sie aus vielen Programmiersprachen bekannt sind. Es sei hier nochmal ein Beispiel in Python gegeben, das die Funktionsweise des Python-Dictionary-Typs rekapituliert:

my_dict = dict() # erzeuge leeres Dictionary

my_dict["kurs1"] = "ADS Basic" # füge ein String Element ein

# ein dict als Wert einfügen

my_dict["kurs2"] = {"name": "ADS Basic", "dauer": 3}

my_dict["kurs2"]["dauer"]

# Ausgabe: 3

In Python-Dictionaries kann man Objekte (values) unter Schlüsseln (keys) speichern und bei Kenntnis des Schlüssels auch wieder abfragen. Ein Dictionary ähnelt also einem Regal mit verschiedenen Schubladen, die alle mit einem key beschriftet sind und jeweils die Objekte enthalten. Sowohl der value als auch der key (dieser jedoch mit Einschränkungen) können hierbei allgemeine Python-Objekte sein (Zahltypen, Strings, Objekte zu selbstdefinierten Klassen, Listen oder andere Dictionaries).

Ein Key-Value-Store ist nun eine eigenständige Anwendung, die diese Funktionalität für andere Programme über eine Schnittstelle verfügbar macht, z.B. über das http-Protokoll oder ein speziell hierfür entwickeltes.

Einige gebräuchliche Systeme, die dieses Konzept umsetzen sind:

– memcached (https://memcached.org/)



– DynamoDB (https://aws.amazon.com/de/dynamodb/, ein Link auf ein maẞgebliches Paper dazu ist http://s3.amazonaws.com/AllThingsDistributed/sosp/amazon-dynamo-sosp2007.pdf



– Redis (https://redis.io/)



– Riak (https://riak.com/riak/)





Die Datenmodelle der Tools sind einfach, oft sind sowohl Schlüssel und Wert einfach Bytearrays.

Als Beispiel betrachten wir Redis ein wenig genauer. Auf einen laufenden Server kann man z.B. mit der mitgelieferten Konsolenanwendung zugreifen (Abbildung 5.16).


Abb. 5.16: Eine kurze Redis-Session mit dem Kommandozeilenwerkzeug. Zunächst werden alle bekannten Schlüssel ausgegeben (die Ausgabe ist leer). Dann wird der Schlüssel kurs:1 mit dem Wert ADS belegt und erneut abgefragt.

Ein Hauptanwendungsgebiet für Key-Value-Stores ist die Nutzung als (verteilter) In-Memory Cache. Das bedeutet, dass Daten lediglich zum Zwecke der schnellen wiederholten Abfrage zwischengespeichert werden. Da keine (oder nur eine verzögerte) Datensicherung auf einem permanenten Speichermedium vorgenommen wird, können Daten durch Hard- oder Softwarefehler dauerhaft verlorengehen. Dieses Risiko sollte beim Einsatz also gegen die Vorteile der kurzen Antwortzeiten abgewogen werden.

Durch das einfache Datenmodell (Speicherung von Bytearrays) sind keine komplexen Anfragen möglich, einfache Lookups von Werten bei gegebenem Schlüssel sind die Regel. Zu Bedenken ist auch, dass sich ein erhöhter Aufwand beim Erstellen der Clientanwendung ergeben kann, wenn komplexere zu speichernde Datenstrukturen auf Anwendungsseite in eine Bytesequenz (de)serialisiert werden müssen. Das führt im Allgemeinen dazu, dass diese Daten jeweils nur von der Applikation gelesen werden können, von der sie auch geschrieben wurden. Als Vorteil steht dem wie schon gesagt eine hohe Performanz gegenüber. Auẞerdem bietet das Datenmodell die natürliche Möglichkeit, Zugriffe über den Schlüssel auf verschiedene Instanzen zu verteilen (Sharding-Szenario).


Bemerkung


Spezielle Systeme bieten eine ganze Reihe zusätzlicher Funktionalitäten, z.B. bietet Redis Persistenz und einige komlexere Datenstrukturen. Das Tool kann auch als Message-Broker mit einem Publish/Subscribe-Mechanismus genutzt werden.


5.4.3.2 Dokument-Datenbanken


Diese Art von Datenbank kann man als eine Erweiterung der Key-Value-Stores verstehen, die komplexere Datenstrukturen und Abfragen unterstützt. Bekannte Vertreter sind z.B. CouchDB32 und MongoDB33. Daten sind in sogenannten Dokumenten organisiert, wobei bei den genannten Vertretern i.W.

Dokument = <JSON-OBJEKT>

gilt. Unter einem Dokument stellt man sich also am besten ein (geschachteltes) Dictionary vor. Intern werden die Strukturen dann teilweise noch um beispielsweise IDs oder Versionierungsinformationen erweitert. Andere Produkte setzen auch teilweise auf XML auf und sind damit näher am HTML/docx-Format, mithin an Dokumenten im umgangssprachlichen Sinn.

Vergleicht man das Modell mit dem der relationalen Datenbanken, so entspricht ein Dokument einer Zeile in einer Tabelle. Oft gibt es weitere Strukturen in den Dokument-Datenbanken, die die Trennung unterschiedlicher Dokument-Typen ermöglichen und dann etwa den Tabellen im relationalen Modell entsprechen. In der MongoDB werden die Dokumente beispielsweise in Collections vorgehalten.

Ein wichtiger Unterschied zwischen Dokumenten in einer Dokument-Datenbank und Einträgen in einer relationalen Tabelle ist, dass Dokumente schemafrei sind, d.h. dass die Struktur der einzelnen Dokumente frei gewählt werden kann (meist abgesehen von einem notwendig vorhandenen Primärschlüssel).

Wir rekapitulieren die Struktur der Datenspeicherung noch einmal am Beispiel von MongoDB:

– Daten sind in einzelnen sog. Datenbanken gespeichert (analog dem entsprechenden Konzept in der relationalen Welt),



– Datenbanken enthalten Collections,



– Collections enthalten die Dokumente (analog den Zeilen in einer relationalen Datenbanktabelle),



– Dokumente in einer Collection sind nicht identisch strukturiert.





Im Extremfall kann also jedes Dokument in einer Collection ganz anders aufgebaut sein als alle anderen. Dieses Extrem ist vermutlich nur selten erstrebenswert, aber gerade in der sich ergebenden Flexibilität liegt eine der Stärken der Dokument-Datenbanken.


Bemerkung


In einem Key-Value-Store herrscht ebenfalls Schemafreiheit: Nutzt man eine Serialisierungstechnik, so kann man beliebige Objekte/Dokumente speichern. Die Dokument-Datenbanken gehen darüber hinaus, indem die Struktur zwar von Dokument zu Dokument unterschiedlich sein kann, aber die Abfragen die Struktur nutzen können, d.h. man kann Dokumente nicht nur anhand eines Schlüssel bzw. eines Primärschlüssels sondern auch über die anderen Felder selektieren.



CouchDB setzt, anders als z.B. MongoDB, noch ein weiteres interessantes Konzept von Haus aus um: Immutable Data. Jedes Dokument wird intern mittels zweier Schlüssel verwaltet, einer ID und einer REVISION_ID, die beide beim Anlegen eines Dokuments erzeugt werden. Updates ändern in diesem Sinn nicht die vorhandenen Daten sondern es wird immer ein neues Dokument erzeugt und dabei die REVISION_ID geändert. Um ein Update durchzuführen muss dem Client die aktuelle REVISION_ID bekannt sein und er muss sie an den Server übermitteln, ansonsten weist der Server das Update zurück.





Um einen Eindruck von den Möglichkeiten und der Anfragesprache von Dokument-Datenbanken zu gewinnen, betrachten wir nun einige Beispieltransaktionen innerhalb einer MongoDB in einer interaktiven Session mit dem Kommandozeilen-Client. Wenn der Server gestartet ist, kann man sich mit dem Kommandozeilen-Client mongo damit verbinden und zur Zieldatenbank wechseln:

PS D:\programming\py\ads_skripten> mongo

MongoDB server version: 3.4.2

2018-05-25T09:34:52.537-0700 I CONTROL [initandlisten]

> use ads

switched to db ads

Wir arbeiten jetzt also in der Datenbank ads. In Datenbanken kann man in MongoDB collections anlegen, die wiederum Dokumente beherbergen (also die eigentlichen Daten). Wir fügen jetzt zwei Dokumente ein, wodurch auch die Collection items bei Bedarf automatisch miterstellt wird:

> db.items.insert({butter: 1, milk: 12})

> db.items.insert({butter: 1, milk: 2})

Die find-Methode der Collection stellt die Such-Abfrage dar, ihr wird ein Argument übergeben, das die Suchabfrage kodiert, das leere Objekt {} liefert alles:

> db.items.find({})

{ "_id" : ObjectId("5b084056c08c46c5296e63c1"), "butter" : 1, "milk" : 12 }

{ "_id" : ObjectId("5b0848a5efdcff201758f252"), "beer" : 12, "milk" : 2 }

Das Feld_id wird systemseitig beim Einfügen vergeben, wenn es nicht explizit gesetzt wird. Es stellt den Primärindex der Collection dar.

Die Mongo-Konsole beherrscht JavaScript, was im nächsten Fragment bei der Erstellung des Datumsobjekts benutzt wird:

> db.items.insert({inserted_at: new Date()})

WriteResult({ "nInserted" : 1 })

> db.items.find({})

{ "_id" : ObjectId("5b084056c08c46c5296e63c1"), "butter" : 1, "milk" : 12 }

{ "_id" : ObjectId("5b0848a5efdcff201758f252"), "beer" : 12, "milk" : 2 }

{ "_id" : ObjectId("5b084ab7dd46ed2af2d29480"), "inserted_at" : ISODate("2018-05-25T17

:41:11.836Z") }

Das Beispiel illustriert auch die Schemafreiheit: Die Dokumente in einer Collection können strukturell gänzlich verschieden sein.

Es folgen noch ein paar Beispiele, ein Update:

> db.items.update({"_id": ObjectId("5b084056c08c46c5296e63c1")}, {$set:{butter: 13,

bread: 3}})

WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1})

> db.items.find({"_id": ObjectId("5b084056c08c46c5296e63c1")})

{ "_id" : ObjectId("5b084056c08c46c5296e63c1"), "butter" : 13, "milk" : 12, "bread" :

3 }

Und ein Insert:

> db.items.insert({beer: 6})

WriteResult({ "nInserted" : 1 })

Zeige nur Dokumente mit mindestens einem Bier: Hierfür wird der Operator $gte: 1 verwendet, durch den die Bedingung gröẞer oder gleich (greater than or equal) ausgedrückt werden kann:

> db.items.find({beer: {$gte: 1}})

{ "_id" : ObjectId("5b0848a5efdcff201758f252"), "beer" : 12, "milk" : 2 }

{ "_id" : ObjectId("5b084dba6f289fca8efc9e05"), "beer" : 6 }

Die Selektion der benötigter Rückgabefelder kann durch das zweite Argument der find-Methode gesteuert werden:

> // das Feld _id wird in der Ausgabe unterdrückt

> db.items.find({beer: {$gte: 1}}, {beer: 1, _id: 0})

{ "beer" : 12 }

{ "beer" : 6 }

Beim bedingten Löschen werden nur die Datensätze gelöscht, die eine Bedingung erfüllen, hier wird die Bedingung an das Feld beer über den Operator $lte: 10 definiert, was für kleiner oder gleich (less than or equal) steht, vgl. das folgende Beispiel.

> db.items.deleteMany({beer: {$lte: 10}})

{ "acknowledged" : true, "deletedCount" : 1 }

> db.items.find({beer: {$gte: 1}}, {beer:1, _id: 0})

{ "beer" : 12 }

Will man komplexere Änderungen durchführen, kann man Skripte schreiben, wofür kann man sich entweder eines Treibers und der Programmiersprache der Wahl bedienen kann – falls sie unterstützt wird, die Chancen stehen gut – oder man nutzt die JavaScript-Fähigkeiten des Kommandozeilen-Clients.

Mit einem Skript haben wir Zufallsdaten erzeugt, wie sie an den Verkaufstheken eines überregional agierenden Delikatessengeschäftes anfallen könnten, welches seine Daten zentral für Analysezwecke in Echtzeit zusammenführt, für Details zur Erzeugung der Daten des Beispiels vgl. A.2.3. Eine beispielhafter Datensatz ist hierbei folgendermaẞen aufgebaut:

> db.items.findOne()

{

"_id" : ObjectId("5b095075ca820a6214429350"),

"storeId" : "Berlin 1",

"paymentType" : "VISA",

"products" : [

{

"name" : "Trüffelsalami",

"pricePerItem" : 3.99,

"numberOfItems" : 9

}

]

}

Es gibt offenbar Informationen zur jeweiligen Niederlassung (storeId), zur Zahlungsweise (paymentType) und zu den gekauften Produkten (products, ein Array, das auch mehrere Elemente enthalten kann).

Anhand der Beispieldaten demonstrieren wir nun mit einer komplexeren Query noch weitere Möglichkeiten. Das Aggregation Framework der MongoDB erlaubt das Aufbauen einer Sequenz von Transformationen

db.items.aggregate([{ < 1. Transformation > },

{ < 2. Transformation > },

... ])

Das Ergebnis von Transformation k ist hierbei der Input für Transformation k + 1. Will man nur die zusammen gekauften Produkte erhalten (z.B. als Ausgangspunkt für eine Association Analysis über den Datensatz), so könnte man etwa so vorgehen:

db.items.aggregate([

{$project: {"products.name": 1}},

{$unwind: "$products"},

{$group: { _id: "$_id", "products": {$addToSet: "$products.name"}}},

{$project: {_id:0}}

])

Wir erklären die Anweisung im Folgenden Schritt für Schritt anhand der Struktur der Ergebnismenge, wobei wir nur die Ideen vermitteln wollen, eine Erklärung der Details würde den Rahmen an dieser Stelle sprengen.

Der erste Schritt ist eine Projektionsphase, die durch den Operator $project initiiert wird. Das Ziel ist das Filtern der Spaltenmenge, Einträge der Ergebnismenge nach diesem Schritt haben folgende Struktur:

{ "_id" : ObjectId("5b095075ca820a6214429350"), "products" : [ { "name" : "

Trüffelsalami" } ] }

{ "_id" : ObjectId("5b095075ca820a6214429351"), "products" : [ { "name" : "

Trüffelsalami" }, { "name" : "Frische Feigen" } ] }

Es folgt im nächsten Schritt der $unwind-Operator auf dem Listenfeld products. Unwind expandiert ein eingebettetes Array-Feld in eine Liste von einzelnen Records unter Beibehaltung der übrigen Felder. Angewendet auf das vorstehende Beispiel erhält man jetzt drei Zeilen in der Ergebnismenge, eine korrespondiert mit der ersten und die beiden anderen mit der zweiten der vorher erhaltenen Zeilen:

{ "_id" : ObjectId("5b095075ca820a6214429350"), "products" : { "name" : "Trüffelsalami

" } }

{ "_id" : ObjectId("5b095075ca820a6214429351"), "products" : { "name" : "Trüffelsalami

" } }

{ "_id" : ObjectId("5b095075ca820a6214429351"), "products" : { "name" : "Frische

Feigen" } }

Nun folgt eine Gruppierungsphase, die durch den Operator $group eingeleitet wird. Die Gruppen werden durch das jeweilige _id-Feld definiert und enthalten je Gruppe noch das (in diesem Statement neu definierte) Ergebnisfeld products, welches wiederum ein Arraytyp ist, der aus dem verschachtelten Feld products. name aufgebaut wird, indem alle Feldeinträge einer Gruppe zusammengefasst werden. Wir kehren also den vorherigen Unwind-Schritt wieder um, wobei allerdings nur die interessierenden Teile der Informationen erhalten geblieben sind:

{ "_id" : ObjectId("5b095075ca820a62144419ed"), "products" : [ "Frische Feigen", "

Trüffelsalami" ] }

{ "_id" : ObjectId("5b095075ca820a62144419ec"), "products" : [ "Trüffelsalami" ] }

In der abschlieẞenden zweiten $project-Phase wird dann noch das _id-Feld ausgeblendet, jede Zeile der Ergebnismenge gibt jeweils die Produkte eines Einkaufs an:

{ "products" : [ "Frische Feigen", "Trüffelsalami" ] }

{ "products" : [ "Trüffelsalami" ] }




Es folgt noch ein weiteres Beispiel, man mache sich klar, was genau passiert:

>db.items.aggregate( [

// nur VISA-Zahlungen berücksichtigen

{ $match: {"paymentType" : "VISA"} },

// Spalten-Filter

{ $project: {_id: 1, storeId: 1, products: 1}},

// Expandiere mehrere Produkte pro Einzeleinkauf zu ganzen Dokumenten

{ $unwind: "$products"},

// Berechnen Preis der Artikelgruppe

{ $addFields: { price: {$multiply: ["$products.pricePerItem",

"$products.numberOfItems"]}}},

// Berechnen Gesamtpreis pro Einkauf

{ $group: { _id: "$_id", "storeId": { "$first": "$storeId"},

"totalPrice": {"$sum": "$price"} }},

// gruppiere je Store und berechne Gesamtumsatz

{ $group: { _id: "$storeId", "revenue": {"$sum": "$totalPrice"},

"highestPricePerSale": {"$max": "$totalPrice"},

"salesCount": { $sum: 1 }}}

]);

In diesem Beispiel erhalten wir folgendes Ergebnis:

> items

{ "_id" : "Jüterbog", "revenue" : 606924.6, "highestPricePerSale" : 276.63, "

salesCount" : 6284 }

{ "_id" : "Augsburg", "revenue" : 615780.47, "highestPricePerSale" : 269.46, "

salesCount" : 6314 }

{ "_id" : "Berlin 1", "revenue" : 624552.5399999999, "highestPricePerSale" : 275.01, "

salesCount" : 6314 }

{ "_id" : "Eltville", "revenue" : 602595.2, "highestPricePerSale" : 288.06, "

salesCount" : 6180 }





Wir gehen abschlieẞend noch kurz auf den Zugriff mittels Python ein, was die Installation des Treibers (z.B. mittels python -m pip install pymongo) voraussetzt. Ein einfacher Zugriff gestaltet sich dann wie folgt:

# mit der DB "ads" verbinden

import pymongo

client = pymongo.MongoClient("localhost", 27017)

db = client.ads

# aus der collection `items` lesen:

db.items.find_one({"paymentType" : {"$ne": "VISA"}})

Der Rückgabewert liegt dann als Python-Dictionary vor und ist in unserem Beispiel

{'_id': ObjectId('5b095075ca820a6214429351'),

'storeId': 'Jüterbog',

'paymentType': 'EC',

'products': [{'name': 'Trüffelsalami',

'pricePerItem': 3.99,

'numberOfItems': 9.0},

{'name': 'Frische Feigen', 'pricePerItem': 0.69, 'numberOfItems': 165.0}]}

Wie schon erklärt wurde, stehen die Dokument-Datenbanken konzeptionell zwischen relationalen Datenbanken und Key-Value-Stores und die Entwicklung ist noch im Gange. Es war in MongoDB beispielsweise bis vor kurzem kein join, also eine Abfrage, die Dokumente aus mehreren Collections verknüpft, möglich. Das ist inzwischen jedoch anders und seit Oktober 2019 (Version 4.2) werden auch Transaktionen in verteilten Installationen unterstützt.

Erwägt man den Einsatz von einer Dokument-Datenbank, können folgende Fragen bei der Entscheidung und der Wahl eines passenden Produktes eine Rolle spielen:

– Werden Transaktionen benötigt oder reicht die Garantie des atomaren Updates auf einzelnen Dokumenten?



– Performance: Hier muss man immer selbst testen und sich über das Datenmodell und Indizes Gedanken machen



– Welche Vor- und Nachteile bringt die Schemafreiheit mit sich?



– Gibt es Treiber/Dokumentation/Support für die Plattform, die vorgesehen ist?



– Vorhandene Expertise/Einarbeitungszeiten



– Lizenz- und Supportkosten



– Austauschbarkeit (Wie stark hängt das Projekt von dieser Entscheidung ab?)





5.4.3.3 Graphdatenbanken am Beispiel von Neo4J


Als Alternative zu den relationalen sind für bestimmte Zwecke sog. Graphdatenbanken entwickelt worden, wobei Neo4J derzeit anscheinend der populärste Vertreter ist. Die community edition kann man hier34 herunterladen, für erste Experimente ist sie in jedem Fall ausreichend. Dieser Abschnitt soll einen Eindruck vom Einsatz einer Graphen-Datenbank an Hand einer interaktiven Session geben.

Zum Laufen bringt man den Server einfach durch das Entpacken des heruntergeladenen zip-Files. Danach wechselt man auf der Konsole in das Installationsverzeichnis und startet bin\neo4j console. Der Server setzt eine Java8-Installation voraus. Er stellt ein Web-UI zur Verfügung, das unter http://localhost:7474 zu erreichen und zur weiteren Exploration geeignet ist.

Um auf die Datenbank zuzugreifen, bedient man sich am besten eines Treibers in der Programmiersprache der Wahl. Im Folgenden kommt das Community-Tool py2neo zum Einsatz, welches zunächst mittels pip install py2neo zu installieren ist.

Graphdatenbanken speichern Graphen im Sinn der Graphentheorie, also Objekte bestehend aus Knoten und Kanten. In Neo4J haben diese Objekte jeweils ein Label, das üblicherweise den semantischen Typ des Objekts definiert und weitere Properties, in denen die Attribute, also die eigentlichen Daten gespeichert werden, die mit den Knoten und Kanten assoziiert sind.

Zum Zugriff existieren mehrere Sprachen, wobei ganz offenbar Cypher von den Entwickler/Vermarktern inzwischen klar favorisiert wird.

In einer interaktiven Python-Session stellen wir zunächst ein Verbindung zum Server her und legen dann einen Knoten in einer leeren Datenbank an:

import datetime

import pandas as pd

from py2neo import Graph, Node, Relationship

# diese Variable repräsentiert die Graphen-Datenbank

graph = Graph(password="<mein Password>")

# alles Löschen:

# graph.run("MATCH (n) DETACH DELETE n")

# Anlegen eines Knotens: Variante 1: Cypher-Query

query_create_prod = """CREATE (pg:Product { name: {name},

productGroup: {productGroup}})

RETURN pg"""

prod_riester = graph.run(query_create_prod, productGroup="Annuities",

name="Riester_01")

Nun besteht der Graph aus genau einem Knoten, den wir in der Web-UI inspizieren können. Wir fügen noch ein paar Elemente hinzu, diesmal unter Benutzung der Python-API in einer Transaktion:

# Transaktion beginnen

tx = graph.begin()

# vier Knoten definieren

alice = Node("Client", name="Alice")

bob = Node("Client", name="Bob")

alice_addr = Node("Address", city="Berlin", country="GER", street="Ginsterweg 12")

bob_addr = Node("Address", city="Dresden", country="GER", street="Am Bühl 31")

tx.create(alice)

tx.create(bob)

# zwei Relationen definieren

alice_rechn = Relationship(alice, "Anschrift", alice_addr)

bob_rechn = Relationship(bob, "Anschrift", bob_addr)

# anlegen der Relationen (und implizit der `Address`-Knoten)

tx.create(alice_rechn)

tx.create(bob_rechn)

# Anlegen von zwei Versicherungsprodukten

prod_sterbegeld_alice = Node("Product", productGroup="Mortality", name="Funeral")

prod_sterbegeld_bob = Node("Product", productGroup="Mortality", name="Funeral")

tx.create(prod_sterbegeld_alice)

tx.create(prod_sterbegeld_bob)

# Definition und Anlage von Versicherungsverträgen als Relationen

alice_sterbegeld = Relationship(alice,

"HAT_ABGESCHLOSSEN",

prod_sterbegeld_alice,

valid_from="2018/04/21")

bob_sterbegeld = Relationship(bob,

"HAT_ABGESCHLOSSEN",

prod_sterbegeld_bob,

valid_from="2012/01/19")

tx.create(alice_sterbegeld)

tx.create(bob_sterbegeld)

tx.commit()

Neo4J unterstützt ACID-Transaktionen, vgl. Abschnitt 5.3.3. Das Ergebnis ist in Abbildung 5.17 visualisiert.

Nachdem wir nun Daten in der Datenbank angelegt haben, können wir programmatisch einige Abfragen stellen, wobei hier auch erstmals das wichtige match Statement zum Einsatz kommt. Als erstes Beispiel sollen alle Kunden ausgegeben werden. Hierfür wird jeweils ein Cipher-Statement an die Methode graph. run übergeben. Das dump()-Statement sorgt für die unmittelbare Ausgabe des Ergebnisses

graph.run("""MATCH (c:Client) RETURN c""").dump()

Die Ausgabe ist dann

c

-------------------------------

(alice:Client {name:"Alice"})

(bob:Client {name:"Bob"})

Das match Statement definiert eine Struktur (hier sinngemäẞ alle Knoten vom semantischen Typ ‘Client‘), zu der die Neo4J-Engine dann die passenden Knoten zurückgibt.


Abb. 5.17: Graph Datenbank: mehrere Knoten und Kanten.

Im folgenden Beispiel lassen wir uns alle Adressen in Dresden zurückgeben. Die Abfrage lautet:

graph.run("""MATCH (a:Address{city:"Dresden"})

RETURN a""").dump()

In dieser Abfrage wurde neben dem Knotentypen noch ein Wert für das Attribut city vorgegeben, um die Knotenmenge geeignet einzuschränken. Das Ergebnis ist

a

----------------------------------------------------------------------

(d11f255:Address {city:"Dresden",country:"GER",street:"Am Bühl 31"})

Will man alle Kunden in Dresden finden, so muss man für die Formulierung der Einschränkung vom Kunden aus der Relation Anschrift folgen, wie im folgenden Statement illustriert wird:

graph.run("""MATCH (c:Client)-[Anschrift]->(Address{city: "Dresden"})

RETURN c""").dump()

Die Ausgabe ist dann (bob:Client {name:"Bob"}) .

Wir fügen noch eine Kante mittels Cypher hinzu. Jetzt soll Alice auch ein Riesterprodukt erwerben. Hierzu müssen zunächst die Knoten c für den Kunden und p für das Produkt selektiert werden und anschlieẞend wird eine Kante mittels (c)-[]->(p) eingefügt:

insert_query = """

MATCH (c:Client {name:'Alice'}), (p:Product{name:'Riester_01'})

CREATE (c)-[:HAT_ABGESCHLOSSEN{valid_from:{valid_from}}]->(p)

"""

today = datetime.datetime.now().strftime("%Y/%m/%d")

graph.run(insert_query, valid_from=today)

Der entstandene Graph ist nun in Abbildung 5.18 dargestellt.


Abb. 5.18: Graph Datenbank: Riestervertrag für Alice hinzugefügt.

Praktisch für die weitere Arbeit mit aus Neo4J geladenen Daten ist auch die Möglichkeit, ein Query-Ergebnis direkt in einen Pandas-DataFrame überführen zu können. Hier fragen wir z.B. nach allen Policen einschlieẞlich dem Abschlussdatum. Anschlieẞend liegt das Ergebnis als Pandas-DataFrame in der Variablen df_clients vor:

query = """MATCH (c:Client)-[r:HAT_ABGESCHLOSSEN]->(p)

RETURN c.name, r.valid_from, p.name"""

df_clients = pd.DataFrame(graph.run(query).data())


5.4.3.4 Column Store Databases


Der Begriff der Spaltenorientierung bezieht sich zunächst darauf, wie tabellarische Daten im Speicher oder auf Speichermedien vorgehalten werden: Gehören die einzelnen Zeilen zusammen und werden als Block gelesen und verarbeitet oder eher die einzelnen Spalten? Wir illustrieren die gebräuchlichen Konzepte an Hand der Daten in Tabelle 5.6.


Tab. 5.6:Beispieldaten zum Vergleich von spalten- und zeilenweiser Speicherung.



Spalte A Spalte B Spalte C

1 AAA T

2 BBB T

3 CCC F



Klassisch erfolgt die Speicherung zeilenweise, d.h. das Layout im Speicher kann man sich etwa so vorstellen:


1 AAA T 2 BBB T 3 CCC F



In der spaltenorientierten Variante erfolgt die Speicherung nach diesem Schema:


1 2 3 AAA BBB CCC T T F



Die gewählte Speicherarchitektur kann entscheidenden Einfluss auf das Verhalten des Systems haben. Im spaltenweisen Layout ist das Hinzufügen neuer Spalten vergleichsweise einfach, da diese in einem neuen Speicherbereich liegen und lediglich die Tabellendefinitions-Metadaten aktualisiert (und ggf. umkopiert) werden müssen. Dadurch gewinnt man eine höhere Flexibilität in Bezug auf das Datenschema, welches dann leicht dynamisch erweitert werden kann.

Durch das spaltenweisen Layout kann man darüber hinaus auch Speicherplatz einsparen, da es möglich ist, die Daten in jeder Spalte separat zu komprimieren und durch die zu erwartende höhere Homogenität der Daten der einzelnen Spalten kann die Kompressionsrate deutlich höher ausfallen als bei Kompression von Daten in zeilenweiser Speicherung.

Auch die Performanz wird durch das Speicherlayout beeinflusst, gerade wenn, was häufig der Fall ist, in Queries nur wenige Spalten einer Tabelle abgefragt werden. Da von der Harddisk üblicherweise jedoch gröẞere Blöcke gelesen werden (z.B. kann die Mindestgröẞe mehrere Kilobyte betragen), kann das Auslesen einer einzigen Spalte einer zeilenweise gespeicherten Tabelle dazu führen, dass effektiv die ganze Tabelle von der Disk gelesen werden muss und entsprechend Energie und Zeit benötigt wird. Bei spaltenweiser Speicherung ergibt sich hier ein Vorteil, weil der Umfang nicht notwendiger Lesevorgänge deutlich reduziert werden kann.

Einen wichtigen Anstoẞzur Popularisierung der spaltenorientierten Datenbankenwar Arbeit bei Google und deren anschlieẞende Veröffentlichung, insbesondere das Big-Table Paper 35. In dem Paper beschreiben die Autoren das System BigTable als Abbildung wie folgt:

m: (string, string, int64), → string

(row, column, time) ↦ m(row, column, time).

Der Zugriff auf Daten erfolgt also über eine Zeilen-ID und einen Spaltennnamen und es gibt eine zusätzliche zeitliche Versionierung.

Ziele der Entwicklung bei Google waren Skalierbarkeit im Umgang mit groẞen Datenmengen und eine verteilte Installation des Systems auf günstiger commodity hardware zu erreichen. Es folgen einige weitere Eigenschaften, die im Google-Paper genannt werden:

– pro Zeile sind Lese- und Schreibvorgänge atomar,



– die Datenbank verwaltet uninterpretierte Zeichenketten,



– die Verteilung auf unterschiedliche Maschinen erfolgt über die Zeilen-ID,



– Spalten sind in Column Families (Notation: "family:column_name") organisiert, die Zahl der Spalten kann sehr groẞ sein und pro Eintrag variieren,



– der timestamp time dient zur Versionierung und wird i.A. automatisch erzeugt, dazu gehört auch eine konfigurierbare Garbage-Collection: man kann vorgeben, wie viele historische Versionen vorgehalten werden sollen,



– keine Multi-Row Transactions (2006),



– Daten können Input/Output für Map-Reduce-Jobs sein.





Konzeptionell stehen spaltenorientierten Datenbanken zwischen den Key-Value-Stores (KV-Stores) und den Dokumentendatenbanken:

– der Row-Key entspricht dem Schlüssel eines KV-Stores,



– im Gegensatz zu KV-Store gibt es mehrere Felder (die Column-Families),



– im Gegensatz zu Dokument-Datenbanken haben die Dateneinträge keine unterstützte Substrukturen/Schemata.





Die Interpretation von Strukturen erfolgt beim Client. Freie Produkte, die sich eng am BigTable-Konzept orientieren sind z.B. Apache Cassandra, ein von Facebook entwickeltes und genutztes System, sowie HBase, ein Produkt der Apache Software Foundation.

Beispiel: Cassandra wurde ursprünglich für Facebooks Inbox search entwickelt. Es handelt sich um ein verteiltes Datenbanksystem mit Unterstützung von sharding/rebalancing. Wir skizzieren ein Anwendungsszenario in der Industrie 4.0 in Abbildung 5.19.


Abb. 5.19: Einsatzszenario: Die Sensordaten einer Maschine in einer Fabrik werden zum Hersteller geschickt und werden dort nahezu in Echtzeit gemonitort, was u.a. Grundlage von Predictive Maintenance-Prozessen sein kann. Durch das hohe Datenvolumen kommen Column-Stores in der Rolle des No-SQL Data Storage für solche Szenarien in Frage.


5.4.3.5 Andere No-SQL-Vertreter


Es gibt noch einige weitere Systemtypen, die man ebenfalls der Kategorie NoSQL zuordnen kann. In der Regel wurden sie für spezielle Einssatzszenarien konstruiert.


Broker/Message-Queues/Publish-Subscribe


Dies sind Systeme, die als Kopplungsmechanismus zwischen verschiedenen Komponenten eines Systems ins Spiel kommen: Producer-Systeme erzeugen Nachrichten, die von Consumer-Systemen gelesen und verarbeitet werden sollen. Das Bindeglied sind die Message-Broker, bei denen sich Producer und Consumer anmelden können und die die Zustellung und Zwischenspeicherung der Nachrichten übernehmen.

Bekannte Implementierungen unterscheiden sich teilweise durch folgende Eigenschaften:

– Nachrichten können gepuffert sein oder nicht (gehen evt. verloren, wenn gerade niemand liest),



– teilweise müssen gesendete Nachrichten einem Topic zugeordnet werden, die Consumer abonnieren dann bestimmte Topics,



– Nachrichten können an einen einzelnen oder an alle Consumer ausgeliefert werden,



– Unterschiedliche Persistenzgarantien,



– Clusterbetrieb kann möglich sein oder nicht.





Nicht alle Tools unterstützen alle Szenarien. Bekannte Systeme sind u.a. RabbitMQ, Apache Kafka, Redis.


Zeitreihendatenbanken


Diese Systeme eignen sich besonders für die Speicherung regelmäẞiger Messungen (z.B. Monitoring von Systemparametern) und sie bieten komfortable Möglichkeiten, diese Daten aggregiert über bestimmte Zeiträume auszugeben.


Spezielle Suchmaschinen


Vertreter hierfür sind beispielsweise Solr/Elastic-Search. Diese Systemen sind als Grundbaustein von Suchmaschinen konzipiert, sie bieten eine effiziente Indexierung und Features wie full text search und fuzzy matching, die für bestimmte Zwecke der Textverarbeitung sehr nützlich sind.

Typische Anwendungsfälle entstehen beispielsweise auch dann, wenn gröẞere Mengen an semi-strukturierten Daten anfallen die nach Auffälligkeiten durchsucht werden (z.B. Fehlersuche in Anwendungen mit Hilfe von Log-Files).


5.5 Datenwarenhäuser


In diesem Abschnitt führen wir in die Grundbegriffe von Data Warehouses ein. Diese Systeme haben sich einen festen Platz in vielen Unternehmen erobert und werden dort u.a. für das Reporting und die Analyse von Finanzkennzahlen eingesetzt.


5.5.1 Operative und dispositive Datenhaltung


Operative Datenhaltung. Versicherungsunternehmen werden heutzutage in ihren operativen Anwendungsbereichen durch eine Vielzahl an operativen Informationssystemen unterstützt (siehe hierzu auch Abschnitt 5.10). Als prominentestes Beispiel sind Bestandsführungssysteme zu nennen. Ein solches operatives System besteht natürlich nicht nur aus einer Software, welche die Benutzereingabe und die Businesslogik bereitstellt, sondern auch aus einer oder mehreren Datenbanken für die Speicherung der benötigten Daten. Diese Datenbanken sind für einen schnellen und fehlerfreien Zugriff der operativen Systeme optimiert und zeichnen sich mitunter durch folgende Eigenschaft aus:

– hohe Zugriffsfrequenz,



– dynamische transaktionsbezogene Aktualisierung der Daten,



– keine Redundanz der Datenspeicherung und referentielle Integrität.





Die letztgenannte Eigenschaft hat zur Folge, dass angefragte Informationen oft auf viele Tabellen verteilt sind. Operative Datenbanken sind daher für Datenauswertungen, wie sie z.B. für Reporting-Fragestellungen oder Data Analytics Projekte benötigt werden, eher ungeeignet.

Dispositive Datenhaltung. Um analytische und planerische Managementaufgaben oder das Unternehmenscontrolling zu unterstützen, werden dispositive Informationssysteme benötigt. Mit ihrer Hilfe sollen Entscheidungsgrundlagen für das Management erstellt und die Datenbasis für internes oder externes Reporting (z.B. für Solvency II oder IFRS) sowie für moderne Data Science Anwendungen bereitgestellt werden.

Dispositive Informationsverarbeitung basiert auf Daten, die aus operativen Anwendungssystemen und externen Quellen extrahiert wurden. Typischerweise ist das Ziel dieser Extraktion ein Data Warehouse (DWH).

In jüngster Zeit hat sich neben dem DWH ein weiteres dispositives IT System etabliert: der Data Lake. Dieser beruht oft auf der Apache Hadoop Technologie, die wir im Abschnitt 5.8 genauer besprechen. Mit Hilfe eines Data Lakes können sehr groẞe Mengen an strukturierten und unstrukturierten Daten gespeichert und verarbeitet werden. Er ist daher ein idealer Ausgangspunkt für Data Science Use Cases, wobei allerdings ein erhöhter initialer Aufwand zur Datenaufbereitung anfallen kann, vergleiche die Diskussion in Abschnitt 5.5.4.


5.5.2 Data Warehouses


Ein Data Warehouse ist eine von operativen Systemen separierte Datenbank, in der zyklisch (z.B. real time, täglich) Daten aus operativen Systemen zusammengetragen, vereinheitlicht, geordnet, verdichtet und dauerhaft archiviert werden. Anwender haben in der Regel nur lesende Zugriffsrechte. Die Zielsetzung ist hierbei die Verbesserung der unternehmensinternen Informationsversorgung (Wissensmanagement). Ein Data Warehouse dient damit aber auch der Unterstützung strategischer Entscheidungen.


5.5.2.1 Aufbau und Struktur


Abbildung 5.20 zeigt die typische Architektur eines Data Warehouse. Wir werden nun die verschiedenen Schichten von links nach rechts durchgehen und die wichtigsten Begriffe besprechen.

Die Bestückung eines Data Warehouse erfolgt oft mit Daten aus den operationellen Anwendungen, Beispiele sind Bestandsführungssysteme oder etwa ein Customer-Relationship-Managment (CRM) System. In der Regel schreiben diese Systeme in separate Datenbanken, die jeweils unterschiedliche Schemata haben und meist auch von unterschiedlichen Mitarbeitern / Teams aufgebaut und betrieben werden.


Abb. 5.20: Architektur eines Data Warehouse.

Die Staging Area beinhaltet zunächst eine Kopie der operationellen Daten. Diese wird benötigt, weil das Data Warehouse nicht direkt auf die Produktiv-Datenbanken zugreifen sollte (das könnte die operationellen Anwendungen beispielsweise verlangsamen).

Die operationellen Datenbanken enthalten oft viele Datenfelder, die für dispositive Anwendungen uninteressant sind. Im Operational Data Store (ODS) werden nur noch solche Daten gehalten, die für Reports und andere Anwendungen im Warehouse relevant sind. Während in der operationellen Schicht die Daten auf unterschiedliche Datenbanken verteilt waren, werden sie nun in einer gemeinsamen Datenbank mit einem vereinheitlichten Schema zusammengeführt. Um die Daten für Analysen nutzbar zu machen, werden sie auẞerdem bereinigt und validiert, fehlerhafte Einträge und Duplikate werden entfernt.

Im nächsten Schritt werden die Daten historisiert, um in den Reports auch frühere Datenstände zugänglich zu machen. Genau diese Schicht, der Core, wird in Teilen der Literatur als Data Warehouse bezeichnet, was allerdings etwas irreführend sein kann, weil der gesamte Komplex bestehend aus Staging Area, ODS, Core und Data Marts ebenfalls Data Warehouse genannt wird.

Nun gilt es, die Daten für die jeweiligen Reports aufzubereiten, das ist die Aufgabe der Data Marts. Ein Report hat in der Regel einen bestimmten Zweck (z.B. Darstellung der Schadenverläufe) und eine bestimmte Nutzergruppe. Die Data Marts sind kleinere Datenpools, die auf einen bestimmten Report zugeschnitten sind. Sie enthalten nur noch solche Daten, die für den Report relevant sind. Oft liegen die Kennzahlen des Reports schon vorberechnet im Data Mart vor und die Daten sind falls nötig und möglich aggregiert, so dass der Report performant abgerufen werden kann.

Der Systembenutzer schlieẞlich greift auf die Business Intelligence (BI) Anwendungen zu, um Analysen durchzuführen. Das können zum einen Excel-Reports sein, die über eine Eingabemaske erstellt werden. Eine gängige Software dafür ist SAP Business Objects. Eine Alternative, die derzeit kräftig an Popularität gewinnt, sind interaktive und visuelle Dashboards, die mit Tools wie Microsoft Power BI, Tableau, QlikView oder SAS Visual Analytics umgesetzt werden können.

Die Übergänge zwischen den einzelnen Schichten des Data Warehouse werden mit Hilfe von ETL Prozessen realisiert. Die Abkürzung steht für

– E: Extract bezeichnet das Auslesen von Daten aus der vorherigen Schicht oder dem Quellsystem,



– T: Transform steht für die Umformung der Daten,



– L: Load, d.h. die Speicherung der umgeformten Daten in der nächste Schicht.





Diese Prozesse laufen in einem gewissen Zyklus ab, z.B. täglich oder alle zehn Minuten.


5.5.3 Sternschema


Das Datenmodell im Data Warehouse, insbesondere in den Data Marts, ist nicht wie in den operationellen Systemen auf vollständige Normalisierung ausgelegt, sondern hat die effiziente Ausführbarkeit von Abfragen zum Ziel. Ein üblicher Ansatzpunkt hierfür ist das Sternschema, deren zentrales Element die sogenannte Faktentabelle bildet, mit der die Dimensionstabellen verknüpft sind, vgl. zur Illustration Abbildung 5.21.

Die Faktentabelle enthält als Attribute die Kennzahlen, welche im Report ausgewiesen werden sollen. Der Primärschlüssel setzt sich aus den einzelnen Primärschlüsseln der Dimensionstabellen zusammen. Die Kennzahlen können dadurch mit den Attributen der Dimensionstabellen in Verbindung gebracht und analysiert werden. Die Faktentabelle wird fortlaufend mit neuen Daten erweitert und ist meist verhältnismäẞig groẞ. Im Beispiel aus Abbildung 5.21 würde die Faktentabelle sämtliche Verträge beinhalten.

Die Dimensionstabellen enthalten beschreibende Attribute, wie zum Beispiel den Kunden- oder Mitarbeiternamen. Sie sind meist deutlich kleiner als die Faktentabelle und werden vergleichsweise selten aktualisiert. Es besteht eine 1:n Beziehung zur Faktentabelle, d.h. für einen Eintrag in der Dimensionstabelle kann es mehrere verlinkte Zeilen in der Faktentabelle geben.

In der Regel verletzen die Dimensionstabellen die dritte Normalform. Als Beispiel betrachten wir die Kundentabelle in Abbildung 5.21. Hier besteht eine funktionale Abhängigkeit der Spalte Arbeitgeber Branche zu einem Nichtschlüsselattribut Arbeitgeber Name. Um die Tabelle in die dritte Normalform zu überführen, müsste eine neue Tabelle Arbeitgeber erstellt werden, die dann mit der Kundentabelle verknüpft wäre. Darauf wird bewusst verzichtet, weil sonst ein tief verzweigtes Datenmodell entstehen würde (Schneeflockenschema), dass mehrstufige Verknüpfungen von Tabellen in Abfragen erforderlich machen würde.


Abb. 5.21: Vereinfachtes Beispiel zur Illustration des Sternschemas anhand eines Versicherungsbestands. Im Zentrum befindet sich die Faktentabelle umringt von Dimensionstabellen. Die unterstrichenen Attribute bilden jeweils den Primärschlüssel.

Durch die beschriebene Vorgehensweise erhält man neben einer verbesserten Performanz auch ein einfaches und intuitives Datenmodell.Auf der anderen Seite werden jedoch redundante Informationen vorgehalten: Im obigen Beispiel könnten mehrere Kunden bei einem Arbeitgeber angestellt sein, so dass Duplikate von der Arbeitgeber Branche entstehen. Dies kann zur Verletzung der Datenintegrität führen, etwa wenn beim Aktualisieren der redundanten Dimensionsattribute nicht alle Duplikate angepasst werden. Die Vor- und Nachteile einer nicht durchgeführten Normalisierung müssen in jedem Fall gegeneinander abgewogen werden.


5.5.4 Data Lakes


Data Warehouses sind seit Jahrzehnten für Reporting-Anwendungen in den Versicherungsunternehmen etabliert. Sie weisen in der Regel eine hohe Datenintegrität auf (d.h. man kann den Zahlen vertrauen) und die entsprechenden Reports sind auf die Unterstützung bestimmter Prozesse (z.B. Solvency II Reporting) zugeschnitten. Für diese Standard Reportings werden Data Warehouses auch noch lange im Einsatz sein.

Für die Umsetzung moderner Big Data Analytics Use Cases sind DWHs jedoch nur bedingt geeignet, unter anderem, weil:

– die IT Entwicklungszeiten lange und die Aufwände hoch sind,



– die unterliegenden relationalen Datenbankmanagementsysteme in der Regel keine sehr groẞen Datenmengen speichern bzw. verarbeiten können und auch die Speicherkosten verhältnismäẞig hoch sind,



– die Daten in tabellarischer Form, also strukturiert, vorliegen müssen,



– das Schema der Tabellen im Vorhinein festgelegt werden muss und nur mit hohem Aufwand angepasst werden kann (Schema on Read, mehr dazu unten).





Viele Unternehmen haben deshalb angefangen, parallel zu einem bestehenden Data Warehouse, einen sogenannten Data Lake aufzubauen, auf den wir nun näher eingehen werden.


Konzept und Aufbau


Die grundlegende Idee eines Data Lakes ist die Bereitstellung eines zentralen und hochdimensionierten Datenspeichers, der mit möglichst vielendem Unternehmen zur Verfügung stehenden Daten – die potentiell für Analysen relevant sein könnten – versorgt wird. Zudem werden verschiedenste Analytics Tools angebunden, so dass der Data Lake eine konzernweite Analytics Plattform mit zentraler Datenhaltung darstellt.

Die Daten können in verschiedensten Formaten vorliegen, strukturiert als Tabellen, semistrukturiert (z.B. im Json Format) oder unstrukturiert in Form von Texten, Bildern oder Videos.

Wesentlich ist auch die zentrale Bereitstellung der Daten. Idealerweise gibt es nur den einen Datentopf, auf den alle Konzernmitarbeiter, die Datenanalysen durchführen, zugreifen. Dadurch wird die Umsetzungsdauer für Data Analytics Use Cases deutlich gesenkt, weil die Dauer für die Datenbeschaffung entscheidend verkürzt wird. In der Praxis dürfen natürlich nicht alle Mitarbeiter auf alle Unternehmensdaten zugreifen, zum Beispiel aufgrund von Datenschutzbestimmungen oder internen Vertraulichkeitseinstufungen. Daher müssen Zugriffsrechte differenziert vergeben und eventuell doch verschiedene Datentöpfe gebildet werden.

Als Speichertechnologie für Data Lakes hat sich das Hadoop Distributed File System (HDFS) durchgesetzt, bei dem die Daten nicht auf einem einzelnen Rechner, sondern auf einem ganzen Cluster von Computern redundant gespeichert werden. Es können damit sehr groẞe Datenmengen in unterschiedlichen Fileformaten und zu verhältnismäẞig niedrigen Kosten gespeichert und verarbeitet werden. Eine genauere Beschreibung von HDFS findet sich im Abschnitt 5.5.4.


Abb. 5.22: Data Lake Grundbausteine.

Eine Herausforderung besteht darin, die vielen verschiedenen Datensets im Data Lake für die Nutzer auffindbar zu machen. Zu diesem Zweck gibt es spezialisierte Software, die eine Katalogisierung der Daten ermöglicht (Data Catalog). Hierzu sind die Daten mit Metainformationen (z.B. Beschreibungen, Schema Definition, Tags, Data Owner Name) zu versehen und es werden Suchfunktionen bereitgestellt, um die Daten (z.B. aufgrund des Dateinamens oder der Metadaten) aufzufinden.

Im Gegensatz zum DWH gibt es für Data Lakes noch keine einheitliche Architektur. Man findet im Internet unterschiedlichste Darstellungen. Es gibt jedoch gewisse Grundbausteine die sich schon jetzt herauskristallisiert haben und die in Abbildung 5.22 dargestellt sind.

Der Datenfluss ist dabei von links nach rechts. Die Bullet Points in den einzelnen Blöcken sind natürlich unvollständig und sollen lediglich eine Idee davon geben, was sich hinter den einzelnen Blöcken verbirgt.


Abgrenzung zu Data Warehouse


Wie oben bereits erwähnt, unterscheiden sich Data Warehouse und Data Lake hinsichtlich des Anwendungsbereichs (Standardreporting vs. Big Data Analytics). Ein wesentlicher Aspekt dabei ist der Zeitpunkt der Schema-Definition, auf den wir nun näher eingehen.36

Ein Datawarehouse setzt das Konzept Schema on Write um, wonach das Schema schon zum Zeitpunkt der Erstellung des Datenmodells festgelegt wird. Beim Schreiben von Daten müssen diese dann genau in die durch das Schema vorgegebene Schablone passen. Neue Attribute können nur dann aufgenommen werden, wenn vorher eine Änderung am Datenmodell erfolgt ist (was typischerweise einen IT Change Request mit Planungs-, Implementierungs- und Testaufwänden erfordert). Das Vorgehen unterstützt bei der Wahrung der Datenintegrität und führt zu niedrigen Aufwänden beim Auslesen, weil die Daten bereits in Tabellen vorliegen und für Analysen also nicht erst in ein tabellarisches Format gebracht werden müssen. Demgegenüber stehen als Nachteile:

– die Erschlieẞung neuer Datenquellen ist mit groẞen Aufwänden verbunden,



– zeitliche Änderungen am Schema sind nicht vorgesehen,



– die Daten können nur in tabellarischer Form gespeichert werden.





Beim Data Lake werden die Daten typischerweise in ihrem ursprünglichen Format abgelegt (z.B. Text, Json, Bild, Audio). Zu einem späteren Zeitpunkt, wenn die Daten für Analysezwecke ausgelesen werden, müssen sie dann in der Regel in tabellarische Form überführt werden (das ist insbesondere für die Anwendung von Machine Learning Modellen erforderlich). Der Analyst muss sich dann für ein Schema entscheiden (daher die Bezeichnung Schema on Read). Für semi- und unstrukturierte Daten ist die Wahl des Schemas normalerweise nicht eindeutig. So kann ein verschachtelter Json Datensatz in verschiedene tabellarische Formate umgeformt werden, vgl. die Diskussion in Abschnitt 5.2.1. Die Wahl des Schemas hängt dann vom jeweiligen Use Case ab. Unter diesen Prämissen lassen sich Daten im groẞen Stil, unabhängig von ihrem Format, sammeln (auch wenn der Use Case zum Zeitpunkt der Datensammlung noch nicht bekannt ist). Eine Änderung des Schemas über die Zeit stellt – zumindest beim Sammeln – kein Problem dar. Gröẞere Aufwände entstehen ggf. aber zum Zeitpunkt der Analyse, weil die Daten erst validiert und transformiert werden müssen.

Eine (unvollständige) Übersicht der Unterschiede von Data Warehouse und Data Lake findet sich in der Tabelle in Abbildung 5.23.


Hadoop Distributed File System


Data Lakes basieren auf der Apache Hadoop Technologie, die wir in Abschnitt 5.8 noch detaillierter behandeln werden. Sie ermöglicht es, sowohl groẞe Datenmengen zu speichern, als auch diese Daten algorithmisch in Form von Map-Reduce-Jobs zu verarbeiten. Im Folgenden liegt der Fokus auf der Speichertechnologie, die auch Hadoop Distributed File System (HDFS) genannt wird.


Abb. 5.23: Gegenüberstellung Datawarehouse/Data Lake.

Die Idee dahinter geht auf eine Veröffentlichung von Google37 zurück. Hier wird die damalige in-house Lösung von Google, das Google File System, vorgestellt. Das Konzept wurde dann federführend von Doug Cutting and Mike Cafarella aufgegriffen, in Java implementiert, und als Open Source Lösung bereitgestellt.

Die grundlegende Idee lässt sich folgendermaẞen zusammenfassen: Daten werden in Blöcke zerteilt, wobei Blöcke eine vorgegeben Gröẞe (beispielsweise 64 MB) haben. Die Blöcke werden dann auf einem Cluster, also auf mehreren vernetzten Computern, verteilt abgelegt, so dass auch sehr groẞe Datenmengen Platz haben. Für die Computer des Clusters (im Folgenden Nodes genannt) wird Standard-Server-Hardware (commodity hardware) gewählt, wodurch der Cluster relativ kostengünstig ist.

Bei groẞen Clustern kann es immer wieder zu Ausfällen einzelner Nodes kommen. Um keine Datenblöcke zu verlieren, werden die Blöcke in HDFS meist dreifach repliziert, d.h. von jedem Block gibt es zwei weitere Kopien, die auf anderen Nodes abgelegt werden. Der Cluster funktioniert daher auch bei Ausfällen fehlerfrei, was als fault tolerance bezeichnet wird.

Aufgrund der genannten Eigenschaften skalieren Hadoop Cluster sehr gut. Clustergröẞen von 1 000 Nodes und mehr sind nicht unüblich. Der schematische Aufbau von HDFS ist in Abbildung 5.24 veranschaulicht.

Der Master Node übernimmt die Koordination auf dem Cluster. Dort arbeitet eine Softwarekomponente, welche die Metadaten des Filesystems speichert (z.B. Filenamen, Zugriffsrechte, die Orte der einzelnen Blöcke und welche Blöcke zu welchem File gehören). Die Datenblöcke selbst (hier HDFS Blöcke genannt) sind auf den Slave Nodes gespeichert. Jeden Block gibt es in beispielsweise dreifacher Ausführung (um wie erläutert fault tolerance zu gewährleisten), was hier durch die Farben illustriert ist. Zum Beispiel gibt es vom lila Block zwei Kopien.


Abb. 5.24: Architektur des Hadoop Distributed File Systems.

Auf jedem Slave Node läuft nun auẞerdem eine Softwarekomponente, welche die Speicherung der Blöcke und die Kommunikation mit dem Name Node übernimmt.


Verbindungen von Hadoop zu No-SQL-Technologien


Es gibt einige Gemeinsamkeiten zwischen Hadoop/HDFS und No-SQL. Beide Technologien

– ermöglichen es, groẞe und schnell wachsende Datenmengen unterschiedlichen Formats zu speichern,



– bestehen aus Clustern von herkömmlichen Computern (d.h. günstiger commodity hardware),



– erlauben horizontale Skalierung (vgl. Abschnitt 5.7) – bei Bedarf können mehr Nodes hinzugenommen werden,



– erlauben zeitliche Veränderungen im Schema / Format.





Dennoch gibt es Unterschiede, sowohl in den Eigenschaften wie auch im Anwendungsbereich. Hadoops Ausrichtung und Stärke liegt in der Anwendung von Algorithmen (z.B. Map Reduce, siehe 5.8) auf wirklich groẞen Datenmengen, wodurch sich das Tool für (predictive) Analytics in Verbindung mit groẞen Datenmengen eignet. No-SQL-Datenbanken ermöglichen teilweise hochperformante Zugriffe und eignen sich damit u.a. für die Umsetzung von Benutzerschnittstellen, beispielsweise als Backend in Web Anwendungen. Tatsächlich werden in vielen Big Data Analytics Infrastrukturen beide Technologien eingesetzt, aber eben für unterschiedliche Aufgaben und Zwecke.


5.6 Softwaretests


Tests haben allgemein das Ziel, sich der korrekten Funktionsweise von Software anhand von Stichproben möglicher Abläufe zu versichern. Obwohl seit einiger Zeit integraler Bestandteil eines industrialisierten Softwareentwicklungsprozesses und aktives Forschungsgebiet ([89]) haben sich die Tests im Bereich Data-Analytics vermutlich noch nicht in gleichem Maẞe etablieren können.

Ansätze wie Test Driven Development (s.u.) können jedoch mit entsprechender Disziplin (oder durch verbindliche Vorgaben an die Analysten) auch für die Entwicklung von Data-Science-Modulen oder im aktuariellen Kontext allgemein eingesetzt werden.

Dieser Abschnitt widmet sich Ansätzen, die bei einem automatisierten Testen verwendet werden können. Hier steht der Gedanke im Vordergrund, durch die automatische Ausführung von Tests (etwa als Teil der Builds38 oder nach einem Push in ein Repository) eine Reihe von Checks ablaufen zu lassen, die den bestehenden sowie neu hinzugekommenen Code prüfen.

Tests lassen sich also einsetzen, um

– die Korrektheit des Codes (z.B. von eingesetzten Algorithmen) durch Testfälle zu prüfen (Komponententest),



– die Spezifikation der Funktionalitäten festzuhalten und deren Einhaltung zu validieren,



– die Integrität von bestehenden Funktionalitäten gegenüber neuen Codeänderungen zu gewährleisten (Regressionstest / Integrationstest),



– das Zusammenspiel verschiedener unabhängig entwickelter Komponenten zu prüfen (Systemtest).





Allerdings sind Tests nicht nur nötig und sinnvoll, um die eigene Arbeit zu validieren, sie sind sogar gesetzlich vorgeschrieben. Einmal in den Grundsätzen ordnungsgemäẞer Buchführung (GoBD, [34]) und dann auch noch in den versicherungsaufsichtrechtlichen Anforderungen an die IT (VAIT, [98]).

Tests können nun auf verschiedenen Ebenen angesiedelt werden, oft betrachtet man hier einzelne Module, gröẞere Programmteile und das System als Ganzes. Wichtig ist es in diesem Zusammenhang auch, sich über die Gewichtung der Tests der einzelnen Komponenten Gedanken zu machen. Bewährt hat sich hier das Pyramidenmodell von Cohn ([18]), vgl. Abbildung (5.25). Generell wird ein hoher Anteil an Modultests und ein geringerer Anteil an (aufwendigen) Integrations- und Systemstests empfohlen.


Abb. 5.25: Testpyramide nach Cohn, in der Literatur finden sich aber auch andere Ansätze (wie etwa Testdiamanten).


Unit-Tests


An dieser Stelle soll ein Blick auf Testrunner im Rahmen eines kleinen Beispiels geworfen werden. Inzwischen gibt es in jeder Programmiersprache mehrere Testframeworks, die oft nach jUnit, einem sehr bekannten Java-Framework modelliert sind.

Die Idee ist hier, dass der Entwickler Testcode schreibt, der durch entsprechende Frameworks oder andere Konstrukte als solcher identifiziert und vom Testframework ausgeführt wird. Bei jUnit ist dies über Annotationen gelöst, die eine Klasse als spezielle Testklasse auszeichnen, bei pytest reicht es, entsprechende Testfunktionen mit dem Namensteil test_ beginnen zu lassen.

Es folgen ein jUnit-Beispiel, das eine selbstgeschriebene Counter-Klasse testet und die Implementierung des getesteten Klasse Counter.

public class TestCounter {

@Test public void testSimpleInsert() {

Counter c = new Counter();

String fruit = "Apple";

c.addItem(fruit);

assert(c.getCount(fruit) == 1); // eigentlicher Test!

c.addItem(fruit);

assert(c.getCount(fruit) == 2); // eigentlicher Test!

}

@Test public void testMultiple() {

Counter c = new Counter();

List<String> fruits = new ArrayList<String>();

fruits.add("Apple");

fruits.add("Orange");

fruits.add("Apple");

c.addItems(fruits);

assert(c.getCount("Apple") == 2);

assert(c.getCount("Orange") == 1);

assert(c.getCount("Pineapple") == 0);

}

}

Die mit @Test annotierten Methoden sind die Testfälle, es werden jeweils Instanzen der zu testenden Klasse erzeugt und in einen bestimmten Zustand gebracht. Dann erfolgt die Überprüfung des Zustandes mit den assert-Aufrufen. Die getestete Implementierung könnte beispielsweise so aussehen:

public class Counter {

private Map<String, Integer> map;

public Counter() {

map = new HashMap<String, Integer>();

}

public void addItem(String item) {

map.put(item, this .getCount(item) + 1);

}

public void addItems(List<String> items) {

items.forEach(i -> this .addItem(i));

}

public int getCount(String item) {

return map.getOrDefault(item, 0);

}

public Map<String, Integer> getCounts() {

return new HashMap<String, Integer>(map);

}

}

Führt man die Test-Klasse aus (z.B. in Eclipse), sieht man eine Ausgabe wie in Abbildung (5.26).


Abb. 5.26: Unit-Test: Darstellung des Test-Protokolls in der Entwicklungsumgebung Eclipse.

Die Testklasse behält man von nun an bei und erweitert sie bei Bedarf und führt sie zu passenden Gelegenheiten (beispielsweise nach Code-Anpassungen an der entsprechenden Klasse, oder automatisiert bei jedem Build) aus, um sich davon zu überzeugen, dass der Counter noch funktioniert. In realen Projekten kann es dann Hunderte (oder Tausende) von Testfällen geben.

Das Prinzip funktioniert ganz analog mit pytest. Will man z.B. im aktuellen Python-Projekt einen Testfall anlegen, der eine Password-Abfrage überprüft, erstellt man eine Datei namens test_login.py (z.B. im Unterordner test) und darin the Methode test_admin_login, die den Testcode enthält. Führt man dann im aktuellen Verzeichnis pytest aus, wird die Methode ausgeführt und ein Resultat ausgegeben:

D:\programming\py\my_project\src>pytest tests

============================= test session starts =============================

platform win32 -- Python 3.6.1, pytest-3.6.0, py-1.5.3, pluggy-0.6.0

rootdir: D:\programming\py\my_project\src, inifile:

collected 1 item

tests\test_login.py E [100%]

=================================== ERRORS ====================================

_____________________ ERROR at setup of test_admin_login ______________________

Der Test ist fehlgeschlagen, es folgen noch die Fehlerausgaben.

Zu den Tools gäbe es noch einiges zu sagen. Wichtig ist z.B. oft ein fixierter Ausgangszustand, um gleiche Bedingungen bei jeder Ausführung zu gewährleisten. Allerdings würde ein tiefer Blick in das Erstellen und Ausführen von Tests den Rahmen dieses Buches überschreiten. Hier bieten die entsprechenden Frameworks weitere Unterstützung.


Test Driven Development (TDD)


TDD39 hat ihren Ursprung im sog. Extreme Programming (einer Methode der Softwarentwicklung, vgl. [11]). Bei TDD stellt man vor der Entwicklung der eigentlichen Funktionalität die benötigten Tests fertig. Die Tests sollen die erwartete Funktionalität prüfen und die API definieren und dokumentieren. Der Prozess der Softwareentwicklung stellt sich dann wie in Abbildung 5.27 dar.


Abb. 5.27: TDD-Zyklus.

Zunächst werden die Testfälle geschrieben (die erst mal fehlschlagen). Dann wird die Funktionalität programmiert, bis die Tests erfolgreich sind. Anschlieẞend wird optimiert, wobei danach die Tests immer noch erfolgreich sein müssen.

Eine wichtige Frage ist, wie man Code testbar macht. Hier ist neben einer sinnvollen Modularisierung die Idee der Dependency Injection inzwischen allgemein akzeptiert. Das bedeutet, dass zu testende Methoden oder Funktionen so angelegt werden, dass sie benötigte Abhängigkeiten von auẞen übergeben bekommen. Also nicht

def test_credentials(username, pwd):

db = createDatabaseConnection()

db.query("...")

# ...

sondern

def test_credentials(username, pwd, db):

db.query("...")

# ...

Man spricht dann auch vom Entkoppeln der Komponenten. Hier wird das Object db an die Methode übergeben. Man kann es gegebenenfalls durch eine Instanz einer einfacheren Testklasse ersetzen, die ein kompatibles Interface aufweist (ein sog. mock-Objekt), in der evt. also gar keine Datenbankabfrage ausführt wird (evt. existiert die Database-Connection-Klasse auch noch gar nicht). So kann man sich beim Test auf den Kern der Spezifikation des zu testenden Codeteils konzentrieren und Programmteile unabhängig voneinander testen.

Oft scheinen bei Ausführung der Tests manuelle Schritte notwendig, um z.B. Benutzereingaben zu berücksichtigen. Teilweise kann dies durch andere Tools wieder umgangen werden. Ein interessantes Framework zur Simulation von Nutzeraktionen im Browser ist das schon in Abschnitt 5.2.3 erwähnte Tool Selenium. Damit wird es möglich, Abläufe wie Mausklicks und Tastatureingaben im Browser zu simulieren. Sind Aktionen auẞerhalb des Browsers notwendig, kommt man mit der Ablauf-Automatisierung in den Bereich der Robotic Process Automation. Hier gibt es neben kommerziellen Angeboten auch freie Tools, deren Anwendung weit über automatisierte Testläufe hinausgehen (z.B. AutoIt40).


5.7 Parallele Datenverarbeitung


Skalierbarkeit


Skalierbarkeit bezeichnet allgemein die Fähigkeit eines Systems zur Gröẞenveränderung, üblicherweise wird im IT Kontext darunter die Fähigkeit zuwachsen verstanden. Oder noch konkreter: die Fähigkeit, durch Hinzufügen von Komponenten die Leistungsfähigkeit eines Systems zu steigern. Gerade im Bereich Data Science ist das ein wichtiger Gesichtspunkt, da die behandelten Fragestellungen schnell dazu tendieren gröẞer zu werden – sowohl im Sinne von komplizierteren und komplexeren Methoden als auch im Sinne gröẞerer Datenmengen.

Hierbei werden vertikale und horizontale Skalierung unterschieden. Vertikale Skalierung (scale up) stellt im IT-Bereich die nächstliegende Möglichkeit zur Leistungssteigerung dar: einen Ausbau der verfügbaren Rechenleistung durch schnellere CPUs, mehr Haupt- und Massenspeicher, schnellere Netzwerkanbindung etc. Offenbar gibt es hier aber auch relativ nahe liegende Grenzen der Skalierbarkeit. Potenziell unbegrenzt ist der im Folgenden beschriebene Ausbau in der Breite. Ein wesentlicher Vorteil der vertikalen Skalierung liegt hingegen darin, dass grundsätzlich keine Änderungen an den Algorithmen und Programmen erforderlich sind.

Horizontale Skalierung (scale out) bedeutet eine Skalierung durch Hinzufügen weiterer Rechner bzw. Knoten. Grundsätzlich kann eine beliebige Anzahl von weiteren Einheiten dem System hinzugefügt werden, allerdings sind i.A. Änderungen an den Algorithmen erforderlich, damit die zusätzlich zur Verfügung gestellten Ressourcen auch tatsächlich genutzt werden können (Parallelisierbarkeit).

Neben den Aspekten der technischen Umsetzung von Skalierbarkeit werden folgende konzeptionelle Arten von Skalierbarkeit unterschieden:

– Lastskalierbarkeit: Sie betrifft die Skalierbarkeit in Bezug auf die Belastung des Systems. Ein gutes Beispiel hierfür sind Webseiten bzw. Webserver, die auch bei Lastspitzen, d.h. also einer hochfrequenten Nachfrage, stets responsiv sein sollten.



– Räumliche Skalierbarkeit: Hier geht es um die Ressourcenbeanspruchung bei steigenden Anforderungen, insbesondere bei der Datenspeicherung. Der Einsatz von Kompressionsverfahren in der Datenspeicherung kann beispielsweise dazu führen, dass der Speicherbedarf nur sublinear mit der Anzahl der gespeicherter Datensätze wächst.



– Zeitlich-räumliche Skalierbarkeit: Dies betrifft den zeitlichen Ressourcenverbrauch bei steigender Anzahl von relevanten Objekten. Ein Beispiel hierfür sind Suchmaschinen, die in zeitlich-räumlicher Sicht gut skalieren, wenn ein Suchindex genutzt wird. Sie skalieren jedoch bei einfacher, linearer Suche schlecht.



– Strukturelle Skalierbarkeit: Hier geht es um strukturbedingte Grenzen im Wachstum eines Systems. Ein Gegenbeispiel wäre ein System mit hart implementierten Limits (z.B. mögliche Anzahl der Einträge in ein Verzeichnis < 1 Mio.), ein Positivbeispiel ein System ohne solche grundsätzlichen (strukturellen) Grenzen, das mit den Möglichkeiten der Hardware wächst.





Aufgrund der begrenzten Möglichkeiten zur einfacheren vertikalen Skalierung ist die horizontale Skalierung das Mittel der Wahl, um hohe Skalierbarkeit darzustellen. Wiebereits erwähnt, setzt dies aber die Parallelisierbarkeit von Algorithmen und/oder Datenhaltung voraus.

Häufig fällt in diesem Zusammenhang auch der Begriff der Nebenläufigkeit. Dieser beschreibt die Fähigkeit von Algorithmen parallel zu laufen, ohne dass dies notwendig auch so erfolgen muss. Nebenläufige Algorithmen können aber auf geeigneten Systemen stets parallel laufen. Im Folgenden werden die wichtigsten hierfür relevanten Konzepte vorgestellt.





Verteilte Systeme / Multiprocessing / Multithreading


Der Begriff der verteilten Systeme geht auf Andrew S. Tanenbaum, den Entwickler von Minix (einem für Lehrzwecke entworfenen Betriebssystem), zurück und beschreibt einen Zusammenschluss unabhängiger Computer, auf denen miteinander interagierende aber ansonsten unabhängige Prozesse laufen. Insbesondere läuft jeder Prozess mit einem eigenen Speicherbereich, auf den von den anderen Prozessen nicht zugegriffen werden kann.

Aufgrund der strikten Trennung der Speicherbereiche eignen sich verteilte Systemebzw. Multiprocessing für parallelisierbare Aufgaben gut, bei denen lediglich wenig Daten ausgetauscht werden müssen. Auẞerdem sollten nicht zu viele Prozesse erzeugt werden müssen, da das Erzeugen eines Prozesses einen relativ aufwändigen Vorgang darstellt.

Will man in Python parallel rechnen, kann man auf das multiprocessing-Modul der Standard-Library zurückgreifen. Wir führen das Beispiel aus Abschnitt 4.2.2.2 fort. Die dortige Berechnung stehe hier als Funktion simulate_pf_losses im Kontext zur Verfügung. Die Aufgabe sei nun, die Simulation für verschiedene Eingabeparameter durchzuführen. Das kann man wie folgt erreichen:

from multiprocessing import Pool, cpu_count

# Liste von Parametersaetzen

PARAMS = [

{"_id": 0, "rho": 0.05, "p": 0.003, "lgd": 0.6, "its": 5000, "N": 10000},

{"_id": 1, "rho": 0.2, "p": 0.003, "lgd": 0.6, "its": 5000, "N": 10000},

{"_id": 2, "rho": 0.05, "p": 0.01, "lgd": 0.6, "its": 5000, "N": 10000},

{"_id": 3, "rho": 0.1, "p": 0.01, "lgd": 0.6, "its": 5000, "N": 10000}

]

# Container für die Ergebnisse

mkt_losses_save = [None] * len(PARAMS)

def par_wrapper(p):

return (simulate_pf_losses(**p), p)

pool = Pool(cpu_count())

for market_losses_bndl in pool.map(par_wrapper, PARAMS):

market_losses, p = market_losses_bndl

mkt_losses_save[p["_id"]] = market_losses

Das Objekt pool vom Typ multiprocessing.Pool verwaltet hierbei einen Prozesspool und stellt über die Methode map eine Möglichkeit bereit, eine Funktion (erstes Argument) für mehrere Argumente (zweites Argument) parallel abzuarbeiten.

Nach dem Durchlauf stehen die berechneten Ergebnisse in mkt_losses_save bereit. Da in unserem Beispiel die Arbeitsfunktion simulate_pf_losses mehrere Argumente entgegennimmt aber die map-Methode nur eines übergibt wird die Hilfsfunktion par_wrapper eingeführt, die das erhaltene Argument-Tupel geeignet weitergibt und gleichzeitig noch den Eingabeparameter mit dem Resultat zusammen zurückgibt, wodurch es möglich wird, die Ergebnisse der parallelen Abarbeitung wieder richtig zu ordnen.

Ein wenig anders liegt der Fall beim sogenannten Multithreading. Threads werden auch leichtgewichtige Prozesse genannt und sind im Unterschied zu echten Prozessen günstig zu erzeugen. Threads laufen im Kontext eines Prozesses und teilen sich die Ressourcen des Prozesses, insbesondere den Speicherbereich sowie geöffnete Dateien. Hierdurch ist ein sehr schneller Datenaustausch möglich. Allerdings ist es eine anspruchsvolle Aufgabe, die Threadfähigkeit von Programmen sicherzustellen. Insbesondere muss für sämtliche Bibliotheks- und sonstigen Funktionsaufrufe die Threadsicherheit sicher- bzw. durch die Nutzung von Mutexen oder Semaphoren überhaupt erst einmal hergestellt werden. Grundsätzlich stellt Multithreading aber eine relativ schnelle und einfache Möglichkeit zur horizontalen Skalierung dar. Die praktische Umsetzung von Multithreading im soeben erläuterten Sinn ist in Python nativ nicht möglich, die threading-Module der Standard-Library unterstützen derzeit keine echt parallele Ausführung der erzeugten Threads.41 Will man dies dennoch erreichen, ist ein möglicher Weg die Nutzung von C/C++-Extensionmodulen (und darin z.B. die Nutzung des OpenMP-Standards42), wobei die Parallelisierung dann natürlich auf den C/C++-Code beschränkt bleibt.


GPU-Processing (SIMD)


Gerade im Bereich Data Science trifft man häufig auf Probleme, in denen dieselben Anweisungen auf verschiedene Daten anzuwenden sind, z.B. typische Operationen der Linearen Algebra wie die Matrizenmultiplikation aber auch das Trainieren eines neuronalen Netzes. Eine gebräuchliche Abkürzung für Aufgaben dieser Art ist SIMD (Single Instruction, Multiple Data), sie können mit modernen Grafikkarten oder mit spezieller Hardware, die genau für solche Aufgabenstellungen konstruiert worden ist, sehr effizient bearbeitet werden43. Aber auch moderne CPUs stellen mit Advanced Vector Extensions, AVX und dessen Erweiterungen SIMD-Funktionalität zur Verfügung.

Im Unterschied zu den Prozessoren gibt es im Bereich der Grafikkarten keine Vereinheitlichung. NVIDIA bietet eine proprietäre SIMD-Funktionalität namens CUDA (Compute Unified Device Architecture), ansonsten gibt es noch den übergreifenden Standard OpenCL (Open Computing Language) der sogenannten Khronos Group44. Packages zur Nutzung von OpenCL in Python sind über das Internet leicht zu finden.


Parallel Collections


Im Folgenden thematisieren wir einige Aspekte, die bei Parallelprogrammierung zu berücksichtigen und einige Tücken, die zu beachten sind. Oftmals ist die Parallelisierbarkeit aber recht einfach umzusetzen, das betrifft insbesondere die Problemstellungen, die in die Kategorie der map-reduce-Probleme fallen (siehe Abschnitt 5.8).

Gerade im Bereich der Data-Analytics benutzt man gerne Abstraktionen (wie die Programmiersprachen Python, R oder Scala) zum Zugriff auf komplizierte Data-Analytics-Algorithmen, so dass es sich anbietet, auch für einfachere Parallelisierungen eine Abstraktion anzubieten. Dies bietet beispielsweise die Programmiersprache Scala45 mit den sogenannten parallel collections. Hierbei handelt es sich um Sammlungen von Objekten (da Skalare nicht parallelisiert werden können), für die Routinen zur parallelen Berechnung verfügbar sind. Diese Sammlungen umfassen

– Arrays,



– Vektoren,



– Bereiche,



– diverse veränderbare und unveränderbare Mappingtypen.





Beispiel:

val list = (1 to 1000).toList.par

list.reduce(_+_)

Diese Addition der ersten tausend Zahlen wird dann (je nach System) parallel durchgeführt. Ähnliche Konstrukte stehen teilweise auch in anderen Sprachen zur Verfügung.


Probleme der Parallelisierbarkeit


Ein spezifisches Problem von Multi-Threading-Anwendungen entsteht bei gleichzeitigem Zugriff mehrerer Anwendungen (bzw. Threads) auf ein geteiltes Datum, wenn mindestens einer der Zugriffe schreibend erfolgt. Durch die Möglichkeit, mittels shared memories auf gemeinsam genutzte Speicherbereiche zugreifen zu können, gilt das analog aber auch für Multiprocessing-Aufgabenstellungen.

Sofern auch nur eine Anwendung die Daten ändert, ist es ohne explizite Synchronisierungsmechanismen nicht klar, ob die lesenden Anwendungen den ursprünglichen oder den neuen Wert erhalten (oder gar einen temporär erzeugten dritten). Noch schlimmer ist es, wenn mehrere Anwendungen eine Information gleichzeitig ändern wollen, da hier der Endwert der Information nicht determiniert ist. Hängt der neue Wert gar vom alten ab, können auf diese Weise sogar undefinierte bzw. falsche Werte auftreten.

Atomarität bedeutet, dass gewisse Zugriffe auf Objekte nicht teilbar sind, sondern als Einheit ausgeführt werden. Hierdurch kann vermieden werden, dass durch unglückliches Timing undefinierte Zustände auftreten. Das folgende Beispiel zeigt anhand einer globalen Variable, die eigentlich nur gerade Werte annehmen sollte, dass durch fehlende Atomarität (der Funktionen f1 und f2) auch ungerade Werte gelesen werden können.

import time

import threading

# globale Variable, die nur gerade Zahlen enthalten soll

Gerade = 0

def f1():

global Gerade

print("f1 adding 1...", flush=True)

Gerade = Gerade + 1

time.sleep(1)

print("f1 adding 1 again ...", flush=True)

Gerade = Gerade + 1

def f2():

global Gerade

print("f2 adding 2...", flush=True)

time.sleep(0.25)

Gerade = Gerade + 2

def f3():

time.sleep(0.5)

print("Wert in f3:", Gerade)

Die soeben definierten Funktionen werden nun in unterschiedlichen Threads zur Ausführung gebracht.

# Erzeugen der Threads

thread1 = threading.Thread(target = f1)

thread2 = threading.Thread(target = f2)

thread3 = threading.Thread(target = f3)

# Starten der Threads

thread1.start()

thread2.start()

thread3.start()

# Warten auf Beenden der Threads

thread1.join()

thread2.join()

thread3.join()

# Programmende

print("Wert zum Ende:", Gerade)

f1 adding 1...

f2 adding 2...

Wert in f3: 3

f1 adding 1 again ...

Wert zum Ende: 4

Offenbar (drittletzte Zeile) wird der eigentlich nicht erwartete Wert 3 von der Funktion f 3 verarbeitet. Der Ablauf ist in diesem Beispiel wegen der groẞen Wartezeiten quasi deterministisch und gut nachvollziehbar. Bei realistischen Abläufen mit Wechselwirkungen im Mikro- oder Nanosekundenbereich sind derartige Probleme oft schwer zu erkennen, zumal wenn sie nicht mehr deterministisch auftreten, sondern nur sporadisch. Locking erlaubt es, das Atomaritätsproblem in den Griff zu bekommen, birgt aber, wie wir nachfolgend sehen werden, seinerseits wiederum Risiken wie das von Deadlocks. Locks erlauben durch explizites Nachfragen nach der Verfügbarkeit einer Ressource die Zugriffe zu synchronisieren. Wir betrachten ein ähnliches Beispiel:

import time

import threading

# globale Variable, die nur gerade Zahlen enthalten soll

Gerade = 0

# lock für Gerade

lock_gerade = threading.Lock()

def f1():

global Gerade

lock_gerade.acquire()

print("f1 adding 1...", flush=True)

Gerade = Gerade + 1

time.sleep(1)

print("f1 adding 1 again ...", flush=True)

Gerade = Gerade + 1

lock_gerade.release()

def f2():

global Gerade

lock_gerade.acquire()

print("f2 adding 2...", flush=True)

time.sleep(0.25)

Gerade = Gerade + 2

lock_gerade.release()

def f3():

time.sleep(0.5)

lock_gerade.acquire()

print("Wert in f3:", Gerade)

lock_gerade.release()

Das Ausführen der Funktionen erfolgt dann wie oben bereits gesehen, man erhält die folgende Ausgabe:

f1 adding 1...

f1 adding 1 again ...

f2 adding 2...

Wert in f3: 4

Wert zum Ende: 4

Ein Deadlock (auf Deutsch auch Verklemmung genannt) tritt auf, wenn zwei (oder mehr) Prozesse gegenseitig auf die Freigabe einer Ressource warten, die der andere Prozess freigeben muss, was er aber nicht tun kann, da er gerade auf die Freigabe einer anderen Ressource wartet.

Deadlocks können sowohl bei parallelen Prozessen als auch bei Threads auftreten. Wir zeigen im Folgenden ein Beispiel mit zwei Threads.

import time

import threading

lock1 = threading.Lock()

lock2 = threading.Lock()

def f1():

print("f1 acquiring first lock ...", flush=True)

lock1.acquire()

time.sleep(1)

print("f1 acquiring second lock ...", flush=True)

lock2.acquire()

print("f1 releasing first lock ...", flush=True)

lock1.release()

print("f1 releasing second lock ...", flush=True)

lock2.release()

def f2():

print("f2 acquiring second lock ...", flush=True)

lock2.acquire()

time.sleep(1)

print("f2 acquiring first lock ...", flush=True)

lock1.acquire()

print("f2 releasing second lock ...", flush=True)

lock2.release()

print("f2 releasing first lock ...", flush=True)

lock1.release()

# Erzeugen der Threads

thread1 = threading .Thread(target = f1)

thread2 = threading .Thread(target = f2)

# Starten der Threads

thread1.start()

thread2.start()

# Warten auf Beenden der Threads

thread1.join()

thread2.join()

# Das Ende wird nicht erreicht

print("Ende")

f1 acquiring first lock ...

f2 acquiring second lock ...

f1 acquiring second lock ...

f2 acquiring first lock ...

Aufgrund des Deadlocks muss das Code-Fragment mittels Kernel-Interrupt abgebrochen werden.


Das Aktorenmodell


Aktoren bieten einen abstrakteren Zugang zur Nebenläufigkeit, der es erlaubt, sich nicht mit Implementierungsdetails befassen zu müssen. Aktoren finden sich ursprünglich in der Programmiersprache Erlang, sind aber auch in der im Data Science-Umfeld häufig genutzten Sprache Scala umgesetzt, wo von mehreren Implementierungen akka46 die populärste ist. Eine Implementierung von Aktoren gibt es für viele Programmiersprachen, insbesondere auch für Python. Eine R-Implementierung ist den Autoren nicht bekannt.

Aktoren-Systeme funktionieren auf der Basis von versandten Nachrichten, auf die einzelne Programmbestandteile in definierter Art und Weise reagieren. Ein Aktor ist hierbei eine Einheit, die in Reaktion auf eine Nachricht, die sie erhält, Folgendes durchführen kann:

– eine endliche Anzahl von Nachrichten an andere Aktoren (auch an sich selbst) senden, hierbei kann sie nur an ihr bekannte Aktoren Nachrichten versenden (Lokalitätsprinzip);



– eine endliche Anzahl neuer Aktoren erzeugen;



– ihren Zustand für nachfolgende Nachrichten ändern (aber nicht die Zustände anderer Aktoren).





Ein Aktor kennt andere Aktoren, die ihm per Nachricht übermittelt worden sind (dies beinhaltet üblicherweise den Absender einer Nachricht) oder die er selber erzeugt hat. Verschiedene Aktoren können auf verschiedenen Systemen laufen. Aktoren sind leichtgewichtig, d.h. sie benötigen nur wenige Systemressourcen. Nachrichten werden von Aktoren typischerweise in der Reihenfolge verarbeitet, in der sie ankommen. Es ist aber nicht gewährleistet, dass Nachrichten in derselben Reihenfolge ankommen, in der sie verschickt worden sind.

Ein praktische Vorstellung für das Aktorenmodell liefert ein E-Mail-System. Hierbei sind die Mailprogramme bzw. die einzelnen Mail-Teilnehmer die Aktoren und die Mails die Nachrichten.

Das Aktorenmodell findet weniger im Bereich der Data Science Anwendung als im Bereich von Netzwerktechniken. So bedeutet Erlang Ericsson Language und wurde für den Bereich der Telekommunikation entwickelt und dort zuerst eingesetzt. Weiter bekannt Anwendungsfälle finden sich bei WhatsApp und CouchDB, die das Aktorenmodell in Erlang nutzen.47


5.8 MapReduce, Hadoop und Spark


Die Verarbeitung sehr groẞer Datenmengen erfordert einen teilweisen Paradigmenwechsel in der Programmierung.

Die imperative Programmierung ist das gängigste Paradigma. Die Grundidee ist die Verwendung von Zustandsvariablen, die initialisiert werden und deren Werte im Programmablauf, z.B. in einer Schleife verändert werden. Als Beispiel berechnen wir die Summe der Quadrate natürlicher Zahlen von 0 bis 10:

i = 0

for l in range(11):

i += l ** 2

Die Zustandsvariablen sind l und i. Ihre Werte werden durch Vorschriften geändert, die Laufvariable l wird hier durch eine implizite Vorschrift inkrementiert und zu l wird jeweils das Quadrat der Laufvariablen hinzuaddiert wird. Probleme können bei der Verarbeitung groẞer Datenmengen auftreten, nämlich wenn es erforderlich wird, die Rechnung auf viele Kernen oder Prozessoren zu verteilen, die dann gleichzeitig auf die Zustandsvariablen zugreifen müssen, vgl. Abschnitt 5.7. Der Zustand ist dann unter Umständen nicht mehr wohldefiniert und kann sich in verschiedenen Teilen des Systems unterscheiden. Imperative Programmierung ist somit nicht mehr ohne weiteres anwendbar.

Funktionale Programmierung verarbeitet die Daten durch die sukzessive Anwendung von Funktionen und Funktionalen. Eine Grundidee zur Vermeidung von expliziten Zuständen ist hierbei die Rekursion. Das einfache Beispiel von oben lässt sich auch mit Hilfe einer rekursiv definierten Funktion umsetzen:

def sum_squares(bound):

return 0 if bound == 0 else bound ** 2 + sum_squares(bound - 1)

Der Aufruf sum_squares(10) liefert dann den gesuchten Wert 385. Typische Funktionale Programmiersprachen, wie Scala, Lisp oder Haskell, stellen darüber hinaus geeignete Hilfsmittel zur Verfügung, z.B.:

– grundlegende Funktionale wie Map, Filter und Reduce,



– die Möglichkeit, eigene Funktionale zu definieren.





Das Beispiel von oben könnte dann auch folgendermaẞen aussehen, wobei sowohl Python als auch Scala/Spark Code gezeigt wird. Eine abstraktere Beschreibung und Definition der Map und Reduce Funktionale folgt im nächsten Abschnitt:

import functools

# erzeuge Range [1, 2, 3, ..., 10]

firstTenNumbers = range(1, 11)

squares = map(lambda x: x * x, firstTenNumbers)

sumSquares = functools.reduce(lambda x, y: x + y, squares)

Das Symbol lambda leitet in Python die Definition einer anonymen Funktion, d.h. einer Funktion ohne Namen, ein. Die Zuweisung an squares im gezeigten Listing führt zur Anwendung der anonymen Funktion auf alle Elemente des Range-Objekts. squares beherbergt danach also die Quadratzahlen [1, 4, . . . , 100]. In der letzten Zeile wird wieder eine anonyme Funktion erzeugt, die diesmal ihre beiden Argumente addiert. Das reduce-Funktional nimmt diese Funktion und die berechnete Sequenz der Quadratzahlen entgegen und reduziert die Sequenz in ein Skalar, indem jeweils die anonyme Funktion auf ein noch nicht behandeltes Element der Sequenz und den Ergebnisausdruck aller bereits behandelten Elemente der Sequenz angewendet wird.

Die gleiche Wirkungsweise enthält der folgende Scala/Spark-Code:

// definiere Spark-Dataset, welches auf dem Spark-Cluster verteilt ist

val firstTenNumbers = spark.range(1,11)

val squares = firstTenNumbers.map(x => x*x)

val sumSquares = squares.reduce(_+_)

Der Code ähnelt dem Python-Code sehr, lediglich die Notationen der anonymen Funktionen unterscheiden sich und die Funktionale map und reduce sind als Objektmethoden realisiert. Hier wird deutlich, dass Scala – ebenso wie Python – als Programmiersprache nicht nur dem rein funktionalen Paradigma verpflichtet ist, sondern viele Paradigmen zugleich unterstützt.

Die Idee der funktionalen Programmierung bzw. der funktionalen Programmiersprachen ist nicht neu, spätestens seit den 50er Jahren existierten beispielsweise mit Lisp relevante Implementierungen. Die Wahrnehmung auch durch die Praxis änderte sich erst ungefähr zur Jahrtausendwende deutlich. Google veröffentlichte 2004 ein bedeutsames Paper48, das den damaligen Google Cluster und seine Unterstützung für MapReduce-Operationen beschreibt. Die Ingenieure bei Google hatten die Erfahrung gemacht, dass viele Algorithmen durch die sukzessive Anwendung von Map und Reduce Funktionalen umgesetzt werden können. Programme, die diesem Ansatz folgen, werden von dem Cluster automatisch parallelisiert und auf dem Rechnerverbund aus Standardhardware verarbeitet. Der Cluster kümmert sich um alle Aspekte des verteilten Rechnens – z.B. die Verteilung der Daten auf dem Cluster, die Steuerung des Jobs und die Sicherstellung der Fehlertoleranz –während sich der Programmierer vollständig auf die Formulierung des Algorithmus’ mittels MapReduce fokussieren kann.


Apache Hadoop und Spark


Das Konzept des Google Clusters wurde etwa 2006 von Doug Cutting and Mike Cafarella aufgegriffen und als Open-Source Projekt in Java implementiert. Das Projekt wird inzwischen von der Apache Software Foundation (einer ehrenamtlichen Organisation)49 weiterentwickelt und läuft unter dem Namen Hadoop. Es ist Ausgangspunkt für viele weitere Entwicklungen im Big Data Umfeld.

Um auch gegen Stromausfälle gewappnet zu sein, werden in Hadoop Daten stets auf den Festplatten der Cluster-Nodes gespeichert und nicht im Arbeitsspeicher, vgl. hierzu auch Abschnitt 5.5 zum Hadoop Distributed File Systems (HDFS). Die Datenverarbeitung ist deshalb verhältnismäẞig langsam, was sich insbesondere bei Algorithmen bemerkbar macht, bei denen über viele Arbeitsschritte iteriert wird und somit viele Zwischenergebnisse erzeugt und persistiert werden. Letzteres ist insbesondere auch bei den meisten Machine Learning Anwendungen der Fall und daher ist Hadoop eher ungeeignet für das maschinelle Lernen, obwohl es einzelne Machine Learning Frameworks gibt, die auf Hadoop aufbauen (z.B. Apache Mahout).

Umeine schnelle Verarbeitung auch groẞer Datenmengen zu gewährleisten, hat Matei Zaharia 2009 Apache Spark ins Leben gerufen. Spark versucht, die Daten so weit wie möglich im Arbeitsspeicher der Cluster-Nodes zu halten und ist dadurch wesentlich schneller als Hadoop. Spark implementiert ebenfalls die Map und Reduce Funktionale, wobei mittlerweile viele weitere Sprachelemente hinzugekommen sind, z.B. ein SQL Interface (Spark SQL).

Spark ist keine eigene Programmiersprache sondern – wie Hadoop Map/Reduce – ein Framework zur Ausführung verteilter Berechnungen auf einem Cluster und es wird daher immer in Kombination mit einer anderen Sprache benutzt, in der die eigentlichen Berechnungen beschrieben werden. Derzeit werden Scala, Java, Python und R als Schnittstellen angeboten. Spark selbst ist gröẞtenteils in Scala programmiert, so dass Scala in gewissem Sinne die natürlichste Wahl ist. Scala ist eine relativ moderne Sprache (2003 eingeführt), die auf der Java Virtual Machine aufsetzt und wie bereits erwähnt sowohl funktionale als auch objektorientierte Programmierung ermöglicht.

Spark besteht aus verschiedenen Komponenten. Das Grundgerüst bildet Basic Spark. Der grundlegende Datentyp ist hier das Resilisient Distributed Dataset (RDD). Das ist im wesentlichen eine Liste, die auf dem Cluster verteilt ist. Seit 2012 gibt es zusätzlich Spark SQL, was die beiden Datentypen Dataframes und Datasets bereitstellt und SQL-Abfragen ermöglicht. Dataframes haben Tabellenform und sind vergleichbar mit R oder Pandas Dataframes.

Weitere Komponenten sind die Machine Learning Bibliothek Spark MLib oder die Streaming Bibliothek Spark Streaming.


MapReduce Funktionale


Aufgrund der groẞen Bedeutung der Map und Reduce Funktionale in der Verarbeitung groẞer Datenmengen und der Funktionalen Programmierung wollen wir an dieser Stelle eine formalere Definition einflieẞen lassen.

Das Map Funktional wendet eine Funktion f auf jedes Element einer Sequenz an und gibt eine neue Sequenz mit den berechneten Funktionswerten zurück:



Map eignet sich also für Transformationen auf den Input-Daten.

Mit Hilfe von Reduce können Sequenzen aggregiert werden. Ausgangspunkt ist eine assoziative Funktion mit zwei Argumenten f : 𝔻×𝔻 → 𝔻, die dann auf zwei Elemente einer Liste angewandt wird. Das Ergebnis wird dann wiederum zusammen mit einem weiteren Element der Liste in die Funktion eingesetzt, so dass sich schlussendlich ein skalarer Wert ergibt. Für den Spezialfall einer Liste mit vier Elementen lässt sich das folgendermaẞen ausdrücken:



Die Assoziativität der Funktion f wird gefordert, weil die Reihenfolge der Funktionsanwendung nicht notwendigerweise von Links nach Rechts erfolgen muss. Insbesondere auf einem Cluster, wo die Inputdaten auf vielen Rechnern verteilt sind, ist die Reihenfolge willkürlich. Dennoch sind nichtassoziative Funktionen bei vielen Programmiersprachen als Argument für das Reduce-Funktional nicht ausgeschlossen, sollten aber möglichst vermieden werden. Ein Beispiel für eine nichtassoziative Operation ist die Differenz, f(x, y) = x − y.

Filter ist ein weiteres Funktional, das häufig in Verbindung mit Map Anweisungen benutzt wird. Dabei werden bestimmte Elemente einer Liste gefiltert:



für die eine Bedingung c : 𝔻 → {False, True} erfüllt ist:



Eine gute Erklärung der Map-Reduce-Funktionale und ihrer Umsetzung in Python findet man in Youtube unter diesem Link https://www.youtube.com/watch?v=cKlnR-CB3tk.


Word Count Beispiel


Als konkrete Anwendung von Map-Reduce werden wir nun die Häufigkeiten der Wörter in folgendem Satz zählen:

text = "Actuarial data science applies data science to actuarial problems"

Um die Aufgabe etwas zu erschweren, werden wir zusätzlich nicht zwischen Groẞ- und Kleinschreibung differenzieren und nur Wörter mit mehr als zwei Zeichen betrachten.

Zunächst splitten wir den String bei jedem Leerzeichen und erhalten damit eine Liste. In Spark würden wir hier auẞerdem eine Umwandlung in ein Dataset vornehmen, so dass die Daten auf dem Cluster verteilt vorliegen und nicht lokal auf einer einzelnen Maschine).

wordList = text.split(" ")

# Out: ['Actuarial', 'data', 'science', 'applies', 'data', 'science',

# 'to', 'actuarial', 'problems']

Um kurze Wörter – mit zwei oder einem Zeichen – auszuschlieẞen, wenden wir einen Filter an:

wordListShort = list(filter(lambda s: len(s)>2, wordList))

# Out: ['Actuarial', 'data', 'science', 'applies', 'data', 'science',

# 'actuarial', 'problems']

Als Vorbereitung für die Zählung der Wörter mit Hilfe von Map-Reduce wandeln wir jedes Element der Liste in ein Tupel der Form (’Wort’, 1) um. Dafür benutzen wir das Map Funktional. Bei der Gelegenheit wechseln wir zudem zu Kleinbuchstaben.

wordTuples = list(map(lambda x: (x.lower(), 1), wordListShort))

# Out: [('actuarial', 1),

# ('data', 1),

# ('science', 1),

# ('applies', 1),

# ('data', 1),

# ('science', 1),

# ('actuarial', 1),

# ('problems', 1)]

Im letzten Schritt werden die Wörter gezählt. Dafür benötigen wir eine Verallgemeinerung von Reduce, die auch reduceByKey genannt wird: die Aggregation soll pro Wort (=Key) erfolgen. Das Reduce Funktional nach der obigen Definition liefert aber ein Skalar, es wird also über alle Elemente der Liste aggregiert. In Python müssen wir die reduceByKey Funktion selbst definieren (während z.B. Spark die Funktion bereitstellt):

from functools import reduce

from itertools import groupby

# leider gibt es in Python keine reduceByKey-Funktion

def reduceByKey(func, iterable):

get_first = lambda p: p[0]

get_second = lambda p: p[1]

return map(

lambda l: (l[0], reduce(func, map(get_second, l[1]))),

groupby(sorted(iterable, key=get_first), get_first)

)

wordCount = list(reduceByKey(lambda x, y: x + y, wordTuples))

"""

Out: [('actuarial', 2),

('applies', 1),

('data', 2),

('problems', 1),

('science', 2)]

"""

In der Implementierung der reduceByKey-Funktion wird zunächst (zweites Argument des map-Aufrufs) die Sequenz nach den Wörtern sortiert und dann mit der groupby-Funktion zu einer neuen Sequenz zusammengefasst, die aus 2-Tupeln besteht: Es sind jeweils Kombinationen aus einem Wort und einer Liste aller mit diesem Wort in der ursprünglichen Liste gepaarten Zahlen. Im Beispiel sieht dieses Zwischenergebnis so aus:

[

('actuarial', (1, 1)),

('applies', (1)),

('data', (1, 1)),

('problems', (1))

('science', (1, 1)),

]

Anschlieẞend erfolgt ein map-Aufruf auf diesem Zwischenergebnis, wobei auf jedem Eintrag eine reduce-Operation auf der inneren Liste aufgerufen wird, die als Ergebnis die Summe der Zahlen der inneren Liste liefert, mithin die Anzahl der Vorkommen des jeweiligen Wortes.


Algorithmischer Ablauf eines MapReduce Jobs


Im Folgenden diskutieren wir den algorithmischen Grundablauf eines Map Reduce Jobs auf einem Hadoop bzw. Spark Cluster. Im Detail unterscheiden sich die Abläufe in Hadoop und Spark zwar deutlich, der vereinfachte Grundablauf, wie er hier beschrieben wird, trifft aber auf beide Systeme gleichermaẞen zu.

Zunächst werden die Daten in M Teile aufgesplittet. Jeder dieser Teile wird separat durch die Anwendung der Map Funktion verarbeitet. Es laufen also M parallele Map Prozesse (Mappers) ab. Die Zwischenergebnisse werden dann durch R parallel laufende reduceByKey Prozesse (Reducers) weiterverarbeitet. Zwischen der Map- und Reduce-Verarbeitung müssen die Daten im Cluster umverteilt werden (Shuffle Phase). Dafür werden die Keys, die bei der Map Phase entstehen in R Teilmengen unterteilt und jede Teilmenge wird genau einem Reducer zugeordnet. In der Regel ist M und R deutlich gröẞer gewählt, als der Anzahl der Rechner im Cluster, so dass mehrere Mapper oder Reducer auf einem Rechner laufen. Dies hat den Vorteil, dass es zu einer gleichmäẞigen Auslastung auf dem Cluster kommt. In Abbildung 5.28 ist eine graphische Veranschaulichung des Grundablaufs für das Word Count Beispiel dargestellt.


Abb. 5.28: Ablauf des Word-Count-Beispiels mit drei Mappern und zwei Reducern.

Wir wiederholen nun nochmal die einzelnen Phasen des Ablaufs und gehen dabei auf weitere Details ein:

Split: Die Input-Daten werden in M Teile zerlegt. Häufig sind die Input-Daten bereits auf dem Cluster im HDFS gespeichert und sind daher bereits partitioniert. Der Cluster versucht dann diese Partitionierung für die Map-Phase beizubehalten, so dass möglichst nur wenige Daten auf dem Cluster verschoben werden müssen. Mehr dazu unten im Abschnitt zur Lokalität.



Map und Filter: Jeder Teil wird separat durch die Map und Filter Funktionale abgebildet. In Hadoop wird das Zwischenergebnis dann lokal auf den Festplatten der entsprechenden Cluster-Nodes abgespeichert und in R Teilmengen partitioniert. In Spark wird das Zwischenergebnis im Arbeitsspeicher gehalten, falls genügend RAM verfügbar ist.



Shuffle und Sort: Die Zwischenergebnisse der Map-Tasks müssen nun auf die R Prozesse verteilt werden (Shuffle). Dazu werden die Keys des Map Outputs in R Teile aufgeteilt. In dem obigen Word Count Beispiel kommt es zu den zwei Teilmengen {actuarial, applies, data} und {science, problems}. Es werden dann z.B. alle Datensätze mit dem Key actuarial auf den Rechner mit der ersten Reducer-Task umverteilt. Auẞerdem werden die Datensätze alphabetisch nach dem Key sortiert (das wird für die Reduce Phase benötigt).



Reduce: Es kommt nun auf jedem Reducer zur Anwendung des reduceByKey Funktionals (im Allgemeinen Sprachgebrauch wird dabei nicht zwischen reduce und reduceByKey unterschieden). Das Endergebnis wird üblicherweise im HDFS abgespeichert und ist dann in R Teile partitioniert.





Job-Steuerung in Hadoop


Der obige algorithmische Grundablauf muss von dem Cluster organisiert werden. Für diese Steuerung sind auf dem Cluster verschiedene Softwarekomponenten zuständig, z.B. der Job und der Task Tracker. Wir werden nun die Job Steuerung genauer beleuchten.

Aus didaktischen Gründen diskutieren wir erst die vereinfachte Jobsteuerung aus Hadoop Version 1.x und gehen dann in einem zweiten Schritt auf die Erweiterungen in Hadoop 2.x ein.


MapReduce 1


Die Job Steuerung in MapReduce 1, d.h. Hadoop Version 1.x, ist in der Graphik in Abbildung 5.29 illustriert.

Der Cluster besteht aus verschiedenen Rechnern (Nodes). Dabei gibt es einen Master Node, der die Organisation auf dem Cluster übernimmt, und viele Slave Nodes, die die Arbeit ausführen.

Für das weitere Verständnis ist es wichtig zu erwähnen, dass auf dem Cluster sowohl die Datenspeicherung erfolgt (in Form von HDFS), als auch die Verarbeitung der Daten in Form von Map Reduce Jobs. Man findet daher auf den Nodes sowohl Komponenten die zu HDFS gehören (in der Graphik jeweils links in dem Node gezeigt), als auch Komponenten, die sich um MapReduce kümmern (jeweils rechts gezeigt).


Abb. 5.29: Job-Verarbeitung in Map-Reduce 1.

Auf dem Master Node befinden sich zwei wesentliche Software Komponenten: der Name Node und der Job Tracker. Wie oben erwähnt verwaltet der Name Node die Meta Daten des HDFS Filesystems. Der Job Tracker wiederum orchestriert den MapReduce Ablauf auf dem Cluster

Auf den Slave Nodes sind die Daten in Form von HDFS Blöcken gespeichert. Auẞerdem finden hier die Map und Reduce Rechnungen statt, was auf dem jeweiligen Slave Node durch den Task Tracker verwaltet und überwacht wird.


Der Job Tracker


Seine Aufgaben sind im Detail:

– empfängt vom Client Anfragen zur Ausführung von MapReduce Programmen,



– kommuniziert mit dem Name Node um den Speicherort der Daten zu bestimmen,



– findet für jede Aufgabe den optimalen Slave Node, basierend auf der Lage der Daten und der Verfügbarkeit von Rechenkapazitäten. Das heiẞt, falls freie Rechenslots verfügbar sind, werden die Mapper direkt bei dem Slave Node mit den jeweiligen HDFS Blöcken platziert und falls nicht, dann wird ein Rechner mit freien Slots auf dem gleichen Cluster Segment gewählt,



– überwacht die einzelnen Task Tracker und übermittelt den Gesamtstatus zurück an den Client.





Die Task Tracker


Sie übernehmen Folgendes:

– empfangen Map und Reduce Aufgaben vom Job Tracker,



– senden regelmäẞig Statusinformationen über die laufenden Tasks an den Job Tracker.





MapReduce 2


Der Job Tracker hat in MapReduce 1 zu viele Aufgaben (Job Scheduling, Resource Management und Task Monitoring). MapReduce 1 ist daher nicht beliebig skalierbar. Yahoo hat herausgefunden, dass MapReduce nur mit maximal 5 000 Nodes und 40 000 Tasks betrieben werden kann. Auẞerdem kann die obige Architektur nur mit Map-Reduce Aufgaben umgehen. Moderne Big Data Anwendungen benötigen ein breiteres Spektrum an Verfahren, dass über MapReduce hinausgeht, z.B. interaktive SQL Anfragen und die Verarbeitung von Echtzeitdatenströmen. Um diese Probleme zu beheben, wurde in Hadoop 2.x die Architektur angepasst:

– Der Job Tracker wurde durch zwei neue Komponenten ersetzt: Resource Manager (YARN) und Application Master. Die Aufgaben sind daher besser verteilt und die Skalierbarkeit wird verbessert,



– Statt dem Task Tracker gibt es nun den Node Manager,



– Hadoop 1 ist auf Mapper und Reducer Tasks beschränkt. In Hadoop 2 werden Prozesse von Containern ausgeführt, die beliebe Aufgaben ausführen können.





Es folgen noch einige weitere Details zur Arbeitsweise der neuen Komponenten.

Der Resource Manager läuft auf dem Master Node und hat den Überblick über alle verfügbaren Ressourcen auf dem Cluster. Auf dem Resource Manger laufen verschiedene Services, wobei der wichtigste davon der Resource Scheduler ist. Er empfängt Aufgaben von verschiedenen Clients und entscheidet, welche Anwendung als nächstes Ressourcen gestellt bekommt. Ein weiterer wichtiger Service ist der Application Manager, der Anwendungen vom Scheduler zugewiesen bekommt. Nach einer Zuweisung spricht er mit den Name Nodes um einen freien Container auf einem Slave Node zu finden. Nach erfolgreicher Suche startet der Application Manager den Application Master in dem freien Container.

Der Application Master läuft auf einem Slave Node und ist für den kompletten Lebenszyklus einer Anwendung zuständig. Er erfragt vom Resource Manager freie Container und führt Programme auf den Containern aus. Der Application Master kennt die Anwendungslogik und ist daher Framework-spezifisch. Zum Beispiel hat MapReduce eine eigene Implementierung für den Application Master. Nach erfolgreicher Beendigung der Anwendung wird der Application Master heruntergefahren und alle benötigten Container werden für neue Anwendungen freigegeben.

Node Manager:Auf jedem Slave Node läuft ein Node Manager, der den Überblick über die verfügbaren Ressourcen auf dem jeweiligen Slave Node hat. Er hat den Überblick über die Container, welche auf dem Slave Node laufen, und überwacht deren Auslastung. Konzeptionell ist der Node Manager vergleichbar mit dem Task Tracker aus MapReduce 1. Der Task Tracker hatte jedoch eine feste Zahl an Mappern und Reducern zur Verfügung, während der Node Manager dynamisch erzeugte Container (beliebiger Gröẞe) überwacht. Diese können für beliebige Aufgaben und Frameworks genutzt werden (nicht nur für Map und Reduce).


Skalierbarkeit


Um sehr groẞe Datenmengen verarbeiten zu können, muss ein Cluster Framework auch mit sehr vielen Slave Nodes fehlerfreie Ergebnisse in endlicher Zeit liefern. Diese Skalierbarkeit wird in Hadoop durch verschiedene Mechanismen erreicht, auf die wir jetzt näher eingehen.


Ausfallsicherheit


Bei Clustern mit vielen Nodes sind Hardwareausfälle die Norm und nicht die Ausnahme. Ein Big Data Cluster-Framework sollte in der Lage sein, damit umzugehen und dennoch richtige Rechenergebnisse zu liefern. Hadoop hat sowohl für die Datenspeicherung (HDFS), als auch für die Datenverarbeitung (MapReduce) Mechanismen um Ausfallsicherheit (fault tolerance) zu erreichen.

Fault tolerance in HDFS: Die Datenblöcke werden jeweils dreifach repliziert, d.h. von jedem HDFS Block werden zwei identische Kopien angelegt. Diese werden auf anderen Data Nodes abgelegt. Der Name Node behält den Überblick, wo im Cluster die Replikate gespeichert sind. Die Data Nodes senden regelmäẞig Lebenszeichen an den Name Node. Falls der Name Node von einem Data Node kein Signal mehr empfängt, werden die entsprechenden Blöcken auf einem weiteren Slave Node repliziert.

Fault tolerance in MapReduce: Die Task Tracker senden periodisch Signale an den Job Tracker. Falls keine Signale mehr ankommen, werden sämtliche Map und Reduce Tasks (egal ob noch laufend, bereits abgeschlossen oder noch nicht begonnen) des betroffenen Task Trackers an einen anderen Task Tracker vergeben. Eine Ausnahme sind bereits abgeschlossene Reduce Tasks. Deren Output ist im globalen HDFS Filesystem gespeichert und liegt somit in dreifacher Kopie vor. Diese Tasks müssen daher nicht erneut ausgeführt werden.

In Spark ist Ausfallsicherheit noch kritischer, weil hier die Daten typischerweise im Arbeitsspeicher vorliegen und damit bei Stromausfällen verloren gehen könnten. In Spark gibt es daher eine weitere Sicherheitsmaẞnahme: Lineage. Spark merkt sich jeden Rechenpfad, so dass bei Ausfällen alle Resultate und Zwischenergebnisse aus den Ursprungsdaten regeneriert werden können.


Lokalität


Datentransport innerhalb eines Computernetzwerks ist zeitaufwendig. Hadoop versucht daher möglichst, die Daten dort zu verarbeiten, wo sie anfangs gespeichert sind. Der Job Tracker kommuniziert mit dem Name Node undweiẞ daher auf welchen Slave Nodes die HDFS Blöcke und ihre Replikate liegen. Falls Ressourcen auf dem entsprechenden Slave Nodes verfügbar sind, werden die Map Tasks auf den gleichen Slave Nodes ausgeführt, auf denen auch die entsprechende HDFS Blöcke gespeichert sind. Falls kein freier Slot verfügbar ist, wird ein Slave Node ausgewählt, der über das selbe Network Switch verbunden ist.


Effektive Job-Steuerung


Skalierbarkeit kann nur bei einer effektiven und gleichmäẞigen Verteilung der Aufgaben innerhalb des Clusters erreicht werden. Aus diesem Grund wurde die Job Steuerung mit der Einführung von Map Reduce 2 noch weiter optimiert. Damit sind auch Cluster jenseits von 5 000 Nodes möglich.


Wichtige Tools im Hadoop-Umfeld


Seit der Einführung von Hadoop im Jahr 2006 sind viele weitere Open Source Big Data Tools entstanden, die oft in Verbindung mit Hadoop oder Spark genutzt werden. Typischerweise werden diese Tools ebenfalls von der Apache Foundation entwickelt. Wir geben hier einen kurzen Ausblick.

Hive: Ursprünglich von Facebook entwickelt und auf Apache Hadoop aufbauend, bietet Hive typische Datenbank-Funktionalitäten. So können Schemata angelegt werden und Tabellen geschrieben bzw. ausgelesen werden. Hive bietet dafür eine SQL Abfragesprache genannt Hive Query Language (HQL), welche weitgehend die SQL-92 Standards erfüllt.

Pig: Apache Pig ist eine High-Level Plattform, welche eine vereinfachte Erstellung von Map Reduce Programm enermöglicht. Dafür wird eine eigene Programmiersprache bereitgestellt (Pig Latin). Auẞerdem kann der Nutzer eigene Funktionen in Sprachen wie Java und Python erstellen.

Hue: Hue ist ein Web Interface, das Hadoop Anwendungen und andere Systeme aus dem Big Data Umfeld unterstützt. Es stellt z.B. einen Editor für Hive, Pig, Spark und andere Sprachen zur Verfügung, sowie einen Browser für HDFS Files und Jobs die auf dem entsprechenden Cluster laufen.

Ambari View: Ähnlicher Anwendungsbereich wie Hue. Ambari View wurde von HortonWorks entwickelt, während Hue von Cloudera kommt.

Kafka: Verteilte Plattform für die Verarbeitung von Echtzeitdaten. Nachrichten können von verschiedenen Quellen empfangen, weiterverarbeitet, gespeichert und an unterschiedliche Datensenken gesendet werden.

Storm: Ähnlicher Scope wie Kafka, aber mehr Fokus auf Echtzeit Analytics / Datenverarbeitung. Storm wird oft in Kombination mit Kafka benutzt; die Datenströme werden zunächst von Kafka empfangen und dann von Storm weiterverarbeitet. Eine Storm Anwendung hat die Form eines gerichteten azyklischen Graphen. Es gibt eine ganze Reihe an weiteren Streaming Engines, wie z.B. Spark Streaming.


5.9 Cloud Computing


Unzweifelhaft ist die Wichtigkeit der Clouds in den letzten Jahren gestiegen. Nach Gartner50 wird sich diese ökonomische Erfolgsgeschichte auch weiterhin in zweistelligen Umsatzwachstumszahlen vor allem bei den groẞen Anbietern Amazon, Microsoft und Google widerspiegeln. Auch an den Versicherungsunternehmen im Allgemeinen und den Aktuaren geht dieser Trend nicht vorbei, weshalb wir in den folgenden Abschnitten auf die grundlegenden Konzepte und Begriffe und auf spezielle Data Science Angebote eingehen. Anschlieẞend widmen wir uns einem Beispiel, in dem wir andeuten, wie eine kleine Data Science Anwendung zu einem Cloud-Angebot werden kann.


5.9.1 Begriffe und Konzepte


Für welchen Verantwortlichen in einem Unternehmen klingen die Versprechen des Cloud Computings nicht verlockend? Hard- und Softwareressourcen beliebig verfügbar, Skalierbarkeit nach Bedarf und ohne Wartezeiten und dazu noch eine sekundengenaue Abrechnung nach der Inanspruchnahme. Auch wenn dieses Feld nicht zum primären Fachgebiet der Autoren gehört, fühlen wir uns mit dem Hinweis, diese Versprechen jeweils einer sehr genauen Prüfung zu unterziehen, auf sehr sicherer Seite.

Die grundlegende Idee von Clouds ist es, zentrale Ressourcen wie Hard- oder Software an Kunden zu vermieten. Durch eine damit verbundene Spezialisierung der Anbieter auf bestimmte Dienstleistungen, günstigeren Einkauf der Ressourcen (einschlieẞlich Strom), eine bessere Ausnutzung der Ressourcen und auch eine gewisse Standardisierung lassen sich ökonomische Skalierungseffekte und damit eine echte Effizienzsteigerung erzielen. Auẞerdem ist wegen der, im Vergleich zum on premise Betrieb, niedrigen Initialkosten auch mit der Gewinnung von Kunden zu rechnen, die die anderweitig notwendigen Investitionen gescheut hätten.

Eine entscheidende Variable im Zusammenhang mit der Bereitstellung von Ressourcen ist die Virtualisierung. Hierunter ist eine Zwischenschicht zwischen den beim Cloudanbieter vorhandenen und den an den Kunden weitergegebenen Ressourcen zu verstehen, letztere werden auch logische Ressourcen genannt. Das Ziel der Virtualisierung ist also die bedarfs- bzw. nachfragekonforme Zuteilung der logischen Resourcen. Die verwendeten Technologien müssen in der Lage sein, die logischen Resourcen effizient auf die vorhandenen physikalischen abzubilden und erstere dort zu betreiben. Das wichtigste Beispiel hierfür dürften die virtualisierten Rechner sein, die als Software auf den physischen Rechnern des Anbieters laufen. Auch beispielsweise Speicher- oder Netzwerkkomponenten werden virtualisiert zur Verfügung gestellt.


Arten der Clouds und Clouddiensten


Da gibt es zum einen die sogenannten Public Clouds wie z.B. Amazon AWS, Microsoft Azure oder Google Cloud Platform, die zentral von den genannten Unternehmen betrieben werden und allen Nutzern offen stehen. Dabei gehören sämtliche Hard- und Softwareressourcen dem Betreiber und werden von diesem verwaltet, der Zugriff erfolgt über das Internet.

Im Gegensatz dazu steht eine Private Cloud exklusiv einem Unternehmen zur Verfügung und wird auch von diesem verwaltet. Der Zugriff erfolgt normalerweise über das eigene Netzwerk (Intranet).

Kombinationen aus Public und Private Cloud sind unter dem Namen Hybrid Clouds bekannt, hier können Daten zwischen den Private und den Public Teilen verschoben werden.

Oft werden folgende, jeweils aufeinander aufbauende Angebotstypen unterschieden:

– Infrastructure as a Service (IaaS): Hier werden nur grundlegende Ressourcen wie Server, Speicher und ein Netzwerk bereitgestellt.



– Platform as a Service (PaaS): In dieser Ausbaustufe steht darüber hinaus eine für Anwendungen vorbereitete Umgebung bereit, das schlieẞt üblicherweise zumindest ein Betriebssystem aber oft auch Administrations- und Monotoringtools, Agenten für das automatische Ausrollen und spezielle Laufzeitumgebungen (Docker, Kubernetes, etc.) ein.



– Software as Service (SaaS): Hier wird dem Nutzer eine ganze Anwendung bereitgestellt, die üblicherweise über den Webbrowser erreichbar ist. Beispiele sind Office-Anwenungen oder auch ein Spark-Cluster mit konfigurierten Frontends.





5.9.2 Data Sciene Angebote in der Cloud


Es gibt auch spezialisierte Data-Science- und KI-Angebote in der Cloud. Das schlieẞt zum einen intelligente Dienste wie Textübersetzungen ein, die vor allem aus der Sicht der Anwendungsentwicklung interessant sind, wenn solche Services in eigenen Produkten verwendet werden sollen. Zum anderen werden aber auch fertige Plattformen angeboten, mit denen der Nutzer in der Cloud Algorithmen des maschinellen Lernens erstellen und auf leistungsfähiger Hardware (etwa mit GPU-Unterstützung, vgl. Abschnitt 5.7) trainieren kann.

Die Angebote und auch deren Namen ändern sich häufiger, als Beispiel soll daher ein kurzer Blick auf den Azure Machine Learning Service51 genügen. Einen Azure-Account vorausgesetzt ist ein Deployment inwenigen Minuten abgeschlossen, das Angebot bietet viele Möglichkeiten und ähnelt einem virtuellen Laptop. Es enthält unter anderem Entwicklungsumgebungen für Python und R, vgl. Abbildung 5.30.


Abb. 5.30: Jupyter Notebook im Azure Machine Learning Service.


5.9.3 Deployment von Machine Learning Algorithmen


In diesem Abschnitt wollen wir einen möglichen Weg zu einem produktiven Einsatz einer Machine-Learning-Anwendung skizzieren. Dabei durchlaufen wir mehrere Schritte:

Trainieren des Modells



Export des Modells



Einbinden in ein Skript



Einbinden in eine Backend Server-Applikation



Erstellen eines Web-Frontends



Erstellen eines Container-Images mit Applikation und Laufzeitumgebung



Hochladen des Containers in die Microsoft Cloud



Start der Anwendung in der Microsoft Cloud





Training und Export des Modells


In diesem Fall wird das ML-Modell zunächst in einem Python3-Jupyter-Notebook erstellt. Als Beispiel dient uns in diesem Abschnitt das Boston-Dataset, welches in verschiedenen Packages enthalten ist. Wir fitten in diesem Fall ein XGBoost Modell, wobei der spezielle Datensatz und das gewählte Modell in diesem Abschnitt nebensächlich sind und nur zur Illustration des prinzipiellen Vorgehens dienen sollen.

Wir beginnen mit einigen Import-Statements und laden dann den Datensatz:

import numpy as np

import pandas as pd

import xgboost as xgb

from sklearn.metrics import mean_squared_error

from sklearn.model_selection import train_test_split

from sklearn.datasets import load_boston

boston = load_boston()

data = pd.DataFrame(boston.data)

data.columns = boston.feature_names

# Hinzufügen der Zielvariable zum Datensatz

data['PRICE'] = boston.target

Der Datensatz liegt jetzt als Pandas.DataFrame in der Variablen data vor und kann interaktiv weiter untersucht werden.

Zur Vorbereitung der Modellanpassung zerlegen wir unseren Datensatz geeignet und splitten in einen Test- und eine Trainingsdatensatz:

# Separation von abhängigen und unabhängigen Variablen

X, y = data.iloc[:,:-1], data.iloc[:,-1]

# Zerlegung in Trainings und Testdatensatz

X_train, X_test, y_train, y_test = train_test_split(X, y,

test_size=0.2,

random_state=123)

Nun erfolgt das Training mit xgboost. Hierfür müssen eine ganze Reihe an Parametern gewählt werden. Das eigentliche Fitten passiert dann im Aufruf von fit:

xg_reg = xgb .XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3,

learning_rate = 0.1, max_depth = 5, alpha = 10,

n_estimators = 10)

xg_reg.fit(X_train, y_train)

Nachdem das Modell nun trainiert (und in der Variablen xg_reg gespeichert ist), kann man damit Vorhersagen treffen, z.B. mit xg_reg.predict(X_test). Will man das trainierte Modell an anderer Stelle weiterverwenden, so muss man es speichern, wofür der Aufruf

xg_reg.save_model("xgb_fit.dat")

genutzt werden kann, der das trainierte Modell in der Datei xgb_fit.dat speichert. An anderer Stelle kann man es dann wieder laden:

import xgboost as xgb

xg_reg = xgb .XGBRegressor()

xg_reg.load_model("xgb_fit.dat")


Modularisierung


In diesem Abschnitt verfolgen wir das Ziel, die Modellvorhersagen als Funktion zur Verfügung zu stellen. Hierzu schreiben wir eine Funktion get_predictor, die ihrerseits das trainierte Modell aus der Datei lädt und als Rückgabewert eine Funktion liefert, die Modellvorhersagen liefern kann.

import xgboost as xgb

def get_predictor(f_name="xgb_fit.dat"):

""" Gibt Funktion zurück, die Vorhersagen basierend

auf dem (über den Pfad `f_name`) spezifizierten

Modell macht."""

# Modell laden

xg_reg = xgb .XGBRegressor()

xg_reg.load_model(f_name)

# Spalten-Reihenfolge wie im Training des Modells verwendet

COLUMNS_ORDERED = ['CRIM', 'ZN', 'INDUS', 'CHAS',

'NOX', 'RM', 'AGE', 'DIS', 'RAD',

'TAX', 'PTRATIO', 'B', 'LSTAT']

def predict(df_x_pred):

""" Generiere Vorhersage (numpy-Vektor) auf dem Input `df_x_pred`. """

# Spalten in erwartete Reihenfolge bringen

df_x_ordered = df_x_pred[COLUMNS_ORDERED]

# hier könnte man weitere Validierungen durchführen

return xg_reg.predict(df_x_ordered.values)

return predict

Ruft man nun die Funktion get_predictor auf, etwa durch

predictor_fun = predict.get_predictor("<Pfad zu xgb_fit.dat>"),

so erhält man eine Funktion predictor_fun, die einen DataFrame entsprechend der Struktur des Boston-Datensatzes als Argument erwartet und einen Numpy-Vektor mit den Vorhersagen zurückliefert. Ein typischer Aufruf wäre nun also

predictor_fun(df)

mit einem DataFrame df.


Integration in eine Server-Applikation


Das ML-Modell aus den vorangegangenen Abschnitten soll nun als HTTP-Service zur Verfügung gestellt werden, damit es beispielsweise aus anderen Anwendungen heraus genutzt werden kann. Eine Möglichkeit, dies in Python umzusetzen, bietet der Tornado-Server52, der ohne weitere externe Komponenten (wie einen Apache-Web-Server) auskommt. Alternativ wäre auch der Einsatz einen Web-Frameworks wie Flask/Django denkbar, wofür aber ein WSGI-Web-Server (z.B. Apache) benötigt würde. Allen diesen Webframeworks ist gemein, dass letztlich eine HTTP-Antwort auf eine eingegangene HTTP-Anfrage generiert werden muss. Anfragen (auch als requests bezeichnet) gehen dabei immer an eine bestimmte URL, sind von einem bestimmten Typ (meist POST oder GET) und sie bringen bestimmte Parameter oder Daten mit, die den Aufruf steuern bzw. als Eingaben verwendet werden. Ein Webframework (wie Tornado) nimmt dem Entwickler nun die meisten der Aufgaben ab, die mit der technischen Ebene der Kommunikation, also der Entgegennahme und Beantwortung der Requests zu tun haben.

Um den Tornado-Server benutzen zu können, muss zunächst ein app-Objekt erzeugt werden. Bei der Anlage dieses app-Objektes wird beispielsweise konfiguriert, dass Anfragen an den URL-Pfad / predict zur Bearbeitung an den Request-Handler handlers.PredictHandler weitergeleitet werden sollen:

app = tornado.web.Application([

(r"/", tornado.web.RedirectHandler, {"url": "/app/9783110659344-013"}),

(r"/predict", handlers.PredictHandler)],

static_path=os.path.join(dir_path, "web"),

static_url_prefix="/app/")

# füge predict Funktion dem app-Objekt hinzu

app.predict_fun = predict.get_predictor()

Der soeben beim Framework registrierte Request-Handler reagiert in diesem Fall auf Anfragen vom Typ POST und hat die Aufgabe, die Daten für die Antwort an den Aufrufer bereitzustellen. Er könnte nun etwa folgendermaẞen aufgebaut sein:

class PredictHandler(tornado.web.RequestHandler):

def post(self):

# json Information aus dem Request-Body der Anfrage extrahieren

json = tornado.escape.to_unicode(self.request.body)

self.write(self.generate_predict_reply(json))

def generate_predict_reply(self, json):

""" ML-Modell ausführen und Antwort nach JSON konvertieren. """

df_x_pred = pd.read_json(json)

prediction = self.application.predict_fun(df_x_pred)

# Ergebnis als json-String zurückliefern

df_out = pd.DataFrame({"y": prediction})

df_out.index = df_x_pred.index

return df_out.to_json(orient='columns')

In der post-Methode, die das Framework bei eingehenden Anfragen an die URL / predict aufgrund der Konfiguration oben ansteuert, werden zunächst die Anfrage-Parameter in der Variablen json zwischengespeichert und dann an generate_predict_reply weitergeleitet. Deren Rückgabewert wird dann mittels self.write an den Aufrufer zurückgeleitet.

Die Methode generate_predict_reply erzeugt zunächst einen DataFrame aus dem JSON-String aus der Anfrage. Der DataFrame wird dann an die Funktion predict_fun übergeben, deren Rückgabewert (über den Umweg der Konvertierung in einen Data-Frame) schlieẞlich nach JSON konvertiert wird.


Web-Frontend


Der HTTP-Service aus dem vorangegangen Abschnitt ist schon lauffähig und kann beispielsweise über das Kommandozeilenwerkzeug curl oder von anderen Applikationen konsumiert werden. Will man einen benutzerfreundlicheren Zugriff ermöglichen, so bietet sich ein Interface für den Web-Browser an. Dies ist ein weites, jenseits der Kernkompetenz von Aktuaren liegendes Feld, mit einem unübersichtlichen Pool an Tools und Frameworks. Es kann hier deshalb nur kurz angerissen werden, was im Prinzip zu tun ist.

Zunächst muss eine HTML-Seite gestaltet werden, in der die Anfrageparameter (’CRIM’, ’ZN’, ’INDUS’, ’CHAS’, ...) des Modells eingegeben werden können, typischerweise wird dies über ein HTML-<form>-Tag realisiert werden. Beim Klicken auf den submit-Button des Formulars, werden die Formulardaten an den Server gesendet und bei Erhalt der Antwort wird diese auf der Seite angezeigt (vgl. Abbildung 5.31 für eine illustrative Implementierung).


Abb. 5.31: Beispiel-Frontend: Lokal laufender Server wird im Webbrowser angefragt, nach dem Submit-Klicken übermittelt der Browser die Daten an das Server-Backend und stellt anschlieẞend die erhaltene Antwort (rot umrandet) dar.


Containerisierung


Die Applikation hat inzwischen eine ganze Reihe von Abhängigkeiten, z.B. Python3.x, das Tornado-Framework, XGBoost, numpy etc. Will man die Applikation starten, müssen zunächst alle diese Abhängigkeiten auf dem Zielsystem in kompatiblen Versionen vorhanden sein. Eine Möglichkeit, das Ausrollen von Anwendungen mit Abhängigkeiten zu vereinfachen, sind sogenannte Container-Images, deren bekannteste Vertreter die Docker-Images sind.

Die Idee ist es hierbei, alle Abhängigkeiten einer Applikation (angefangen beim Betriebssystem) zusammen mit der Applikation in ein Container-Image zu verpacken und dieses dann von einer Container-Runtime ausführen zu lassen.

Um einen solches Image zu erstellen, benötigt man(neben der Container-Software, hier Docker) ein Build-File für das Image (meist Dockerfile genannt). Wir zeigen zunächst, wie die Datei für das Beispiel von oben konkret aussehen könnte, anschlieẞend folgen noch einige Erklärungen.

FROM ubuntu:18.04

# Ein Verzeichnis für die App im Container erzeugen

RUN rm /bin/sh && ln -s /bin/bash /bin/sh && mkdir /usr/local/app

# Update und installieren der Packages

RUN apt update && apt upgrade -y &&\

apt install -y --no-install-recommends libgomp1 python3 python3-venv

# App-Dateien vom Host in das Image kopieren

COPY server_app/* /usr/local/app/

COPY containerization/build_env.sh /usr/local/app/

COPY containerization/run_app.sh /usr/local/app/

WORKDIR /usr/local/app

# Erstellen der Python-Umgebung durch das Skript build_env

RUN source ./build_env.sh

ENTRYPOINT ["/bin/bash", "run_app.sh"]

Das Beispiel setzt hier auf Ubuntu-Linux auf, aktualisiert das System, installiert u.a. Python und libgomp (für XGBoost erforderlich), kopiert die benötigten Dateien der Applikation (Python-Code und Frontend sowie Startskripte) in das Image. Die Installation der Python-Libraries ist in das Skript build_env.sh ausgelagert, das eine virtuelle Python-Umgebung erstellt und die Installation der notwendigen Packages (die in der Datei requirements.txt aufgelistet sind) durch folgende Kommandos auslöst:

# virtuelle Python Umgebung erstellen und aktivieren

python3 -m venv /usr/local/myenv

source /usr/local/myenv/bin/activate

# Abhängigkeiten des Projektes installieren

python -m pip install --upgrade pip

pip install -r requirements.txt

Das zweite Skript run_app.sh wird dann beim Start des Images ausgeführt. Es aktiviert die Umgebung und startet den Server.

Nun kann man das Image mit dem folgenden Befehl erstellen:

docker build -f .\containerization\Dockerfile . -t boston_server

Dem docker build-Kommando wird dabei der Pfad zum Dockerfile übergeben. Weiter erhält das fertige Image den Namen (tag) boston_server. Lokal starten kann man es anschlieẞend beispielsweise mit

docker run -it -p 8000:8000 boston_server.

Im vorstehenden Kommando wurde der Port 8000 des Hosts auf den entsprechenden Port des Containers gemappt, so dass die Applikation nun auf dem Host erreichbar ist (z.B. lokal über den Webbrowser).


Cloud-Deployment


Meist bestehen Applikationen nicht nur aus einem einzelnen sondern aus vielen Containern, wodurch die Komplexität deutlich steigt. Zur Koordinierung sind sog. Container-Orchestrierungstools im Einsatz (z.B. kubernetes), die eine ganze Reihe zusätzlicher Hilfestellungen für den Betrieb offerieren. Dazu bieten die groẞen Public-Cloud-Anbieter (und vermutlich auch viele kleinere) Support, eine weitere Diskussion von Container-Orchestrierung würde aber schnell den Rahmen dieses Buches sprengen, zumal sie für das kleine Beispiel dieses Abschnittes auch nicht erforderlich ist. Um die lokal erzeugte Applikation online verfügbar zu machen, kann man zum Beispiel Microsofts Azure-Plattform nutzen und das Image dort zur Ausführung bringen.

Der Vorgang erfordert – neben der Registrierung – mehrere Teilschritte, die hier nur überblicksartig genannt werden, für weitere Details muss auf die Online-Dokumentation von Azure verwiesen werden:

– eine Container-Registry erstellen53: Dies ist der Ort in Azure, an dem das erstellte Image gespeichert wird.



– Hochladen des Images in die Registry. Dies kann manuell vom Entwicklungsrechner geschehen, nachdem das Image dort erstellt wurde, oder es wird eine sog. Build-Pipeline eingerichtet. Letzteres ist ein Cloud-gestützter Prozess, der durch bestimmt Ereignisse ausgelöst werden kann (z.B. nach dem Commit von Code-Änderungen in einem Code-Repository) und der dann das Image direkt in der Cloud neu erstellt und in die Registry hochlädt.



– Um einen Container aus einer Registry zu starten, bieten die verschiedenen kommerziellen Clouds unterschiedliche Möglichkeiten. In Azure beispielsweise kann eine Cloud-Resource vom Typ Container-Instanz in Azure erzeugt und mit dem Image verknüpft werden54.



– Die Container-Instanz kann man ausführen, der Container ist dann sofort online und kann über die IP-Adresse angesprochen werden.



– Nach der Verwendung sollte man ihn wieder stoppen und ggf. die erstellten Ressourcen löschen.





Für einen Produktivbetrieb sind dann noch etliche weitere Schritte nötig, wie z.B.

– Verschlüsselter Zugriff über HTTPS, wofür Zertifikate benötigt werden,



– Verknüpfung mit einem geeigneten Domainnamen, den man erst einmal inne haben muss,



– Authentifizierungs- und Autorisierungsmechanismen, evt. eine Benutzerverwaltung,



– Monitoringprozesse etc.





5.10 Informationsverarbeitung in Versicherungsunternehmen


Um erfolgreich Data Science im Versicherungsumfeld anwenden zu können, reicht es nicht allein aus, die mathematische Methoden, das regulatorische Umfeld und einige spannende Use-Cases zu kennen, sondern es wird auch ein grundlegendes Verständnis davon benötigt wo und wie welche Daten innerhalb der Geschäftsprozesse eines Versicherungsunternehmens anfallen. Mitunter kann es auch hilfreich sein zu wissen warum.

Aus diesem Grund gibt dieses Kapitel einen Einblick in die Informationsverarbeitung in Versicherungsunternehmen. Wir werden kurz auf die typischen Geschäftsprozesse in den Unternehmen eingehen, um anschlieẞend grob die Systemlandschaft zu beleuchten. Natürlich gibt es unternehmensspezifische Unterschiede, aber die hier vorgestellten Grundlagen sollten in jedem Versicherungsunternehmen mit ggf. leichten Abwandlungen vorhanden sein.


5.10.1 Geschäftsprozesse in Versicherungsunternehmen


Bevor wir uns der Systemlandschaft in Versicherungen zuwenden, wollen wir in diesem Abschnitt die wichtigsten Geschäftsprozesse einer typischen Versicherung betrachten. Für zwei Beispiele wollen wir verstehen, welche Daten dort anfallen und prototypisch überlegen, wofür diese verwendet werden könnten. Bei der hier vorgestellten Auflistung der Kernprozesse orientieren wir uns stark an der Auflistung55 von Broschinski ([7]), siehe Tabelle 5.7.

Demnach gibt es strategische Management- und Führungsprozesse wie Strategiedefinition, Controlling, Risikomanagement und interne Revision, operative Kernprozesse wie Vertrieb, Produktentwicklung, Bestandsführung, Schadenmanagement und daneben unterstützende Prozesse wie Beschaffung und Rechnungswesen.


Tab. 5.7:Übersicht Kernprozesse.



Management- und Führungsprozesse Operative Prozesse Unterstützende Prozesse

Strategiedefinition Marketing und Vertrieb Beschaffung

Controlling Produktentwicklung Rechnungswesen

Risikomanagement Bestandsführung Verwaltung

Qualitätsmanagement Schadenmanagement (Kapitalanlagemanagement) Personal Dokumentenmanagement



Aus Data Science Sicht sind die interessanteren Prozesse die operativen, weil gerade aus diesen Informationen über den Kern des Unternehmens extrahiert werden können. Sicher finden sich auch im Rechnungswesen, beispielsweise in den Buchungsdatenbanken, oder in den Solvenzsimulationen des Risikomanagements gröẞere Datenmengen, durch die man interessante Einblicke mithilfe von den in den anderen Kapitel beschriebenen Methoden gewinnen kann. Wenn man als Actuarial Data Scientist allerdings Mehrwert für das Unternehmen generieren will, sollte man vermutlich bei den operativen Prozessen beginnen.

Wir werden zwei dieser Prozesse im Folgenden genauer betrachten, um hierüber auf die typische Systemlandschaft überzuleiten und um Use-Cases für Data Science Anwendungsfälle zu finden.


Schadenmanagement


Der Schadenmanagement-Prozess tritt immer dann ein, wenn vom Kunden ein Schadenereignis bzw. ein Leistungsfall gemeldet wird.56 Nun muss der Versicherer den Schaden prüfen und gegebenenfalls eine Leistung zahlen. Die hierbei auftretenden Teilprozesse sind pro Sparte und sogar pro Unternehmen unterschiedlich, aber grob vergleichbar wie folgt gegliedert:

– Aufnahme aller für Prüfung und Regulierung erforderlichen Daten



– Prüfung der Schadenumstände und -merkmale



– Ablehnung des Schadens oder Auszahlung



– Anstoẞ von Folgeprozessen (Exkasso, Abrechnung mit Rückversicherung / Mitversicherern)





Aus Sicht dieses Buches ist natürlich der erste Schritt einer der wichtigsten. Einen der naheliegendsten Anwendungsfälle für Machine-Learning Algorithmen stellt nämlich die Leistungsprüfung dar. Gerade wenn es darum geht, ob ein Schaden ausgezahlt oder abgelehnt werden soll, kann, z.B. mit Entscheidungsbäumen, sehr gut eine maschinelle Prüfung unterstützend zur manuellen Sachbearbeitung herangezogen werden bzw. diese die manuelle Prüfung komplett unnötig machen.

Allerdings sind hierzu voraussichtlich mehr Daten nötig, als heute vorhanden sind und die heute vorhandenen Daten sind typischerweise nicht für eine maschinelle Verarbeitung aufbereitet. Neben diesen beiden Hindernissen ist auch die Anzahl der Schadenereignisse mitunter zu gering um ein geeignetes Modell zu trainieren. Trotzdem lohnt es sich, ein genauen Blick auf diesen Anwendungsfall zu werfen.


Bestandsverwaltung


Das Herzstück jeder Versicherung ist die Bestandsverwaltung57. Hier wird das Ergebnis jedes erfolgreichen Verkaufes gespeichert und gegebenenfalls angepasst. Der typische Ablauf innerhalb dieses Prozesses gliedert sich in drei Schritte:

– Erstbearbeitung: Antrag- oder Neugeschäftsbearbeitung,



– Folgebearbeitung: Planmäẞige und auẞerplanmäẞige Vertragsänderungen, 58



– Schlussbearbeitung: Abgang, Kündigung.





In allen drei Fällen ist es Aufgabe der Bestandsverwaltung, Vertragsinformationen bereitzustellen und Wertstände des Vertrages zu jedem Zeitpunkt der Laufzeit auszugeben. Daneben müssen natürlich einzelvertragliche und kollektive Berechnungen durchgeführt werden (Schadenreserven, Zinszusatzreserve oder die RfB) und, wie wir unten sehen werden, die Schnittstellen der angeschlossenen Systeme versorgt werden. Inzwischen dient die Bestandsverwaltung allerdings auch als Informationsquelle für Kundenportale und Apps.

Für den Actuarial Data Scientist gibt es gerade in der Bestandsverwaltung einige spannende Use-Cases zu entdecken:

– Ähnlich wie die oben angesprochene Leistungsprüfung, kann auch der Teilprozess der Annahme-/Risikoprüfung im Rahmen des Antragsprozesse sehr gut durch geeignete Data Science Modelle unterstützt werden. Allerdings gilt – wie schon oben – auch hier: es fehlen Daten. Die vorhandenen Daten sind mitunter lediglich handschriftlich auf dem Antrag aufgeführt (z.B. Gesundheitsfragen) und es gibt zu wenig Fälle für das Training von Modellen (beispielsweise gibt es relativ wenige Leistungsfälle für Berufsunfähigkeit bei einem Lebensversicherer). Ausnahme sind hier einige Direktversicherer, die dadurch, dass sie ihre Antragsstrecke in Web-Anwendungen ausgelagert haben, deutlich mehr Daten aufnehmen als klassische Versicherer und die diese dann auch direkt digital auswerten können.



– Innerhalb der Folgebearbeitung liegt der Einsatz von Data Science Methodiken vor allem darin, Cross-Selling-Potenzial zu erkennen und zu nutzen. Dies könnte beispielsweise geschehen, indem durch einen intelligenten Assistenten bei der Adressänderung auf die Deckungssumme der Hausratversicherung geprüft wird oder indem Bilder aus einem Videotelefonat automatisch ausgewertet werden, um zu überprüfen, ob der aktuelle Versicherungsschutz noch ausreicht.





Dies ist zum aktuellen Zeitpunkt noch Zukunftsmusik, aber gerade wenn man als Aktuar heute an der Neugestaltung einer Bestandsführung beteiligt ist, sollte man diese Aspekte im Hinterkopf behalten.


5.10.2 Typische Systemlandschaft


Wie oben schon gesagt wurde, ist der zentrale Kern einer jeden Versicherungssystemlandschaft die Bestandsverwaltung. Umdie Bestandsführung herum gliedern sich verschiedene andere Teilsysteme, welche oft einem der oben genannten Teilprozesse zugeordnet werden können. Beispielsweise gibt es bei einem Lebensversicherungsunternehmen, das Riesterverträge anbietet, typischerweise ein Teilsystem für die Zulagenverwaltung, bei Schadenversicherungen ein Teilsystem zur Schadenbearbeitung und bei eigentlich allen Sparten ein System für die Partnerverwaltung.

Trotz erheblicher unternehmensspezifischer Unterschiede ist die Systemlandschaft oft so aufgebaut, wie in Abbildung 5.32 dargestellt.

Es zeigt sich auch, dass der Aufbau der Systemlandschaft mitunter den Aufbau der Organisation widerspiegelt, oder umgekehrt. Das hat auch den Hintergrund, dass typischerweise für jedes Teilsystem eine Organisationseinheit in der IT und eine Organisationseinheit im Fachbereich verantwortlich ist. Zukünftig wird sich dies eventuell ändern, wenn die Unternehmen sich, wie in Kapitel 3 beschrieben, agiler und interdisziplinärer aufstellen wollen.


Abb. 5.32: Typische Systemlandschaft eines Versicherungsunternehmens. Prominent in der Mitte die Bestandsführung umgeben von verschiedensten Randsystemen.

Betrieben werden viele dieser Systeme auch heute noch auf drei verschiedenen Plattformen:

– den IBM-Groẞrechnern (auch Host oder Mainframe genannt),



– verschiedenen Servern,59



– den Einzelplatz-PCs.





Typischerweise läuft auf dem Host die Bestandsführung und weitere zentrale Komponenten. Auf den Servern läuft Software für Teilprozesse (wie Finanzbuchhaltungssoftware, Profit-Test- und ALM-Software, Data-Warehouse, ...). Auf den dezentralen PCs sind schlieẞlich die Angebotssoftware und natürlich die persönlichen Office-Anwendungen wie E-Mail, Dokumentenerstellung (Textverarbeitung, Präsentationen), etc. beheimatet.

Für den Aktuar, der unternehmensweite Analysen durchführen will, erzeugt diese Vielzahl an Systemen und die darunter liegenden Technologien einen hohen Abstimmungsaufwand.

Nehmen wir nur einmal an, es gibt bereits eine automatisierte Auswertung der Buchungsdaten, beispielsweise um Betrug zu erkennen, und diese soll nun mittels eines maschinellen Lernverfahrens verfeinert werden. Allerdings benötigt das maschinelle Lernverfahren zusätzlich zu den schon vorhandenen Buchungsdaten (Soll-Konto, Haben-Konto, Buchungsbetrag) noch weitere Informationen, zum Beispiel den betroffenen Tarif. Das heiẞt dann, die Buchungen müssen angereichert werden und zwar am besten dort, wo sie entstehen. Dies umfasst allerdings schon die Bestandsverwaltung (Neuzugänge werden hier gebucht), das Schaden-/Leistungssystem (Schäden werden gebucht), Inkasso-/Exkasso (typischerweise werden die Buchungen hier nur weitergereicht), das Provisionssystem und schlieẞlich das System, in welchem die Analyse der Buchungen läuft, vielleicht das Data Warehouse.

Insgesamt kann so eine vermeintlich kleine Anforderung, schnell für hohe Anpassungsaufwände und viele betroffene Abteilungen sorgen.60 Um solche Situationen zu vermeiden oder frühzeitig erkennen zu können, ist es wichtig, die Systemlandschaft und den Ablauf der Prozesse des eigenen Unternehmens zu kennen. Nur mit diesem Wissen können die Aktuare ganzheitlich und unternehmensweit neue, durch Data Science gestützte, Anwendungen einfordern, spezifizieren und die erfolgreiche Umsetzung begleiten.